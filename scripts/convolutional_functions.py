#!/usr/bin/env python# coding: utf-8from torch.optim.lr_scheduler import StepLRimport torch import torchvisionimport torch.nn as nn import torch.nn.functional as Fimport torch.optim as optimfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLRfrom IPython.display import Image from torchvision import transformsimport matplotlib.pyplot as pltimport randomfrom torch.utils.data import DataLoaderfrom tqdm.auto import tqdmdevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")import numpy as npseed = 12345random.seed(seed)torch.manual_seed(seed)##CLASSES# Helper code to support adding different transforms on the dataset lazily after downloading the dataset# From https://discuss.pytorch.org/t/apply-different-transform-data-augmentation-to-train-and-validation/63580/5class MapDataset(torch.utils.data.Dataset):    """    Given a dataset, creates a dataset which applies a mapping function    to its items (lazily, only when an item is called).    Note that data is not cloned/copied from the initial dataset.    """    def __init__(self, dataset, map_fn):        self.dataset = dataset        self.map = map_fn    def __getitem__(self, index):        if self.map:                 x = self.map(self.dataset[index][0])         else:                 x = self.dataset[index][0]          y = self.dataset[index][1]                 return x, y    def __len__(self):        return len(self.dataset)        class View(nn.Module):    def __init__(self, shape):      super().__init__()      self.shape = shape    def forward(self, x):        return x.view(*self.shape)class SimpleConvnet(nn.Module):    def __init__(self, input_channels, num_classes):        super(SimpleConvnet, self).__init__()        self.input_channels = input_channels        self.num_classes = num_classes        self.block1 = nn.Sequential(            nn.Conv2d(in_channels = self.input_channels, out_channels = 64, kernel_size=5, padding=2),            nn.ReLU()        )        self.block2 = nn.Sequential(            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size = 2)        )        self.block3 = nn.Sequential(            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size = 2)        )        self.block4 = nn.Sequential(            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.MaxPool2d(kernel_size = 8)        )        #final linear layer to project into the correct number of classes        self.fc = nn.Linear(256, self.num_classes)        def forward(self, x):               x = self.block1(x)        x = self.block2(x)        x = self.block3(x)        x = self.block4(x)        x = View((-1,256))(x)        x = self.fc(x)        output = x                return output        class SimpleConvnet2(nn.Module):    def __init__(self, input_channels, num_classes):        super(SimpleConvnet2, self).__init__()        # TODO        self.input_channels = input_channels        self.num_classes = num_classes        self.block1 = nn.Sequential(            nn.Conv2d(in_channels = self.input_channels, out_channels = 64, kernel_size=5, padding=2),            nn.ReLU()        )        self.block2 = nn.Sequential(            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(64),            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(64),            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(64),            nn.MaxPool2d(kernel_size = 2)        )        self.block3 = nn.Sequential(            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(128),            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(128),            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(128),            nn.MaxPool2d(kernel_size = 2)        )        self.block4 = nn.Sequential(            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.BatchNorm2d(256),            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.BatchNorm2d(256),            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.BatchNorm2d(256),            nn.MaxPool2d(kernel_size = 8)        )        #final linear layer to project into the correct number of classes        self.fc = nn.Linear(256, self.num_classes)        def forward(self, x):               x = self.block1(x)        x = self.block2(x)        x = self.block3(x)        x = self.block4(x)        x = View((-1,256))(x)        x = self.fc(x)        output = x                return outputclass ResidualConvnet(nn.Module):    def __init__(self, input_channels, num_classes):        super(ResidualConvnet, self).__init__()        self.input_channels = input_channels        self.num_classes = num_classes        self.block1 = nn.Sequential(            nn.Conv2d(in_channels = self.input_channels, out_channels = 64, kernel_size=5, padding=2),            nn.ReLU()        )        self.block21 = nn.Sequential(            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(64),            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(64),            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),            nn.ReLU()        )        self.block22 = nn.Sequential(            nn.BatchNorm2d(64),            nn.MaxPool2d(kernel_size = 2)        )        self.block31 = nn.Sequential(            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(128),            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.BatchNorm2d(128),            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),            nn.ReLU()          )        self.block32 = nn.Sequential(            nn.BatchNorm2d(128),            nn.MaxPool2d(kernel_size = 2)        )        self.block41 = nn.Sequential(            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.BatchNorm2d(256),            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU(),            nn.BatchNorm2d(256),            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),            nn.ReLU()         )        self.block42 = nn.Sequential(            nn.BatchNorm2d(256),            nn.MaxPool2d(kernel_size = 8)        )        #res1 is indentity        self.res2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1)        self.res3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1)        #final linear layer to project into the correct number of classes        self.fc = nn.Linear(256, self.num_classes)        def forward(self, x):                x = self.block1(x)        res1 = x.clone().detach()        x = self.block21(x)        x = x+res1        x = self.block22(x)        res2 = self.res2(x.clone().detach())        x = self.block31(x)        x = x + res2        x = self.block32(x)        res3 = self.res3(x.clone().detach())        x = self.block41(x)        x = x + res3        x = self.block42(x)         # HINT: projection         x = View((-1,256)).forward(x)        x = self.fc(x)        output = x                return output                #TRAINING LOOPS        def train_loop(model, criterion, optimizer,  train_loader, val_loader):  """  Generic training loop  Parameters  ----------  model : Object instance of your model class   criterion : Loss function   optimizer : Instance of optimizer class of your choice   train_loader : Training data loader   val_loader : Validation data loader  Returns  -------  train_losses : List with train loss on dataset per epoch  train_accuracies : List with train accuracy on dataset per epoch  val_losses : List with validation loss on dataset per epoch  val_accuracies : List with validation accuracy on dataset per epoch  """  best_val = 0.0  train_losses = []  val_losses = []  train_accuracies = []  val_accuracies = []  max_patience = 5  patience_counter = 0  # Training  for t in tqdm(range(50)):    epoch_t_acc = 0.0     epoch_t_loss = 0.0    # TODO : Set the model to train mode     model.train()           # TODO: Loop over the training set     for i, samples in enumerate(train_loader):      # TODO: Put the inputs and targets on the write device      data, target = samples      data, target = data.to(device), target.to(device)      # TODO: Feed forward to get the logits            y_pred_train = model(data)            # TODO: Compute the loss and accuracy      loss = criterion(y_pred_train, target)      score, predicted = torch.max(y_pred_train, 1)      acc = (predicted == target).sum().float() / len(target)      # TODO: zero the gradients before running      optimizer.zero_grad()      # the backward pass.      # TODO: Backward pass to compute the gradient      # of loss w.r.t our learnable params.       loss.backward()      # TODO: Update params      optimizer.step()      # TODO: Keep track of accuracy and loss      epoch_t_acc += acc      epoch_t_loss += loss.item()    # TODO: Switch the model to eval mode    train_accuracies.append(epoch_t_acc/len(train_loader))    train_losses.append(epoch_t_loss/len(train_loader))        model.eval()        v_acc = 0.0    v_loss = 0.0    with torch.no_grad():      # TODO: Loop over the validation set       for i, samples in enumerate(val_loader):        # TODO: Put the inputs and targets on the write device        data, target = samples        data, target = data.to(device), target.to(device)        # TODO: Feed forward to get the logits        y_pred_val = model(data)        # TODO: Compute the loss and accuracy        loss = criterion(y_pred_val, target)        score, predicted = torch.max(y_pred_val, 1)        acc = (predicted == target).sum().float() / len(target)        v_loss+= loss.item()        v_acc += acc      # TODO: Keep track of accuracy and loss    val_accuracies.append(v_acc/len(val_loader))    val_losses.append(v_loss/len(val_loader))    if val_accuracies[-1] > best_val:      best_val = val_accuracies[-1]      patience_counter = 0      # TODO: Save best model, optimizer, epoch_number      torch.save({          'model': model.state_dict(),          'optimizer': optimizer.state_dict(),          'epoch': t,      }, '/content/drive/My Drive/Colab Notebooks/checkpoint.pth')        else:      patience_counter += 1          if patience_counter > max_patience:         break    print("[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACCURACY]: %.3f" % (t, train_losses[-1], train_accuracies[-1]))    print("[EPOCH]: %i, [VAL LOSS]: %.6f, [VAL ACCURACY]: %.3f \n" % (t, val_losses[-1] ,val_accuracies[-1]))  return train_losses, train_accuracies, val_losses, val_accuracies                def train_loop2(model, criterion, optimizer, scheduler,  train_loader, val_loader):    """    Generic training loop    Parameters    ----------    model : Object instance of your model class     criterion : Loss function     optimizer : Instance of optimizer class of your choice     scheduler : Instance of scheduler class of your choice     train_loader : Training data loader     val_loader : Validation data loader    Returns    -------    train_losses : List with train loss on dataset per epoch    train_accuracies : List with train accuracy on dataset per epoch    val_losses : List with validation loss on dataset per epoch    val_accuracies : List with validation accuracy on dataset per epoch    """    best_val = 0.0    train_losses = []    val_losses = []    train_accuracies = []    val_accuracies = []    max_patience = 5    patience_counter = 0        # Training    for t in tqdm(range(50)):        # TODO : Set the model to train mode          epoch_t_acc = 0.0         epoch_t_loss = 0.0        # TODO : Set the model to train mode         model.train()                  # TODO: Loop over the training set         for i, samples in enumerate(train_loader):            # TODO: Put the inputs and targets on the write device            data, target = samples            data, target = data.to(device), target.to(device)                        # TODO: Feed forward to get the logits            y_pred_train = model(data)            # TODO: Compute the loss and accuracy            loss = criterion(y_pred_train, target)            score, predicted = torch.max(y_pred_train, 1)            acc = (predicted == target).sum().float() / len(target)            # TODO: zero the gradients before running            optimizer.zero_grad()            # the backward pass.            # TODO: Backward pass to compute the gradient            # of loss w.r.t our learnable params.             loss.backward()            # TODO: Update params            optimizer.step()            # TODO: Keep track of accuracy and loss            epoch_t_acc += acc            epoch_t_loss += loss.item()                train_accuracies.append(epoch_t_acc/len(train_loader))        train_losses.append(epoch_t_loss/len(train_loader))        # Switch the model to eval mode        # TODO        model.eval()            v_acc = 0.0        v_loss = 0.0        with torch.no_grad():            for i, samples in enumerate(val_loader):                # TODO: Put the inputs and targets on the write device                data, target = samples                data, target = data.to(device), target.to(device)                # TODO: Feed forward to get the logits                y_pred_val = model(data)                # TODO: Compute the loss and accuracy                loss = criterion(y_pred_val, target)                score, predicted = torch.max(y_pred_val, 1)                acc = (predicted == target).sum().float() / len(target)                v_loss+= loss.item()                v_acc += acc        # TODO: Keep track of accuracy and loss        val_accuracies.append(v_acc/len(val_loader))        val_losses.append(v_loss/len(val_loader))        if val_accuracies[-1] > best_val:          best_val = val_accuracies[-1]          patience_counter = 0          # TODO: Save best model, optimizer, epoch_number          torch.save({          'model': model.state_dict(),          'optimizer': optimizer.state_dict(),          'epoch': t,      }, '/content/drive/My Drive/Colab Notebooks/checkpoint.pth')                  else:          patience_counter += 1                        if patience_counter > max_patience:             break        print("[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACCURACY]: %.3f" % (t, train_losses[-1], train_accuracies[-1]))        print("[EPOCH]: %i, [VAL LOSS]: %.6f, [VAL ACCURACY]: %.3f \n" % (t, val_losses[-1] ,val_accuracies[-1]))        # TODO : scheduler step        scheduler.step()    return train_losses, train_accuracies, val_losses, val_accuracies                
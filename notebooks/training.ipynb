{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn \nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nimport os\n#import natsort\nfrom torch.utils import data\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader\nimport random\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nseed = 12345\nrandom.seed(seed)\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#thanks for the preprocessed dataset Tawara!\ntrain_data_arr = np.load(\"../input/ranzcr-clip-train-numpy/train_384x384.npy\", mmap_mode=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_arr[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv')\ntrain_annotations = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train_annotations.csv')\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels_dict = {train.iloc[i][0]:torch.Tensor(train.iloc[0][1:-1]) \n#               for i in range(len(train))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_dict = {}\nfor i in range(len(train)):\n    k = train.iloc[i][0]\n    v = torch.Tensor(train.iloc[i][1:-1])\n    labels_dict[k] = v","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the commented things here can change the images to become 3 channel\nclass CustomDataSet(Dataset):\n    def __init__(self, main_dir, transform, labels):\n        self.main_dir = main_dir\n        self.transform = transform\n        all_imgs = os.listdir(main_dir)\n        self.total_imgs = all_imgs#natsort.natsorted(all_imgs)\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.total_imgs)\n\n    def __getitem__(self, idx):\n        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n        image = Image.open(img_loc).convert(\"RGB\")\n        tensor_image = self.transform(image)\n        key = os.path.basename(img_loc)[:-4]#this is how we get our ids\n        #input labels as dictionary with id number as the key\n        label = self.labels[key]\n        return tensor_image, label\n\ntrain_transform = transforms.Compose([\n     transforms.Resize((384,384)),\n     transforms.ToTensor(),\n     transforms.Normalize(\n         [0.4826, 0.4824, 0.4824],\n         [0.2190, 0.2142, 0.2142])\n ])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = CustomDataSet('../input/ranzcr-clip-catheter-line-classification/train',train_transform, labels_dict)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 12345\nvalidation_split = .2\nshuffle_dataset = True\n\ndataset_size = len(images)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\n    #print(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\nval_sampler = torch.utils.data.SubsetRandomSampler(val_indices)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 12\ntrain_loader = torch.utils.data.DataLoader(images, batch_size=batch_size, \n                                           sampler=train_sampler, num_workers = 10)\nval_loader = torch.utils.data.DataLoader(images, batch_size=batch_size,\n                                                sampler=val_sampler, num_workers = 10)\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = SimpleConvnet(3,11)\n# model = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# targs = []\n# preds = []\n# for i, samples in enumerate(train_loader):\n#     if i < 5:\n#         data, target = samples\n#         data, target = data.to(device), target.to(device)\n#         pred = model(data)\n#         targs += target.cpu().detach().numpy().tolist()\n#         preds += pred.cpu().detach().numpy().tolist()\n#     else:\n#         break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# targs += [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n# preds += [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class View(nn.Module):\n    def __init__(self, shape):\n        super().__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        return x.view(*self.shape)\n    \n    \n    \nclass ShallowConvnet(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_channels : Number of input channels\n        num_classes : Number of classes for the final prediction \n        \"\"\"\n        \n        super().__init__()\n\n        self.input_channels = input_channels\n        self.num_classes = num_classes\n        \n        self.block1 = nn.Sequential(\n            nn.Conv2d(in_channels = self.input_channels, out_channels = 64, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2)\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2)\n        )\n        \n        self.block3 = nn.Sequential(\n            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 8)\n        )\n        \n        self.fc = nn.Linear(12, self.num_classes)\n        self.sig = nn.Sigmoid()\n\n\n    def forward(self, x):\n        \n        x = self.block1(x)\n\n        x = self.block2(x)\n\n        x = self.block3(x)\n            \n        x = View((-1,256))(x)\n            \n        x = self.fc(x)\n        x = self.sig(x)\n        return x\n\n\n\n        \n    \nclass SimpleConvnet(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(SimpleConvnet, self).__init__()\n\n        self.input_channels = input_channels\n        self.num_classes = num_classes\n\n        self.block1 = nn.Sequential(\n            nn.Conv2d(in_channels = self.input_channels, out_channels = 64, kernel_size=5, padding=2),\n            nn.ReLU(),\n        )\n\n        self.block2 = nn.Sequential(\n            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size = 2)\n        )\n\n        self.block3 = nn.Sequential(\n            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size = 2)\n        )\n\n        self.block4 = nn.Sequential(\n            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size = 96)\n        )\n\n        #final linear layer to project into the correct number of classes\n        self.fc = nn.Linear(256, 11)\n        #self.sig = nn.Sigmoid() beacuse BCEwithlogits does it for us\n    \n    def forward(self, x):\n       \n        x = self.block1(x)\n\n        x = self.block2(x)\n\n        x = self.block3(x)\n\n        x = self.block4(x)\n\n        x = View((-1,256))(x)\n        x = self.fc(x)\n        #x = self.sig(x)\n        output = x\n        \n        return output\n    \n    ","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(model, criterion, optimizer,  train_loader, val_loader):\n    \"\"\"\n    Generic training loop\n\n    Parameters\n    ----------\n    model : Object instance of your model class \n    criterion : Loss function \n    optimizer : Instance of optimizer class of your choice \n    train_loader : Training data loader \n    val_loader : Validation data loader\n\n    Returns\n    -------\n    train_losses : List with train loss on dataset per epoch\n    train_accuracies : List with train accuracy on dataset per epoch\n    val_losses : List with validation loss on dataset per epoch\n    val_accuracies : List with validation accuracy on dataset per epoch\n\n    \"\"\"\n    train_preds = np.array([])\n    train_targs = np.array([])\n    #make this into numpy array, and make another for the target\n    val_preds = []\n    val_targs = []\n    \n    best_loss = 1.0\n    train_losses = []\n    val_losses = []\n    train_roc = [0]\n    val_roc = [0]\n    max_patience = 5\n    patience_counter = 0\n    # Training\n    for t in tqdm(range(10)):\n        \n        epoch_t_acc = 0.0 \n        epoch_t_loss = 0.0\n        model.train()       \n        for i, samples in enumerate(train_loader):\n            \n            data, target = samples\n            #target = target.long()\n            data, target = data.to(device), target.to(device)\n            \n            y_pred_train = model(data)\n            \n#             np.concatenate((train_targs, target.cpu().detach().numpy()), axis=None)\n#             np.concatenate((train_preds, y_pred_train.cpu().detach().numpy()), axis=None)\n            \n            #y_pred_train = y_pred_train.round()\n            \n           # print(y_pred_train.size())\n           # print(target.size())\n\n            loss = criterion(y_pred_train, target)\n#             score, predicted = torch.max(y_pred_train, 1)\n#             acc = (predicted == target).sum().float() / len(target)\n            # for each example in the batch\n            acc = 0\n            '''\n            acc= 0\n            Because we have removed the sigmoid layer, it no longer makes sense \n            to calculate the accuracy this way. I should figure out how to do the \n            roc curve. Maybe just find the pandas way to calculate it for each epoch\n            and print that instead of this goofy thing\n            '''\n#             each_ex_acc = (target == y_pred_train.round()).sum(dim=1)/len(target[0])\n#             acc = each_ex_acc.sum()/len(target)\n\n#             try:\n#                 acc = roc_auc_score(target.cpu().detach().numpy(), y_pred_train.cpu().detach().numpy(), average=None)\n#             except ValueError:\n#                 acc = 0\n\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_t_acc += acc\n            epoch_t_loss += loss.item()\n\n        #train_roc.append(roc_auc_score(train_targs, train_preds), average='macro')\n        train_losses.append(epoch_t_loss/len(train_loader))\n        \n\n        model.eval()\n\n        v_acc = 0.0\n        v_loss = 0.0\n\n\n        with torch.no_grad():\n        # TODO: Loop over the validation set \n            for i, samples in enumerate(val_loader):\n\n                # TODO: Put the inputs and targets on the write device\n                data, target = samples\n                #target = target.long()\n                data, target = data.to(device), target.to(device)\n\n                # TODO: Feed forward to get the logits\n                y_pred_val = model(data)\n                \n                \n#                 val_targs = np.concatenate((val_targs, target.cpu().detach().numpy()), axis=None)\n#                 val_preds = np.concatenate((val_preds, y_pred_val.cpu().detach().numpy()), axis=None)\n\n                # TODO: Compute the loss and accuracy\n                loss = criterion(y_pred_val, target)\n#                 score, predicted = torch.max(y_pred_val, 1)\n#                 acc = (predicted == target).sum().float() / len(target)\n#                 each_ex_acc = (target == y_pred_val.round()).sum(dim=1)/len(target[0])\n#                 acc = each_ex_acc.sum()/len(target)\n        \n               # roc_auc_score(target.cpu().detach().numpy(), y_pred_val.cpu().detach().numpy(), average=None)\n        \n                acc = 0\n                v_loss+= loss.item()\n                v_acc += acc\n\n\n        # TODO: Keep track of accuracy and loss\n        #val_roc.append(roc_auc_score(val_targs, val_preds))\n        val_losses.append(v_loss/len(val_loader))\n\n        if val_losses[-1] < best_loss:\n            best_loss = val_losses[-1]\n            patience_counter = 0\n\n      # TODO: Save best model, optimizer, epoch_number\n    \n    \n    \n            torch.save(model.state_dict(), './model_state.pt')\n\n        else:\n            patience_counter += 1    \n            if patience_counter > max_patience: \n                break\n\n        print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ROC]: %.5f\" % (t, train_losses[-1], train_roc[-1]))\n        print(\"[EPOCH]: %i, [VAL LOSS]: %.6f, [VAL ROC]: %.5f \\n\" % (t, val_losses[-1] ,val_roc[-1]))\n\n    return train_losses, train_accuracies, val_losses, val_accuracies","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO : Initialize the model and cast to correct device\ninput_channels = 3\nnum_classes = 11\nmodel_sc = SimpleConvnet(input_channels, num_classes)\nmodel_sc.to(device)\n\n# TODO : Initialize the criterion\ncriterion = nn.BCEWithLogitsLoss()\n# TODO : Initialize the SGD optimizer with lr 1e-3\noptimizer = optim.SGD(model_sc.parameters(), lr = 0.001)\n\n# TODO : Run the training loop using this model\n\ntrain_losses, train_accuracies, val_losses, val_accuracies = train_loop(model_sc, criterion, optimizer, train_loader, val_loader)\n\n\n","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2243deb076c42cb8e3c6e6a6d3fa244"}},"metadata":{}},{"output_type":"stream","text":"[EPOCH]: 0, [TRAIN LOSS]: 0.300434, [TRAIN ROC]: 0.00000\n[EPOCH]: 0, [VAL LOSS]: 0.283992, [VAL ROC]: 0.00000 \n\n[EPOCH]: 1, [TRAIN LOSS]: 0.281118, [TRAIN ROC]: 0.00000\n[EPOCH]: 1, [VAL LOSS]: 0.278681, [VAL ROC]: 0.00000 \n\n[EPOCH]: 2, [TRAIN LOSS]: 0.273509, [TRAIN ROC]: 0.00000\n[EPOCH]: 2, [VAL LOSS]: 0.268395, [VAL ROC]: 0.00000 \n\n[EPOCH]: 3, [TRAIN LOSS]: 0.268824, [TRAIN ROC]: 0.00000\n[EPOCH]: 3, [VAL LOSS]: 0.265223, [VAL ROC]: 0.00000 \n\n[EPOCH]: 4, [TRAIN LOSS]: 0.264127, [TRAIN ROC]: 0.00000\n[EPOCH]: 4, [VAL LOSS]: 0.265415, [VAL ROC]: 0.00000 \n\n[EPOCH]: 5, [TRAIN LOSS]: 0.259610, [TRAIN ROC]: 0.00000\n[EPOCH]: 5, [VAL LOSS]: 0.266316, [VAL ROC]: 0.00000 \n\n\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'train_accuracies' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-3ad6468e145c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# TODO : Run the training loop using this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-5ac1638b4710>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[EPOCH]: %i, [VAL LOSS]: %.6f, [VAL ROC]: %.5f \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mval_roc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_accuracies' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mordel = torch.load('./model_state.pt')","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mogle = SimpleConvnet(3,11)\n#mogle.load_state_dict(mordel)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mordel","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"OrderedDict([('block1.0.weight',\n              tensor([[[[ 0.1131,  0.0883,  0.1143, -0.0069, -0.0944],\n                        [-0.0722, -0.0303,  0.0161, -0.0359, -0.0641],\n                        [-0.0086,  0.0724,  0.0957, -0.0558, -0.0647],\n                        [-0.0046,  0.0500,  0.0511, -0.0413,  0.0402],\n                        [-0.0195, -0.0155, -0.0944,  0.0830, -0.0825]],\n              \n                       [[ 0.0307, -0.0571, -0.0967, -0.0052,  0.0402],\n                        [-0.1038,  0.0087, -0.0430, -0.0149, -0.0490],\n                        [-0.0136,  0.1035, -0.1113, -0.0534,  0.0997],\n                        [-0.0053, -0.0249,  0.0382, -0.0471,  0.0477],\n                        [-0.0711, -0.0364,  0.0876,  0.0116, -0.0924]],\n              \n                       [[ 0.0713, -0.0761, -0.0068,  0.0018, -0.0490],\n                        [ 0.0538, -0.0111,  0.0635,  0.0603, -0.0878],\n                        [ 0.0870,  0.0025,  0.0909,  0.0731, -0.0324],\n                        [-0.0221, -0.0431, -0.0553,  0.0911,  0.1079],\n                        [ 0.0513,  0.1021,  0.0384,  0.0183, -0.0470]]],\n              \n              \n                      [[[ 0.1068, -0.0838,  0.0867, -0.0134, -0.0449],\n                        [-0.0003,  0.0713, -0.1001, -0.0872, -0.0188],\n                        [-0.1148,  0.0151, -0.0800, -0.1015, -0.0451],\n                        [-0.0626, -0.0020,  0.0783, -0.0259,  0.0241],\n                        [ 0.0514,  0.0293,  0.0057,  0.0203,  0.0689]],\n              \n                       [[-0.1047, -0.0473,  0.1035, -0.0289,  0.0737],\n                        [-0.0919,  0.0262, -0.0011, -0.1061,  0.0282],\n                        [-0.0126, -0.1127, -0.0342,  0.0974,  0.0250],\n                        [-0.0712, -0.0610, -0.0517, -0.0576,  0.0228],\n                        [-0.0550,  0.0803,  0.0616,  0.0861,  0.0030]],\n              \n                       [[ 0.1033, -0.0980, -0.0609, -0.0643,  0.0546],\n                        [-0.1095, -0.0543,  0.0483,  0.1104,  0.0775],\n                        [ 0.0474, -0.0822,  0.0368, -0.0398,  0.0085],\n                        [-0.0155,  0.1066,  0.0763,  0.0144, -0.1112],\n                        [ 0.0897,  0.0774,  0.0392, -0.1127, -0.0758]]],\n              \n              \n                      [[[ 0.0627, -0.1063,  0.0491,  0.0131,  0.1064],\n                        [ 0.0408,  0.0630,  0.0386, -0.0537, -0.0613],\n                        [-0.1112, -0.0166,  0.0028, -0.0362,  0.0338],\n                        [ 0.0824,  0.0110, -0.0449, -0.1016, -0.0593],\n                        [ 0.0372, -0.1058, -0.1152,  0.0045,  0.0027]],\n              \n                       [[-0.0888,  0.0294,  0.0503, -0.0901, -0.0162],\n                        [ 0.0697,  0.1050, -0.0795, -0.0612, -0.0639],\n                        [-0.0580, -0.0734,  0.0897, -0.0047, -0.0859],\n                        [-0.0804, -0.0965, -0.1085,  0.0057,  0.0124],\n                        [ 0.0680,  0.0974,  0.0809,  0.0987, -0.0451]],\n              \n                       [[-0.0850, -0.0085, -0.0250, -0.0057,  0.0037],\n                        [-0.0637, -0.0966, -0.0462,  0.0225,  0.0606],\n                        [-0.1027, -0.1022, -0.0055, -0.0432, -0.0451],\n                        [-0.1102,  0.0535, -0.0194,  0.0901, -0.0741],\n                        [-0.1033,  0.0003,  0.0269,  0.0655, -0.0140]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0250,  0.0529, -0.0801, -0.0775,  0.0077],\n                        [-0.0831,  0.0599, -0.0628, -0.0616, -0.0852],\n                        [-0.0199,  0.0452,  0.0897, -0.0245, -0.0847],\n                        [-0.0659,  0.1062, -0.0248,  0.0579, -0.0848],\n                        [-0.1099, -0.0822,  0.0214, -0.0834,  0.0963]],\n              \n                       [[-0.0040,  0.1101, -0.0838,  0.0225, -0.0893],\n                        [-0.0349, -0.0845, -0.0296,  0.0945,  0.0928],\n                        [-0.1082, -0.0015, -0.0043,  0.0668,  0.0670],\n                        [ 0.0447,  0.0888,  0.0401,  0.0191,  0.0013],\n                        [ 0.0462,  0.0644, -0.0100, -0.0644, -0.0796]],\n              \n                       [[-0.0579,  0.0985, -0.0435, -0.0642, -0.0198],\n                        [ 0.0366, -0.0609, -0.0242, -0.0352,  0.0256],\n                        [ 0.0568,  0.0710, -0.0567,  0.0708, -0.0372],\n                        [ 0.0030,  0.0288,  0.0871, -0.1079, -0.0577],\n                        [-0.0045,  0.0049, -0.0633, -0.0138, -0.1123]]],\n              \n              \n                      [[[-0.0054, -0.1121,  0.1040,  0.0586,  0.0571],\n                        [ 0.0334,  0.0455,  0.0809,  0.0474, -0.0186],\n                        [-0.0452,  0.0936, -0.0057,  0.0054,  0.1087],\n                        [-0.0868,  0.0515,  0.0575, -0.0421, -0.0367],\n                        [ 0.0439, -0.0801,  0.0811,  0.1080, -0.1097]],\n              \n                       [[-0.1136,  0.1056,  0.0249,  0.0502, -0.0581],\n                        [-0.0210,  0.0082, -0.0118, -0.0304,  0.0495],\n                        [-0.0215, -0.0396,  0.0113, -0.0101, -0.0306],\n                        [-0.1155,  0.1003,  0.0663, -0.0736,  0.0433],\n                        [ 0.0932, -0.1056, -0.0120,  0.1144,  0.0108]],\n              \n                       [[-0.0040,  0.0335, -0.0446, -0.0833, -0.1063],\n                        [ 0.0862, -0.0085, -0.0566, -0.0315,  0.0846],\n                        [-0.1079, -0.0084, -0.0013, -0.0359,  0.0553],\n                        [ 0.0661,  0.0786, -0.0945,  0.0770,  0.0170],\n                        [-0.0816, -0.0300,  0.0610,  0.0655, -0.0306]]],\n              \n              \n                      [[[ 0.0098,  0.1027, -0.0411, -0.1141, -0.0456],\n                        [ 0.0567,  0.0740,  0.0435,  0.0659,  0.0649],\n                        [ 0.0870,  0.0303,  0.0006,  0.1030, -0.0777],\n                        [ 0.0142,  0.0059,  0.0627,  0.0631, -0.1145],\n                        [-0.0923,  0.0172, -0.0192, -0.0653, -0.0842]],\n              \n                       [[ 0.0621, -0.0339,  0.0611, -0.0860,  0.0281],\n                        [ 0.0128, -0.0227, -0.0472,  0.0799, -0.0587],\n                        [ 0.0386,  0.0864,  0.0573,  0.0175, -0.0078],\n                        [ 0.0597,  0.0469, -0.0145, -0.0701,  0.0348],\n                        [ 0.0613, -0.0245,  0.0852, -0.0554,  0.0394]],\n              \n                       [[-0.0551, -0.0271,  0.0543, -0.0298, -0.0677],\n                        [-0.0633,  0.0368,  0.0182,  0.0295, -0.1019],\n                        [ 0.1113,  0.0505, -0.0465,  0.0964, -0.0090],\n                        [ 0.0539, -0.0410,  0.1087, -0.0407, -0.0163],\n                        [ 0.0390,  0.0426, -0.0354,  0.0296,  0.0108]]]], device='cuda:0')),\n             ('block1.0.bias',\n              tensor([ 0.0311, -0.0530,  0.0302,  0.0436, -0.0822, -0.0656,  0.0366, -0.0843,\n                       0.0144, -0.0772, -0.0087,  0.0870, -0.1055, -0.1045,  0.0080,  0.0801,\n                       0.0686, -0.0148,  0.1091, -0.1022,  0.1129, -0.1135, -0.0639,  0.0142,\n                      -0.0986,  0.0750,  0.0455,  0.0007,  0.1126,  0.0005,  0.1148,  0.0639,\n                       0.0859, -0.0633, -0.1082,  0.0369, -0.1112, -0.0122, -0.0851,  0.0967,\n                       0.0079, -0.0856,  0.0031,  0.0006, -0.0530,  0.0211,  0.1014,  0.0428,\n                      -0.0306, -0.0614,  0.0381, -0.0623,  0.0244, -0.0296,  0.0447, -0.0275,\n                      -0.0611,  0.0703, -0.0186, -0.0467,  0.0381, -0.0510, -0.0208,  0.0910],\n                     device='cuda:0')),\n             ('block2.0.weight',\n              tensor([[[[ 3.9246e-03,  5.9061e-03, -1.4913e-02],\n                        [-1.5686e-04, -3.7599e-02, -3.8520e-02],\n                        [ 2.5901e-02, -5.4349e-03,  3.4094e-02]],\n              \n                       [[ 3.1104e-02,  4.0258e-02, -2.8644e-02],\n                        [-3.3992e-02,  2.8204e-02,  3.8796e-02],\n                        [ 3.2764e-02, -2.6316e-02,  3.7798e-02]],\n              \n                       [[ 2.3047e-02, -1.4122e-02, -3.9981e-03],\n                        [ 3.1469e-02, -1.2095e-02,  2.4337e-03],\n                        [ 1.6666e-02,  1.7963e-02, -1.9576e-02]],\n              \n                       ...,\n              \n                       [[ 3.5560e-02, -3.5664e-02,  7.2120e-03],\n                        [-2.3470e-02, -2.5198e-02,  2.4509e-03],\n                        [ 6.9422e-03,  1.4481e-02, -3.1874e-02]],\n              \n                       [[-2.7135e-02, -1.6683e-02, -2.5391e-02],\n                        [-3.1035e-02,  3.2555e-03, -7.1374e-03],\n                        [ 3.0261e-02,  4.0315e-02, -3.1550e-03]],\n              \n                       [[-1.3862e-02,  1.4200e-02,  1.5905e-02],\n                        [ 2.9747e-03,  1.6768e-02, -4.7006e-03],\n                        [-1.6759e-03,  2.2296e-02,  2.6107e-02]]],\n              \n              \n                      [[[-2.2161e-02, -1.1144e-02,  1.5299e-02],\n                        [-4.8917e-03, -1.6915e-02, -1.8853e-02],\n                        [-2.9594e-03, -5.0038e-03, -3.2515e-02]],\n              \n                       [[-2.2865e-02,  3.3141e-03, -2.1040e-02],\n                        [ 5.3281e-03, -4.1017e-02, -2.6587e-02],\n                        [-3.0920e-03, -3.6914e-02,  2.2685e-02]],\n              \n                       [[-1.8842e-02, -4.1458e-02,  1.2222e-02],\n                        [-2.6761e-02, -1.7573e-02,  1.2240e-02],\n                        [ 3.0162e-02,  3.8152e-02,  2.7438e-02]],\n              \n                       ...,\n              \n                       [[ 3.8729e-02, -2.4663e-02, -7.8181e-03],\n                        [-1.6588e-02, -3.9323e-02, -2.2876e-02],\n                        [ 8.1124e-03,  2.7115e-02, -8.4911e-03]],\n              \n                       [[-2.7416e-02, -1.9896e-02, -2.9387e-02],\n                        [-3.7703e-02, -3.3240e-02, -3.0637e-02],\n                        [ 1.4207e-02, -2.5506e-02, -3.1042e-02]],\n              \n                       [[ 2.4226e-02, -3.0390e-02, -3.3339e-02],\n                        [ 3.5709e-03, -5.0694e-03, -3.4923e-02],\n                        [-2.4329e-02, -1.5304e-02,  2.6793e-02]]],\n              \n              \n                      [[[ 1.9650e-04, -4.2425e-02,  3.9359e-02],\n                        [ 4.0672e-02,  1.5577e-02, -1.2759e-02],\n                        [ 3.0351e-02,  4.3895e-03, -3.8241e-02]],\n              \n                       [[-3.6832e-02, -3.1579e-03, -3.9639e-02],\n                        [-2.0573e-02,  2.5576e-02,  2.8453e-02],\n                        [-2.2931e-02, -4.1169e-02,  1.9069e-02]],\n              \n                       [[ 3.6923e-02,  3.3665e-02,  1.5087e-02],\n                        [ 2.3910e-02,  8.3251e-04, -6.9601e-05],\n                        [-3.3230e-02,  1.1054e-02, -3.7832e-02]],\n              \n                       ...,\n              \n                       [[ 4.3196e-03, -1.1375e-02, -3.4981e-02],\n                        [ 2.0139e-02,  2.5222e-03, -1.7756e-02],\n                        [ 1.4307e-02,  6.1376e-04, -2.8123e-02]],\n              \n                       [[ 2.2531e-02,  1.5427e-02, -8.0965e-03],\n                        [ 2.8918e-02, -3.3272e-02, -3.2091e-02],\n                        [ 4.2038e-02,  3.5632e-02,  3.8259e-02]],\n              \n                       [[ 3.0848e-02, -2.8520e-02, -5.1867e-03],\n                        [-4.7007e-03,  4.1284e-02, -2.6538e-02],\n                        [-3.0309e-02, -3.0178e-02, -3.5560e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-4.1229e-02,  1.7266e-02, -2.4797e-02],\n                        [ 2.6725e-02,  4.1423e-02, -6.6918e-03],\n                        [ 6.1693e-04, -1.9274e-02, -3.8197e-02]],\n              \n                       [[-4.2024e-03,  1.6538e-02, -1.9638e-02],\n                        [-1.0040e-03, -3.8800e-02, -3.4285e-02],\n                        [ 3.5061e-02, -1.0498e-02,  9.5418e-03]],\n              \n                       [[ 3.6711e-02,  2.0826e-02, -3.0554e-02],\n                        [ 4.0572e-02,  2.7620e-02,  2.6344e-02],\n                        [ 1.5947e-02, -1.9279e-02, -4.0353e-03]],\n              \n                       ...,\n              \n                       [[-1.1578e-02, -3.8665e-02,  3.4454e-02],\n                        [-5.3060e-03,  4.1792e-02, -2.2378e-02],\n                        [ 8.4755e-03,  2.1841e-03,  2.1742e-02]],\n              \n                       [[-2.8892e-02,  2.4001e-03, -3.3343e-02],\n                        [ 1.9462e-02, -8.6196e-03, -2.6473e-02],\n                        [ 1.1054e-02, -2.1620e-02, -9.0788e-03]],\n              \n                       [[-8.7092e-03,  2.7983e-02, -4.1832e-02],\n                        [-3.4176e-02, -2.0737e-02,  6.5018e-03],\n                        [ 2.3047e-02,  3.6373e-02, -1.9306e-02]]],\n              \n              \n                      [[[-1.1171e-02,  5.4897e-03,  2.1426e-02],\n                        [-3.7125e-03, -8.8049e-03, -1.9851e-02],\n                        [-3.5308e-02,  3.1340e-02,  2.5799e-02]],\n              \n                       [[-2.8328e-02,  1.7313e-02,  2.4468e-02],\n                        [ 1.7264e-02, -2.5062e-02,  8.6293e-03],\n                        [-2.7191e-02, -2.8509e-02, -1.4966e-03]],\n              \n                       [[-2.0653e-02,  2.4439e-02, -1.9290e-02],\n                        [-2.3542e-02,  3.2288e-02, -9.2871e-03],\n                        [-2.9746e-02, -3.4761e-02, -2.0702e-03]],\n              \n                       ...,\n              \n                       [[ 3.8514e-02,  1.9385e-02, -4.1308e-02],\n                        [-3.1353e-02, -1.0371e-02, -2.3724e-02],\n                        [ 1.4776e-02,  7.3002e-03,  1.2032e-02]],\n              \n                       [[-8.2473e-03,  3.0181e-02,  1.5755e-02],\n                        [ 6.7251e-03,  8.4784e-03,  5.3777e-03],\n                        [ 2.0016e-02, -7.4160e-03, -2.8899e-02]],\n              \n                       [[-4.0396e-03, -1.1010e-02,  3.6116e-02],\n                        [ 1.0811e-02, -3.8670e-02, -3.0426e-03],\n                        [-2.8921e-02,  9.7125e-03, -3.8325e-02]]],\n              \n              \n                      [[[ 1.9010e-03,  2.7560e-02, -3.3986e-02],\n                        [ 4.1379e-02, -1.1997e-02, -3.9449e-02],\n                        [ 2.5147e-02,  1.3378e-02,  1.0664e-02]],\n              \n                       [[-2.0122e-02,  3.6837e-02,  4.1595e-02],\n                        [ 2.4693e-02,  7.3467e-03,  7.9793e-03],\n                        [ 9.0395e-03,  2.0037e-03, -3.0652e-02]],\n              \n                       [[ 4.0285e-02, -3.4916e-02,  2.2738e-02],\n                        [-3.8258e-02,  2.7454e-02, -1.8976e-02],\n                        [ 1.3841e-02,  1.9625e-02, -1.8322e-02]],\n              \n                       ...,\n              \n                       [[-2.8316e-02,  3.5440e-02, -3.5313e-03],\n                        [-3.6235e-04, -1.5200e-02, -3.1764e-03],\n                        [-3.9900e-02,  3.3994e-02, -4.5838e-04]],\n              \n                       [[-2.2082e-02,  3.9812e-03,  2.1981e-02],\n                        [ 2.6053e-02,  3.2542e-02,  1.1420e-02],\n                        [-2.3997e-02,  3.5027e-02,  2.3339e-02]],\n              \n                       [[-1.7887e-02, -6.5010e-03,  2.7113e-02],\n                        [ 3.0101e-03, -3.0677e-02,  2.6678e-02],\n                        [ 2.4902e-03, -1.5518e-02,  2.4645e-02]]]], device='cuda:0')),\n             ('block2.0.bias',\n              tensor([-0.0016,  0.0087,  0.0320, -0.0313,  0.0359, -0.0007, -0.0201, -0.0370,\n                       0.0084, -0.0048,  0.0168, -0.0364,  0.0413,  0.0087, -0.0407, -0.0234,\n                       0.0287, -0.0138, -0.0235, -0.0003,  0.0198,  0.0147, -0.0068,  0.0091,\n                       0.0187, -0.0307,  0.0235,  0.0088,  0.0020, -0.0259, -0.0317, -0.0290,\n                       0.0183,  0.0039, -0.0377, -0.0018, -0.0015,  0.0333, -0.0348, -0.0017,\n                      -0.0155, -0.0189, -0.0192,  0.0208, -0.0084,  0.0310,  0.0562,  0.0248,\n                       0.0315, -0.0251,  0.0132,  0.0125, -0.0004,  0.0282, -0.0103,  0.0102,\n                      -0.0342,  0.0255, -0.0187, -0.0110,  0.0182, -0.0126, -0.0015,  0.0063],\n                     device='cuda:0')),\n             ('block2.2.weight',\n              tensor([0.9999, 0.9997, 0.9990, 1.0004, 0.9993, 1.0001, 0.9997, 0.9993, 0.9988,\n                      1.0003, 1.0003, 0.9992, 1.0007, 1.0000, 0.9996, 0.9999, 1.0015, 1.0010,\n                      1.0005, 0.9998, 0.9997, 0.9997, 0.9992, 0.9996, 1.0000, 0.9996, 1.0007,\n                      0.9995, 1.0009, 1.0001, 1.0006, 1.0000, 1.0009, 0.9998, 1.0001, 1.0000,\n                      1.0001, 1.0007, 0.9992, 0.9998, 1.0000, 0.9995, 1.0004, 0.9994, 1.0007,\n                      0.9997, 0.9999, 1.0002, 1.0006, 1.0010, 1.0002, 1.0001, 0.9997, 0.9989,\n                      1.0005, 1.0008, 1.0002, 0.9998, 0.9996, 0.9979, 0.9995, 1.0005, 1.0000,\n                      1.0010], device='cuda:0')),\n             ('block2.2.bias',\n              tensor([ 3.9814e-04, -1.1074e-03,  4.2258e-04,  4.0546e-04,  3.5520e-04,\n                      -9.0703e-05,  3.0501e-04, -6.3198e-04, -2.8677e-04,  1.7943e-04,\n                       6.9112e-06,  2.9747e-04,  3.8922e-05,  2.2823e-04, -6.9011e-06,\n                      -3.7793e-04,  2.2687e-04, -4.3998e-05, -2.8599e-04,  1.4024e-04,\n                       1.8685e-04,  4.2427e-04,  4.3087e-04,  3.4140e-04,  4.2716e-04,\n                       2.7158e-04,  4.9239e-04,  1.2344e-04, -4.7674e-04, -5.1141e-04,\n                       3.3119e-04, -5.3817e-04, -3.7872e-04,  2.0106e-04, -3.5784e-04,\n                       1.3421e-06,  6.6406e-04,  2.3487e-04,  2.8532e-04,  1.1660e-03,\n                       1.2323e-04,  9.1813e-04, -6.7387e-04,  4.1546e-05, -1.8150e-04,\n                       3.7239e-04, -1.0169e-04, -4.5216e-04,  1.1004e-04, -4.0773e-04,\n                       3.4008e-04,  5.8791e-04, -4.7030e-04, -2.4840e-04, -2.5798e-04,\n                       3.4201e-04,  5.2702e-04, -3.6076e-04, -1.6345e-04, -1.5126e-04,\n                       4.8114e-05,  1.6855e-04, -4.7336e-05,  2.4454e-04], device='cuda:0')),\n             ('block2.2.running_mean',\n              tensor([1.5846e-01, 5.4111e-02, 9.2109e-02, 8.1111e-02, 2.1693e-01, 7.1574e-02,\n                      1.2948e-01, 6.8038e-02, 6.0123e-03, 1.3347e-01, 1.0564e-01, 7.2618e-02,\n                      1.5676e-01, 1.8372e-01, 4.8938e-08, 8.0740e-02, 2.4667e-02, 9.9426e-02,\n                      2.1968e-02, 9.9843e-02, 1.3478e-01, 3.0654e-03, 1.6647e-01, 3.7812e-05,\n                      5.8585e-02, 3.7941e-03, 1.1215e-01, 2.8773e-02, 6.6429e-02, 2.1102e-01,\n                      9.9800e-02, 1.8400e-01, 1.2998e-02, 7.4000e-02, 1.0865e-01, 1.5088e-01,\n                      3.3413e-07, 1.0096e-01, 4.0882e-02, 5.5991e-02, 1.9226e-01, 2.0857e-01,\n                      1.5200e-01, 1.3192e-01, 1.0543e-01, 1.3795e-01, 1.5439e-02, 2.1186e-01,\n                      7.1514e-02, 1.9330e-01, 6.6422e-02, 6.5828e-02, 2.1345e-01, 1.0963e-01,\n                      1.3147e-01, 6.8895e-02, 9.7227e-02, 8.5270e-02, 2.8071e-01, 4.6005e-03,\n                      1.0394e-01, 8.6589e-02, 1.0748e-06, 5.4074e-02], device='cuda:0')),\n             ('block2.2.running_var',\n              tensor([5.8536e-02, 5.8196e-03, 1.1001e-02, 1.5105e-02, 1.0595e-01, 1.0198e-02,\n                      1.8637e-02, 1.4496e-02, 7.1677e-04, 2.6952e-02, 2.4859e-02, 1.2332e-02,\n                      1.0938e-02, 5.0478e-02, 2.0071e-09, 1.0689e-02, 2.4192e-03, 2.6608e-02,\n                      1.5627e-03, 1.3462e-02, 2.5975e-02, 2.1360e-04, 6.6816e-02, 3.2445e-06,\n                      4.4307e-03, 8.6843e-04, 2.6053e-02, 1.2478e-03, 1.0405e-02, 7.5575e-02,\n                      1.7287e-02, 7.6623e-02, 1.7086e-03, 9.8100e-03, 2.4646e-02, 5.0513e-02,\n                      3.2680e-08, 1.3754e-02, 4.2504e-03, 6.6451e-03, 7.6379e-02, 6.2083e-02,\n                      2.1273e-02, 2.0855e-02, 2.0970e-02, 3.0812e-02, 7.1242e-04, 4.4620e-02,\n                      6.5654e-03, 3.7881e-02, 3.9998e-03, 5.7613e-03, 7.4251e-02, 2.1808e-02,\n                      4.0859e-02, 1.2977e-02, 2.3956e-02, 9.1693e-03, 1.0403e-01, 7.3957e-04,\n                      1.7917e-02, 1.8524e-02, 7.7516e-08, 6.6254e-03], device='cuda:0')),\n             ('block2.2.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block2.3.weight',\n              tensor([[[[-0.0408, -0.0310, -0.0364],\n                        [-0.0345, -0.0130, -0.0126],\n                        [ 0.0249,  0.0122, -0.0124]],\n              \n                       [[-0.0016,  0.0334, -0.0216],\n                        [ 0.0142,  0.0420,  0.0300],\n                        [ 0.0128,  0.0137, -0.0275]],\n              \n                       [[ 0.0228, -0.0393, -0.0287],\n                        [ 0.0177, -0.0174,  0.0148],\n                        [-0.0407, -0.0299, -0.0026]],\n              \n                       ...,\n              \n                       [[ 0.0308, -0.0063, -0.0012],\n                        [-0.0306,  0.0070,  0.0226],\n                        [ 0.0004, -0.0264, -0.0077]],\n              \n                       [[-0.0376,  0.0419, -0.0291],\n                        [ 0.0249, -0.0045, -0.0034],\n                        [-0.0300,  0.0349,  0.0410]],\n              \n                       [[ 0.0082,  0.0294,  0.0396],\n                        [-0.0088, -0.0377, -0.0184],\n                        [ 0.0105,  0.0203,  0.0153]]],\n              \n              \n                      [[[-0.0189,  0.0346,  0.0363],\n                        [ 0.0193, -0.0415, -0.0208],\n                        [ 0.0046,  0.0383,  0.0143]],\n              \n                       [[ 0.0029,  0.0223,  0.0212],\n                        [ 0.0115,  0.0151, -0.0311],\n                        [ 0.0072,  0.0187, -0.0360]],\n              \n                       [[ 0.0351,  0.0081,  0.0004],\n                        [ 0.0034, -0.0408, -0.0084],\n                        [-0.0115, -0.0316, -0.0408]],\n              \n                       ...,\n              \n                       [[-0.0133, -0.0245, -0.0313],\n                        [-0.0048, -0.0403, -0.0383],\n                        [ 0.0389,  0.0050, -0.0205]],\n              \n                       [[ 0.0106, -0.0346,  0.0249],\n                        [ 0.0067, -0.0145,  0.0067],\n                        [-0.0198, -0.0107,  0.0331]],\n              \n                       [[ 0.0170,  0.0333, -0.0141],\n                        [ 0.0032,  0.0286, -0.0347],\n                        [-0.0304,  0.0087,  0.0226]]],\n              \n              \n                      [[[ 0.0332,  0.0314,  0.0411],\n                        [ 0.0040,  0.0271,  0.0274],\n                        [ 0.0020,  0.0247, -0.0323]],\n              \n                       [[-0.0125, -0.0148, -0.0416],\n                        [ 0.0212,  0.0075,  0.0268],\n                        [-0.0358, -0.0321,  0.0191]],\n              \n                       [[ 0.0133, -0.0100,  0.0219],\n                        [ 0.0158, -0.0404,  0.0296],\n                        [-0.0047, -0.0245, -0.0222]],\n              \n                       ...,\n              \n                       [[-0.0096,  0.0244,  0.0003],\n                        [ 0.0016, -0.0267,  0.0252],\n                        [ 0.0365,  0.0057, -0.0072]],\n              \n                       [[ 0.0274, -0.0371, -0.0343],\n                        [-0.0129, -0.0013,  0.0117],\n                        [-0.0150,  0.0371,  0.0392]],\n              \n                       [[-0.0240,  0.0371,  0.0257],\n                        [-0.0071,  0.0359,  0.0178],\n                        [-0.0124,  0.0095,  0.0371]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0220, -0.0198, -0.0208],\n                        [ 0.0375, -0.0250, -0.0254],\n                        [ 0.0013,  0.0024, -0.0105]],\n              \n                       [[ 0.0396,  0.0133, -0.0073],\n                        [ 0.0280,  0.0111,  0.0220],\n                        [ 0.0317, -0.0157,  0.0084]],\n              \n                       [[-0.0341, -0.0376, -0.0391],\n                        [ 0.0123,  0.0368,  0.0382],\n                        [ 0.0215,  0.0138,  0.0235]],\n              \n                       ...,\n              \n                       [[ 0.0103,  0.0323, -0.0200],\n                        [ 0.0032,  0.0003, -0.0358],\n                        [ 0.0070, -0.0173,  0.0212]],\n              \n                       [[-0.0308,  0.0102,  0.0301],\n                        [-0.0291,  0.0073, -0.0040],\n                        [ 0.0175, -0.0100, -0.0328]],\n              \n                       [[ 0.0026,  0.0143,  0.0165],\n                        [ 0.0184, -0.0394, -0.0044],\n                        [ 0.0255,  0.0075, -0.0135]]],\n              \n              \n                      [[[ 0.0255,  0.0180, -0.0015],\n                        [ 0.0132, -0.0229, -0.0087],\n                        [ 0.0127,  0.0206,  0.0181]],\n              \n                       [[ 0.0089, -0.0366,  0.0082],\n                        [-0.0367, -0.0075,  0.0297],\n                        [ 0.0016, -0.0012, -0.0290]],\n              \n                       [[-0.0072, -0.0195,  0.0346],\n                        [ 0.0274, -0.0195, -0.0358],\n                        [ 0.0120,  0.0077,  0.0029]],\n              \n                       ...,\n              \n                       [[ 0.0395, -0.0413, -0.0150],\n                        [-0.0280,  0.0140,  0.0251],\n                        [ 0.0381,  0.0081,  0.0365]],\n              \n                       [[-0.0320,  0.0227,  0.0326],\n                        [ 0.0060,  0.0152, -0.0261],\n                        [ 0.0288,  0.0351,  0.0095]],\n              \n                       [[ 0.0324, -0.0095, -0.0159],\n                        [-0.0338,  0.0166, -0.0227],\n                        [-0.0325, -0.0007,  0.0301]]],\n              \n              \n                      [[[ 0.0303, -0.0210, -0.0160],\n                        [-0.0284, -0.0192,  0.0010],\n                        [-0.0212,  0.0302, -0.0259]],\n              \n                       [[-0.0269,  0.0244, -0.0345],\n                        [ 0.0417, -0.0002,  0.0125],\n                        [-0.0142, -0.0322,  0.0274]],\n              \n                       [[-0.0350,  0.0383,  0.0342],\n                        [ 0.0372,  0.0038, -0.0316],\n                        [ 0.0129, -0.0045,  0.0354]],\n              \n                       ...,\n              \n                       [[ 0.0015,  0.0220, -0.0105],\n                        [ 0.0115, -0.0074,  0.0174],\n                        [-0.0367,  0.0306,  0.0013]],\n              \n                       [[-0.0316,  0.0265, -0.0321],\n                        [-0.0296, -0.0276, -0.0235],\n                        [ 0.0232,  0.0340,  0.0130]],\n              \n                       [[-0.0404,  0.0357,  0.0089],\n                        [-0.0041, -0.0257, -0.0110],\n                        [ 0.0165,  0.0273, -0.0325]]]], device='cuda:0')),\n             ('block2.3.bias',\n              tensor([ 0.0286, -0.0023,  0.0420, -0.0391,  0.0336,  0.0389, -0.0211,  0.0335,\n                      -0.0176, -0.0405, -0.0148,  0.0116,  0.0177,  0.0281, -0.0412,  0.0299,\n                      -0.0046, -0.0004, -0.0325, -0.0203,  0.0105, -0.0068,  0.0421,  0.0111,\n                       0.0037, -0.0006, -0.0077,  0.0021, -0.0087, -0.0193,  0.0225,  0.0061,\n                       0.0140,  0.0329,  0.0120,  0.0118, -0.0136, -0.0148, -0.0406,  0.0153,\n                      -0.0378, -0.0156, -0.0348,  0.0209,  0.0225, -0.0374, -0.0128,  0.0393,\n                      -0.0224, -0.0229,  0.0291, -0.0338, -0.0068, -0.0316, -0.0311,  0.0293,\n                      -0.0090,  0.0120,  0.0250,  0.0198, -0.0287, -0.0245,  0.0277,  0.0162],\n                     device='cuda:0')),\n             ('block2.5.weight',\n              tensor([1.0003, 0.9999, 1.0000, 0.9999, 0.9990, 0.9989, 1.0002, 1.0007, 0.9995,\n                      1.0002, 1.0002, 0.9995, 1.0009, 0.9999, 1.0001, 1.0009, 1.0005, 1.0010,\n                      1.0002, 0.9995, 1.0002, 1.0002, 1.0003, 1.0009, 0.9988, 1.0004, 1.0000,\n                      0.9994, 1.0003, 0.9997, 0.9999, 0.9998, 1.0001, 0.9989, 0.9996, 0.9999,\n                      0.9996, 0.9990, 1.0005, 1.0001, 1.0009, 0.9999, 0.9997, 1.0009, 0.9998,\n                      1.0003, 1.0001, 0.9999, 1.0005, 1.0003, 0.9999, 0.9990, 0.9994, 0.9993,\n                      1.0000, 1.0011, 1.0002, 0.9993, 1.0001, 1.0007, 1.0003, 1.0005, 0.9998,\n                      0.9992], device='cuda:0')),\n             ('block2.5.bias',\n              tensor([ 2.8738e-04, -2.0927e-04,  1.6941e-04, -4.8577e-04,  3.4714e-04,\n                      -2.3810e-04,  2.7651e-05,  4.9159e-04,  3.1742e-06,  1.4687e-04,\n                       3.3753e-04,  3.1574e-04,  1.8086e-05, -3.9380e-04,  3.0065e-04,\n                      -2.1777e-04,  3.8241e-04,  7.0376e-04, -5.6456e-05,  2.0572e-05,\n                       3.2138e-04, -2.4839e-04, -2.8553e-04, -7.2883e-04,  6.6832e-04,\n                      -2.1643e-04, -5.4951e-04, -7.8211e-05,  1.0586e-04, -2.9161e-04,\n                      -8.4138e-04, -2.8833e-04, -6.0731e-04,  3.2061e-04,  1.2323e-04,\n                       7.6273e-04,  1.8357e-04, -2.2819e-04,  4.8966e-04,  1.3383e-04,\n                      -2.3751e-04, -1.8003e-04, -4.3979e-04, -1.3873e-04, -1.7138e-04,\n                      -1.2262e-04,  4.2055e-05,  1.7190e-04,  9.0355e-05, -2.3228e-04,\n                      -6.4767e-04,  2.0524e-04,  8.5340e-04, -2.8310e-05, -3.3852e-04,\n                      -4.7815e-04, -1.6187e-04, -1.2609e-05, -3.0767e-04, -4.6303e-04,\n                       5.0678e-04, -2.8886e-04,  2.1043e-04,  2.0746e-04], device='cuda:0')),\n             ('block2.5.running_mean',\n              tensor([0.3492, 0.1647, 0.1355, 0.2462, 0.2273, 0.1228, 0.2105, 0.1913, 0.4056,\n                      0.1893, 0.1907, 0.3707, 0.1954, 0.1541, 0.1569, 0.1330, 0.2021, 0.2084,\n                      0.2794, 0.3298, 0.3440, 0.1876, 0.1492, 0.1715, 0.2781, 0.2688, 0.2128,\n                      0.2321, 0.1876, 0.2719, 0.2971, 0.1544, 0.2909, 0.3559, 0.2682, 0.2765,\n                      0.1856, 0.1485, 0.2832, 0.2466, 0.1475, 0.1650, 0.3074, 0.3378, 0.1564,\n                      0.1002, 0.3075, 0.4862, 0.1128, 0.3123, 0.3691, 0.2977, 0.3653, 0.2901,\n                      0.2440, 0.1554, 0.2582, 0.2599, 0.5923, 0.0913, 0.2960, 0.2912, 0.1860,\n                      0.3753], device='cuda:0')),\n             ('block2.5.running_var',\n              tensor([0.2720, 0.0519, 0.0555, 0.2098, 0.1415, 0.0578, 0.0471, 0.0440, 0.4335,\n                      0.0606, 0.1312, 0.3903, 0.0739, 0.0677, 0.0643, 0.0390, 0.0573, 0.0625,\n                      0.0878, 0.2702, 0.3342, 0.0460, 0.0522, 0.0555, 0.1709, 0.0966, 0.1575,\n                      0.1045, 0.1266, 0.0930, 0.2386, 0.0285, 0.2347, 0.1209, 0.0746, 0.0874,\n                      0.1257, 0.0373, 0.0993, 0.2182, 0.0511, 0.1059, 0.2481, 0.1865, 0.0546,\n                      0.0436, 0.2140, 0.5082, 0.0228, 0.3917, 0.2262, 0.1695, 0.3180, 0.0868,\n                      0.1791, 0.0536, 0.1330, 0.0713, 0.6299, 0.0256, 0.3738, 0.2711, 0.0536,\n                      0.3117], device='cuda:0')),\n             ('block2.5.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block2.6.weight',\n              tensor([[[[ 6.2955e-03,  3.3858e-02,  1.3050e-02],\n                        [-1.5362e-02, -1.3803e-02,  2.4967e-02],\n                        [ 2.8229e-02, -5.8657e-03,  1.8871e-02]],\n              \n                       [[ 1.3128e-02,  1.4176e-02, -1.4931e-02],\n                        [ 1.1624e-02, -3.6953e-02, -3.1098e-02],\n                        [-1.7206e-02, -1.1158e-02,  4.7119e-03]],\n              \n                       [[-2.2195e-02,  4.1547e-02,  3.0332e-02],\n                        [-1.3341e-02, -8.9437e-03, -2.5498e-02],\n                        [ 4.1129e-02, -2.5725e-02,  1.8311e-02]],\n              \n                       ...,\n              \n                       [[ 2.6294e-02,  1.3931e-02, -3.6983e-02],\n                        [-2.7607e-02, -2.6922e-02,  3.8271e-02],\n                        [-3.0903e-02,  1.9333e-02, -2.3345e-03]],\n              \n                       [[ 4.5663e-03,  4.0765e-03,  1.4716e-02],\n                        [ 1.0679e-02, -2.8576e-02,  3.7099e-03],\n                        [-4.1185e-02,  2.0388e-02,  3.6427e-02]],\n              \n                       [[ 1.3574e-02,  3.7450e-02, -8.2439e-03],\n                        [ 1.1332e-02, -3.2343e-02, -3.3513e-03],\n                        [ 7.8770e-03, -4.0343e-02,  3.7119e-02]]],\n              \n              \n                      [[[ 3.0888e-02,  1.7724e-02,  1.4303e-02],\n                        [ 2.0240e-02, -1.7538e-02,  1.0186e-02],\n                        [ 3.3791e-02,  8.4714e-03, -7.2942e-04]],\n              \n                       [[ 2.4124e-02,  3.6796e-02, -3.4895e-03],\n                        [ 2.4801e-02,  2.1102e-02, -2.3378e-02],\n                        [ 1.7747e-02,  2.2740e-02,  2.4224e-02]],\n              \n                       [[ 2.2067e-02,  3.6316e-02,  2.8329e-02],\n                        [ 2.6618e-02, -1.0443e-02,  2.8936e-02],\n                        [-1.7541e-02, -3.5956e-02, -2.3423e-02]],\n              \n                       ...,\n              \n                       [[ 1.7106e-02, -6.6356e-03, -2.8779e-02],\n                        [ 3.6358e-02,  2.9142e-02,  3.3993e-02],\n                        [-2.5002e-02,  2.4095e-02, -9.3665e-04]],\n              \n                       [[-3.3526e-02,  3.0242e-02,  3.1154e-02],\n                        [-1.5545e-02,  8.1118e-03, -3.7380e-02],\n                        [ 3.0032e-02,  5.5028e-03, -1.7583e-02]],\n              \n                       [[-2.0923e-02, -2.7769e-02, -5.9668e-03],\n                        [ 2.7622e-02,  1.4042e-02, -6.3629e-03],\n                        [-1.3344e-02, -3.2022e-02, -5.8401e-03]]],\n              \n              \n                      [[[ 3.2320e-02, -6.4175e-03, -9.0188e-03],\n                        [-2.1925e-02, -2.5021e-02, -3.8661e-02],\n                        [-3.2271e-02, -2.1419e-02,  3.7542e-02]],\n              \n                       [[-4.0181e-04, -4.6250e-03,  8.8601e-03],\n                        [-1.9242e-02, -1.6308e-02,  3.6808e-03],\n                        [ 1.7592e-02,  1.7621e-02, -4.0144e-02]],\n              \n                       [[ 3.4978e-02, -9.8190e-03,  3.0591e-02],\n                        [-3.0620e-02,  2.9637e-02,  3.7175e-02],\n                        [ 4.1058e-02, -3.9450e-02, -4.2040e-02]],\n              \n                       ...,\n              \n                       [[-3.2681e-02, -1.1185e-02,  2.2051e-02],\n                        [ 8.4153e-03,  7.0513e-03,  8.5787e-03],\n                        [ 1.4324e-02, -2.6527e-02, -1.0898e-02]],\n              \n                       [[ 1.7923e-02,  2.0836e-02,  8.7425e-03],\n                        [-2.8725e-03,  3.8537e-03,  2.2468e-02],\n                        [ 3.5354e-02,  3.0863e-02,  2.4324e-02]],\n              \n                       [[-8.0952e-03, -2.3568e-02, -1.0948e-02],\n                        [ 3.6615e-02,  7.8174e-03, -2.2172e-02],\n                        [ 2.0524e-02,  3.3955e-02,  2.7164e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.7968e-03,  1.6655e-02,  1.7572e-02],\n                        [ 1.5487e-02, -3.9018e-03, -1.6681e-02],\n                        [ 3.7846e-02,  4.1513e-02,  2.6899e-02]],\n              \n                       [[-3.7588e-02, -3.7914e-02,  1.0061e-02],\n                        [-2.0674e-02, -6.4667e-04,  2.3124e-02],\n                        [-6.3906e-03,  1.9168e-02, -1.9638e-02]],\n              \n                       [[ 3.5504e-02, -4.1828e-02,  1.8608e-02],\n                        [-3.2531e-02, -3.5482e-02,  3.9012e-02],\n                        [-1.3288e-02, -1.3637e-02,  5.7497e-03]],\n              \n                       ...,\n              \n                       [[ 5.7329e-03,  1.8019e-02, -1.0924e-03],\n                        [-1.9197e-04, -3.4835e-03, -3.9484e-02],\n                        [-3.7117e-02, -1.3422e-02,  2.0656e-02]],\n              \n                       [[-3.5020e-03,  1.9569e-02, -2.5956e-02],\n                        [ 2.8184e-02,  3.1158e-02,  3.3269e-02],\n                        [-3.3272e-02, -2.8298e-02, -1.2363e-02]],\n              \n                       [[-8.5205e-04, -1.2680e-02, -2.9326e-02],\n                        [ 2.3340e-02,  2.6161e-02, -3.9515e-02],\n                        [-3.4112e-02,  1.7072e-02, -9.5886e-03]]],\n              \n              \n                      [[[ 3.0878e-02,  1.4878e-02, -1.8302e-02],\n                        [-9.5677e-03, -8.0352e-03,  1.0966e-02],\n                        [ 9.5049e-03,  1.8167e-02, -3.6475e-02]],\n              \n                       [[ 9.1301e-03,  2.4228e-02, -8.0282e-03],\n                        [-2.6670e-02,  2.3970e-02, -1.4194e-02],\n                        [-2.3678e-03, -1.7767e-02, -1.2052e-02]],\n              \n                       [[ 1.6051e-02, -4.1044e-02, -1.7556e-02],\n                        [-3.5415e-02, -3.2837e-02, -7.4279e-03],\n                        [ 3.6395e-02, -2.6841e-02,  1.6280e-02]],\n              \n                       ...,\n              \n                       [[-3.1272e-02,  2.7006e-02, -2.9524e-02],\n                        [-3.6978e-02,  2.2968e-02, -1.9068e-02],\n                        [-4.0036e-02, -6.3220e-03, -3.2856e-02]],\n              \n                       [[ 3.9464e-02,  2.1884e-02, -1.9923e-02],\n                        [-2.5044e-02, -7.4038e-03, -2.0609e-02],\n                        [-1.5884e-02,  2.4338e-02,  3.6080e-02]],\n              \n                       [[-1.4576e-02,  1.2215e-04, -2.3225e-02],\n                        [-2.6085e-02,  1.3990e-02, -2.6344e-02],\n                        [-3.2142e-02,  1.3082e-02, -2.8147e-02]]],\n              \n              \n                      [[[ 2.4403e-02,  3.4178e-02, -7.2685e-03],\n                        [-2.6458e-03, -2.4418e-02,  1.6194e-02],\n                        [-5.7482e-03,  2.5278e-02, -1.4374e-02]],\n              \n                       [[ 2.5847e-05, -2.9287e-02, -2.1654e-02],\n                        [ 3.1341e-02, -1.1038e-02, -1.3977e-04],\n                        [ 2.3685e-02, -2.4355e-02,  3.5140e-02]],\n              \n                       [[ 1.0649e-02, -2.9610e-02,  3.0118e-02],\n                        [-6.7988e-04,  4.9043e-03,  1.4622e-02],\n                        [-2.9904e-02, -2.5209e-02,  2.3032e-02]],\n              \n                       ...,\n              \n                       [[-2.2281e-02,  3.3436e-02,  4.5954e-03],\n                        [ 1.2173e-02, -8.3976e-03, -1.4952e-02],\n                        [ 3.5927e-02,  2.9467e-02,  3.4829e-02]],\n              \n                       [[ 1.7596e-04, -2.1636e-02, -2.8042e-03],\n                        [ 3.6401e-02, -1.3553e-02, -9.7237e-03],\n                        [ 3.2018e-02,  2.3381e-02, -2.2913e-02]],\n              \n                       [[ 2.5208e-02,  1.3372e-02, -4.0816e-03],\n                        [-2.1135e-03,  2.0517e-02,  3.2051e-02],\n                        [-1.7007e-02,  2.4472e-02, -3.1620e-02]]]], device='cuda:0')),\n             ('block2.6.bias',\n              tensor([ 0.0351, -0.0412, -0.0137,  0.0232, -0.0018,  0.0133,  0.0341,  0.0048,\n                       0.0040,  0.0080, -0.0173, -0.0289,  0.0232,  0.0026, -0.0261, -0.0058,\n                       0.0328, -0.0405, -0.0247, -0.0024, -0.0170, -0.0134,  0.0157,  0.0021,\n                       0.0197, -0.0169, -0.0077,  0.0037,  0.0217,  0.0392,  0.0221,  0.0134,\n                      -0.0236,  0.0270, -0.0161,  0.0125,  0.0379, -0.0236,  0.0024,  0.0374,\n                      -0.0036, -0.0021, -0.0391,  0.0344, -0.0162,  0.0086, -0.0133, -0.0359,\n                      -0.0269, -0.0124,  0.0117,  0.0317,  0.0175,  0.0083, -0.0187, -0.0231,\n                      -0.0091,  0.0292, -0.0203, -0.0322,  0.0309,  0.0184, -0.0236, -0.0012],\n                     device='cuda:0')),\n             ('block2.8.weight',\n              tensor([1.0004, 0.9996, 1.0004, 0.9998, 0.9999, 1.0004, 0.9989, 1.0001, 0.9996,\n                      1.0004, 0.9992, 1.0004, 1.0001, 1.0000, 0.9989, 1.0009, 1.0007, 1.0013,\n                      1.0006, 0.9998, 1.0001, 0.9998, 1.0000, 1.0006, 1.0002, 0.9995, 1.0002,\n                      1.0001, 0.9993, 0.9998, 1.0000, 1.0006, 0.9992, 1.0005, 1.0006, 1.0001,\n                      0.9983, 1.0002, 1.0007, 0.9994, 0.9994, 1.0006, 1.0000, 1.0003, 0.9997,\n                      0.9992, 1.0003, 1.0000, 1.0012, 0.9998, 0.9996, 0.9997, 0.9996, 1.0005,\n                      1.0004, 1.0000, 1.0011, 1.0005, 0.9998, 0.9987, 0.9997, 0.9999, 1.0004,\n                      0.9989], device='cuda:0')),\n             ('block2.8.bias',\n              tensor([ 1.5402e-05,  2.0868e-04,  3.6630e-05, -4.0308e-04, -3.4848e-04,\n                       1.3122e-04,  9.9128e-05, -5.1270e-05, -1.3294e-04,  1.0041e-04,\n                      -1.8494e-04,  1.2254e-04,  2.0648e-05,  3.0974e-04,  4.7441e-04,\n                       1.5859e-04, -1.7387e-04, -3.1359e-06,  1.2379e-04, -5.1671e-05,\n                       2.0504e-04,  1.3608e-04, -1.9607e-05,  5.3487e-04,  1.1065e-04,\n                      -1.2687e-04,  3.4545e-04,  3.5116e-04,  2.8388e-04,  4.3920e-06,\n                      -1.2596e-04,  3.5651e-04,  1.2523e-04, -7.4191e-06,  1.3367e-04,\n                       6.7666e-05, -6.6629e-05, -3.0917e-04,  4.0964e-04,  3.9235e-05,\n                      -3.5058e-04,  2.7187e-04, -5.7771e-05,  3.6496e-04,  1.1316e-04,\n                      -8.4266e-05,  1.3535e-04, -2.6626e-04,  1.1095e-05,  1.0532e-05,\n                       5.5702e-05, -1.4055e-04,  8.5264e-05, -3.5033e-05,  8.8387e-05,\n                      -5.3231e-04, -6.5211e-05, -5.3348e-04,  3.7881e-04,  1.3855e-05,\n                       1.4547e-04, -1.3546e-05, -8.1391e-04,  4.8419e-05], device='cuda:0')),\n             ('block2.8.running_mean',\n              tensor([0.3063, 0.2605, 0.3208, 0.2981, 0.1371, 0.2119, 0.2603, 0.3020, 0.2678,\n                      0.2126, 0.3185, 0.3537, 0.1855, 0.1580, 0.3397, 0.2538, 0.1330, 0.1499,\n                      0.1737, 0.2459, 0.4204, 0.3177, 0.3284, 0.2525, 0.2883, 0.2449, 0.2823,\n                      0.1581, 0.3775, 0.2962, 0.2256, 0.2883, 0.3555, 0.3968, 0.2863, 0.2385,\n                      0.1838, 0.2444, 0.1694, 0.2152, 0.2719, 0.1820, 0.2746, 0.4090, 0.1577,\n                      0.2565, 0.2623, 0.2639, 0.1860, 0.2157, 0.1516, 0.1627, 0.2779, 0.3131,\n                      0.1925, 0.1314, 0.2540, 0.2148, 0.2162, 0.1725, 0.2259, 0.2395, 0.1980,\n                      0.2541], device='cuda:0')),\n             ('block2.8.running_var',\n              tensor([0.1446, 0.1675, 0.1417, 0.1628, 0.0674, 0.0647, 0.1068, 0.1887, 0.1032,\n                      0.1587, 0.1548, 0.2827, 0.1249, 0.0424, 0.1568, 0.1226, 0.0440, 0.0454,\n                      0.0788, 0.0769, 0.3971, 0.1083, 0.2070, 0.0977, 0.2169, 0.1293, 0.1383,\n                      0.0409, 0.2637, 0.1067, 0.0643, 0.1238, 0.1712, 0.3676, 0.1342, 0.0795,\n                      0.0880, 0.1134, 0.0478, 0.0829, 0.0986, 0.0956, 0.0829, 0.4875, 0.0594,\n                      0.1555, 0.1101, 0.1413, 0.0762, 0.1395, 0.0759, 0.0940, 0.2069, 0.1726,\n                      0.0799, 0.0583, 0.0993, 0.1359, 0.1731, 0.0671, 0.1015, 0.1134, 0.0660,\n                      0.2250], device='cuda:0')),\n             ('block2.8.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block3.0.weight',\n              tensor([[[[ 0.0009,  0.0408,  0.0004],\n                        [-0.0019, -0.0359, -0.0344],\n                        [-0.0079,  0.0303,  0.0197]],\n              \n                       [[ 0.0268, -0.0355, -0.0280],\n                        [-0.0153,  0.0032, -0.0246],\n                        [ 0.0378,  0.0360, -0.0236]],\n              \n                       [[-0.0011,  0.0395, -0.0291],\n                        [-0.0200,  0.0213, -0.0121],\n                        [ 0.0217, -0.0129,  0.0282]],\n              \n                       ...,\n              \n                       [[ 0.0197,  0.0163, -0.0073],\n                        [-0.0053,  0.0415, -0.0388],\n                        [-0.0254,  0.0305, -0.0133]],\n              \n                       [[ 0.0043,  0.0377, -0.0401],\n                        [-0.0357,  0.0166,  0.0214],\n                        [ 0.0300, -0.0120, -0.0155]],\n              \n                       [[ 0.0171, -0.0064,  0.0108],\n                        [-0.0086, -0.0093, -0.0232],\n                        [ 0.0328,  0.0286,  0.0030]]],\n              \n              \n                      [[[-0.0146, -0.0409, -0.0272],\n                        [ 0.0072,  0.0138, -0.0237],\n                        [ 0.0413,  0.0394, -0.0218]],\n              \n                       [[ 0.0186, -0.0196, -0.0331],\n                        [-0.0300,  0.0089, -0.0026],\n                        [-0.0247,  0.0075, -0.0018]],\n              \n                       [[-0.0236, -0.0327, -0.0190],\n                        [-0.0418,  0.0096, -0.0321],\n                        [-0.0311,  0.0336, -0.0413]],\n              \n                       ...,\n              \n                       [[ 0.0019, -0.0354, -0.0220],\n                        [ 0.0147,  0.0148, -0.0294],\n                        [-0.0350, -0.0388, -0.0331]],\n              \n                       [[ 0.0076, -0.0380,  0.0259],\n                        [-0.0125,  0.0255,  0.0195],\n                        [ 0.0030,  0.0046,  0.0331]],\n              \n                       [[ 0.0047,  0.0160,  0.0060],\n                        [-0.0280,  0.0015,  0.0366],\n                        [-0.0134, -0.0015, -0.0219]]],\n              \n              \n                      [[[-0.0131, -0.0060, -0.0142],\n                        [-0.0276, -0.0370, -0.0323],\n                        [-0.0164,  0.0410, -0.0287]],\n              \n                       [[-0.0122,  0.0160,  0.0380],\n                        [-0.0362,  0.0222, -0.0398],\n                        [ 0.0142,  0.0343, -0.0177]],\n              \n                       [[ 0.0390,  0.0390, -0.0174],\n                        [-0.0313, -0.0295,  0.0200],\n                        [ 0.0416, -0.0159,  0.0037]],\n              \n                       ...,\n              \n                       [[ 0.0325, -0.0255, -0.0344],\n                        [-0.0089, -0.0017,  0.0038],\n                        [-0.0125, -0.0414,  0.0187]],\n              \n                       [[ 0.0047,  0.0404, -0.0230],\n                        [-0.0172, -0.0144, -0.0159],\n                        [ 0.0129, -0.0183, -0.0388]],\n              \n                       [[ 0.0346,  0.0054,  0.0374],\n                        [-0.0190, -0.0318,  0.0387],\n                        [-0.0322,  0.0073, -0.0302]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0404,  0.0413,  0.0400],\n                        [ 0.0365,  0.0135,  0.0393],\n                        [-0.0417,  0.0049, -0.0195]],\n              \n                       [[-0.0167,  0.0398,  0.0207],\n                        [ 0.0267,  0.0400,  0.0048],\n                        [-0.0074,  0.0049, -0.0099]],\n              \n                       [[ 0.0361, -0.0368,  0.0283],\n                        [-0.0364,  0.0188,  0.0083],\n                        [ 0.0305,  0.0011, -0.0386]],\n              \n                       ...,\n              \n                       [[-0.0231,  0.0118,  0.0055],\n                        [ 0.0379,  0.0365, -0.0103],\n                        [-0.0101, -0.0123,  0.0368]],\n              \n                       [[ 0.0413, -0.0293, -0.0213],\n                        [-0.0013, -0.0387, -0.0389],\n                        [-0.0142,  0.0324, -0.0382]],\n              \n                       [[ 0.0349, -0.0251, -0.0013],\n                        [ 0.0327, -0.0237, -0.0245],\n                        [-0.0276,  0.0199, -0.0190]]],\n              \n              \n                      [[[-0.0213, -0.0216, -0.0236],\n                        [ 0.0401,  0.0106, -0.0118],\n                        [ 0.0148, -0.0414, -0.0146]],\n              \n                       [[ 0.0375, -0.0229,  0.0352],\n                        [-0.0095,  0.0238,  0.0341],\n                        [ 0.0384, -0.0397,  0.0135]],\n              \n                       [[-0.0071, -0.0146,  0.0258],\n                        [-0.0195,  0.0237, -0.0305],\n                        [ 0.0124, -0.0041,  0.0149]],\n              \n                       ...,\n              \n                       [[ 0.0336,  0.0134, -0.0016],\n                        [-0.0334,  0.0044,  0.0169],\n                        [ 0.0408,  0.0306, -0.0391]],\n              \n                       [[ 0.0362, -0.0366, -0.0393],\n                        [-0.0140, -0.0172, -0.0044],\n                        [-0.0120,  0.0121, -0.0394]],\n              \n                       [[-0.0307, -0.0120, -0.0107],\n                        [-0.0342,  0.0081,  0.0309],\n                        [ 0.0187, -0.0248, -0.0082]]],\n              \n              \n                      [[[ 0.0346,  0.0139,  0.0317],\n                        [-0.0185,  0.0127,  0.0238],\n                        [ 0.0392, -0.0396, -0.0292]],\n              \n                       [[ 0.0113,  0.0396, -0.0292],\n                        [-0.0004, -0.0046,  0.0178],\n                        [ 0.0238,  0.0266,  0.0292]],\n              \n                       [[ 0.0355,  0.0031,  0.0401],\n                        [-0.0191, -0.0114,  0.0165],\n                        [-0.0071,  0.0315,  0.0049]],\n              \n                       ...,\n              \n                       [[-0.0224,  0.0299, -0.0346],\n                        [ 0.0241,  0.0012, -0.0328],\n                        [-0.0015,  0.0116, -0.0403]],\n              \n                       [[-0.0068, -0.0208, -0.0149],\n                        [-0.0070,  0.0271, -0.0187],\n                        [ 0.0285, -0.0295, -0.0105]],\n              \n                       [[ 0.0337, -0.0378, -0.0338],\n                        [ 0.0208, -0.0150, -0.0366],\n                        [-0.0091, -0.0319, -0.0090]]]], device='cuda:0')),\n             ('block3.0.bias',\n              tensor([-0.0292, -0.0004, -0.0350,  0.0016,  0.0320, -0.0293, -0.0225, -0.0288,\n                       0.0059, -0.0248, -0.0301, -0.0186,  0.0354, -0.0288, -0.0113,  0.0274,\n                      -0.0241, -0.0005, -0.0007,  0.0292,  0.0025,  0.0346,  0.0074,  0.0142,\n                       0.0090,  0.0356,  0.0396,  0.0040,  0.0197,  0.0005,  0.0099, -0.0323,\n                      -0.0128, -0.0254,  0.0096, -0.0014,  0.0157, -0.0044,  0.0033, -0.0100,\n                       0.0395,  0.0117,  0.0197,  0.0340, -0.0372,  0.0269,  0.0271, -0.0180,\n                      -0.0011, -0.0161, -0.0234,  0.0031, -0.0340,  0.0153, -0.0223, -0.0187,\n                      -0.0379, -0.0175,  0.0075, -0.0174, -0.0031,  0.0024, -0.0345,  0.0330,\n                      -0.0159,  0.0178, -0.0274,  0.0241, -0.0041, -0.0387,  0.0349, -0.0141,\n                       0.0222,  0.0125, -0.0095, -0.0109,  0.0314,  0.0024, -0.0218, -0.0327,\n                       0.0363, -0.0412, -0.0291,  0.0412,  0.0041,  0.0214,  0.0287, -0.0410,\n                       0.0341, -0.0206, -0.0038,  0.0182, -0.0145,  0.0258,  0.0240, -0.0060,\n                      -0.0198,  0.0111, -0.0016,  0.0322,  0.0331, -0.0136, -0.0415, -0.0197,\n                      -0.0270, -0.0403, -0.0214, -0.0097,  0.0409,  0.0385, -0.0012,  0.0214,\n                       0.0015,  0.0158, -0.0049, -0.0097,  0.0037, -0.0306, -0.0005, -0.0020,\n                       0.0315, -0.0126, -0.0238,  0.0018,  0.0104,  0.0166, -0.0031,  0.0340],\n                     device='cuda:0')),\n             ('block3.2.weight',\n              tensor([1.0002, 1.0004, 1.0000, 0.9998, 1.0000, 1.0002, 0.9998, 1.0002, 1.0005,\n                      0.9995, 1.0003, 0.9992, 1.0002, 1.0000, 0.9998, 1.0002, 1.0001, 1.0006,\n                      0.9999, 0.9993, 1.0000, 0.9997, 1.0004, 0.9995, 1.0007, 1.0000, 1.0003,\n                      0.9997, 0.9998, 0.9994, 1.0002, 0.9996, 1.0002, 0.9997, 1.0003, 0.9998,\n                      0.9999, 0.9999, 1.0003, 0.9999, 1.0003, 1.0001, 0.9999, 1.0000, 1.0000,\n                      0.9994, 1.0007, 0.9998, 1.0001, 1.0004, 0.9997, 1.0000, 1.0000, 1.0001,\n                      0.9994, 1.0000, 0.9998, 0.9999, 0.9997, 0.9999, 1.0002, 0.9999, 1.0004,\n                      0.9998, 1.0002, 1.0004, 1.0003, 1.0003, 1.0002, 1.0003, 0.9999, 0.9997,\n                      0.9995, 1.0000, 1.0003, 1.0002, 0.9997, 0.9998, 1.0001, 0.9995, 0.9999,\n                      1.0000, 0.9999, 1.0006, 1.0002, 0.9996, 0.9999, 0.9996, 1.0000, 1.0004,\n                      0.9997, 0.9996, 0.9998, 0.9995, 1.0006, 1.0005, 0.9999, 0.9998, 0.9996,\n                      1.0003, 0.9994, 1.0004, 0.9997, 1.0006, 0.9995, 0.9998, 0.9999, 1.0001,\n                      0.9998, 1.0002, 1.0002, 0.9997, 1.0002, 1.0000, 0.9997, 1.0002, 1.0002,\n                      1.0003, 1.0001, 1.0007, 1.0002, 1.0001, 1.0000, 0.9997, 1.0002, 1.0001,\n                      1.0001, 0.9999], device='cuda:0')),\n             ('block3.2.bias',\n              tensor([-2.0416e-05,  3.0143e-04,  1.7818e-05,  2.6168e-04,  1.2909e-04,\n                       2.7146e-04,  1.8337e-04,  1.2085e-04,  1.9393e-04, -3.6325e-06,\n                       3.0961e-04,  2.8277e-04,  1.8629e-04,  5.1375e-05,  5.6298e-05,\n                       2.6411e-04, -1.1041e-04, -3.3196e-04, -1.6240e-05, -1.3682e-04,\n                      -2.2012e-05, -2.6137e-04, -1.4515e-04, -3.3386e-04,  2.5525e-04,\n                      -1.4809e-04,  1.5718e-04,  8.0119e-05,  1.1746e-04, -1.7474e-05,\n                       1.1771e-04,  2.9816e-04, -4.4646e-04, -5.4870e-05,  4.1333e-06,\n                       1.1760e-04,  6.3307e-05, -3.1442e-05, -1.0230e-04,  9.3489e-05,\n                      -6.1031e-05,  1.6198e-04,  5.0011e-05, -1.5583e-04,  8.9010e-05,\n                       2.6539e-04, -2.9768e-04,  2.0343e-05, -1.4731e-04,  4.3761e-05,\n                      -1.5672e-05,  1.1679e-05, -1.0789e-04,  2.2914e-04, -3.4722e-04,\n                      -7.6716e-05,  1.2685e-04,  2.2284e-04, -1.2088e-04, -1.2906e-04,\n                       4.6419e-04,  2.9945e-04, -1.3548e-04, -1.0304e-04, -9.3732e-05,\n                      -1.0725e-04,  2.1176e-05, -2.0232e-04, -9.1704e-05,  1.0219e-04,\n                       2.5151e-04,  2.2029e-04,  5.3019e-05,  6.5964e-05, -4.3554e-05,\n                      -5.8716e-05,  2.5230e-05, -1.8800e-04,  2.6642e-04, -2.4231e-04,\n                       1.9504e-04,  8.4643e-08,  1.1016e-04,  9.4272e-05, -9.4755e-05,\n                      -2.6253e-05, -1.5098e-04,  5.6406e-05,  7.0890e-05,  1.5880e-04,\n                       3.6225e-06, -9.9439e-07, -9.5004e-06,  9.7760e-05,  6.5455e-05,\n                       2.0429e-04, -2.9271e-05,  8.9336e-05,  6.9746e-05, -5.8765e-05,\n                       1.4818e-04, -1.4545e-04,  2.1318e-04,  2.8771e-05, -7.0608e-05,\n                       1.8521e-04,  1.5909e-04,  7.4731e-05, -3.0222e-04, -1.2136e-05,\n                       9.3649e-05, -9.1369e-05, -2.6724e-04,  2.1518e-04,  9.7531e-05,\n                       3.1714e-04, -6.4267e-05, -1.9963e-04,  2.1691e-04, -1.4182e-04,\n                      -1.8746e-04, -2.2361e-04,  2.6265e-04,  6.0981e-05,  2.9906e-05,\n                      -3.2948e-05, -1.6991e-04,  4.0407e-05], device='cuda:0')),\n             ('block3.2.running_mean',\n              tensor([0.2400, 0.3150, 0.4037, 0.1509, 0.1787, 0.3561, 0.2383, 0.2281, 0.2904,\n                      0.3741, 0.2879, 0.4010, 0.3013, 0.2764, 0.1502, 0.3636, 0.2539, 0.2319,\n                      0.2532, 0.1703, 0.1624, 0.1973, 0.2330, 0.3457, 0.1639, 0.3219, 0.2381,\n                      0.1303, 0.2395, 0.2034, 0.2904, 0.3021, 0.1363, 0.2738, 0.2091, 0.2760,\n                      0.1840, 0.2567, 0.3205, 0.3015, 0.2717, 0.2040, 0.2213, 0.1900, 0.2022,\n                      0.2472, 0.4016, 0.2220, 0.3225, 0.1957, 0.2770, 0.2547, 0.1957, 0.4710,\n                      0.1060, 0.3114, 0.2359, 0.1539, 0.2390, 0.5441, 0.2223, 0.2315, 0.1285,\n                      0.2708, 0.2252, 0.3351, 0.2221, 0.2344, 0.2431, 0.1688, 0.2227, 0.1273,\n                      0.3489, 0.4059, 0.2613, 0.1160, 0.1832, 0.2288, 0.2523, 0.1718, 0.3667,\n                      0.2523, 0.1216, 0.3256, 0.2029, 0.1799, 0.4156, 0.1730, 0.4311, 0.5040,\n                      0.2720, 0.1682, 0.2986, 0.3868, 0.3129, 0.4337, 0.3445, 0.2956, 0.3514,\n                      0.1978, 0.2979, 0.3536, 0.2632, 0.1996, 0.2365, 0.1609, 0.2339, 0.1586,\n                      0.2774, 0.2542, 0.3007, 0.1954, 0.2588, 0.3118, 0.2746, 0.3192, 0.2324,\n                      0.3875, 0.3557, 0.2100, 0.2587, 0.1170, 0.2041, 0.3013, 0.2437, 0.4901,\n                      0.2038, 0.3103], device='cuda:0')),\n             ('block3.2.running_var',\n              tensor([0.0957, 0.2394, 0.3414, 0.0779, 0.0657, 0.2899, 0.1548, 0.1236, 0.1576,\n                      0.2956, 0.2653, 0.2732, 0.1395, 0.2969, 0.0843, 0.1682, 0.1653, 0.1051,\n                      0.1405, 0.0614, 0.1043, 0.1137, 0.1655, 0.2463, 0.0843, 0.2183, 0.1156,\n                      0.0676, 0.0853, 0.1283, 0.2102, 0.2173, 0.1133, 0.0852, 0.0891, 0.1718,\n                      0.0732, 0.1911, 0.2226, 0.2234, 0.1910, 0.1629, 0.1703, 0.0609, 0.0803,\n                      0.0998, 0.1838, 0.1792, 0.1304, 0.0743, 0.1396, 0.1166, 0.1223, 0.3129,\n                      0.0657, 0.1943, 0.1264, 0.0624, 0.1458, 0.3185, 0.0927, 0.1631, 0.0712,\n                      0.0947, 0.0918, 0.1919, 0.1136, 0.0742, 0.1128, 0.0663, 0.0783, 0.0307,\n                      0.2353, 0.2487, 0.1120, 0.0315, 0.0750, 0.0957, 0.1339, 0.0742, 0.4539,\n                      0.1266, 0.0630, 0.2216, 0.0824, 0.0593, 0.4804, 0.1364, 0.2763, 0.4817,\n                      0.1699, 0.0520, 0.1549, 0.1875, 0.2226, 0.2394, 0.2215, 0.1421, 0.1639,\n                      0.0938, 0.2847, 0.2474, 0.1752, 0.1034, 0.1273, 0.0769, 0.0888, 0.0587,\n                      0.1973, 0.2162, 0.1337, 0.1137, 0.1425, 0.1593, 0.2138, 0.3000, 0.1246,\n                      0.2674, 0.3325, 0.0904, 0.1250, 0.0284, 0.0908, 0.2186, 0.1299, 0.4712,\n                      0.0684, 0.1234], device='cuda:0')),\n             ('block3.2.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block3.3.weight',\n              tensor([[[[-0.0131,  0.0149, -0.0094],\n                        [ 0.0197,  0.0037, -0.0164],\n                        [ 0.0047, -0.0249,  0.0077]],\n              \n                       [[ 0.0069,  0.0038, -0.0032],\n                        [ 0.0043, -0.0126, -0.0041],\n                        [-0.0129, -0.0208, -0.0065]],\n              \n                       [[-0.0280, -0.0128,  0.0111],\n                        [ 0.0223,  0.0148, -0.0048],\n                        [-0.0119,  0.0022, -0.0043]],\n              \n                       ...,\n              \n                       [[ 0.0152,  0.0120, -0.0052],\n                        [-0.0002,  0.0014, -0.0120],\n                        [ 0.0287, -0.0087, -0.0187]],\n              \n                       [[ 0.0248,  0.0204, -0.0190],\n                        [-0.0112, -0.0205, -0.0275],\n                        [ 0.0264,  0.0035, -0.0220]],\n              \n                       [[ 0.0285,  0.0182,  0.0251],\n                        [-0.0193, -0.0184,  0.0176],\n                        [ 0.0282, -0.0130,  0.0030]]],\n              \n              \n                      [[[-0.0129,  0.0175,  0.0215],\n                        [ 0.0174,  0.0144,  0.0023],\n                        [ 0.0129,  0.0255,  0.0278]],\n              \n                       [[-0.0218, -0.0061,  0.0033],\n                        [ 0.0257,  0.0149,  0.0119],\n                        [-0.0014, -0.0302,  0.0027]],\n              \n                       [[ 0.0112, -0.0306, -0.0194],\n                        [ 0.0114,  0.0158, -0.0272],\n                        [-0.0238, -0.0260, -0.0275]],\n              \n                       ...,\n              \n                       [[ 0.0234, -0.0101,  0.0195],\n                        [-0.0011,  0.0189, -0.0038],\n                        [ 0.0132,  0.0207, -0.0297]],\n              \n                       [[ 0.0259, -0.0289,  0.0189],\n                        [ 0.0165,  0.0177,  0.0169],\n                        [-0.0025,  0.0200, -0.0046]],\n              \n                       [[-0.0291,  0.0265,  0.0005],\n                        [-0.0232,  0.0196, -0.0143],\n                        [-0.0065,  0.0237,  0.0260]]],\n              \n              \n                      [[[-0.0204, -0.0107, -0.0042],\n                        [ 0.0155, -0.0156,  0.0190],\n                        [ 0.0189,  0.0130,  0.0042]],\n              \n                       [[-0.0192,  0.0168,  0.0072],\n                        [ 0.0234, -0.0247,  0.0161],\n                        [ 0.0103,  0.0020, -0.0266]],\n              \n                       [[-0.0150,  0.0033, -0.0115],\n                        [ 0.0025, -0.0103,  0.0231],\n                        [ 0.0223,  0.0185,  0.0118]],\n              \n                       ...,\n              \n                       [[ 0.0071, -0.0133, -0.0225],\n                        [ 0.0239,  0.0209, -0.0211],\n                        [-0.0162,  0.0197,  0.0033]],\n              \n                       [[-0.0024,  0.0032,  0.0137],\n                        [ 0.0153,  0.0237, -0.0217],\n                        [ 0.0275, -0.0268,  0.0124]],\n              \n                       [[ 0.0265,  0.0070,  0.0138],\n                        [ 0.0227,  0.0185, -0.0258],\n                        [ 0.0039,  0.0277, -0.0007]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0104,  0.0010, -0.0141],\n                        [ 0.0180, -0.0139,  0.0161],\n                        [ 0.0162,  0.0022,  0.0282]],\n              \n                       [[ 0.0059,  0.0157,  0.0184],\n                        [ 0.0210, -0.0217, -0.0019],\n                        [-0.0263, -0.0240, -0.0043]],\n              \n                       [[ 0.0050, -0.0089,  0.0156],\n                        [ 0.0206, -0.0060,  0.0239],\n                        [ 0.0116,  0.0128, -0.0111]],\n              \n                       ...,\n              \n                       [[-0.0288, -0.0248,  0.0289],\n                        [ 0.0048, -0.0036, -0.0173],\n                        [ 0.0122, -0.0075,  0.0101]],\n              \n                       [[ 0.0210, -0.0204, -0.0072],\n                        [-0.0187,  0.0183,  0.0043],\n                        [-0.0289, -0.0247,  0.0286]],\n              \n                       [[ 0.0055, -0.0215, -0.0069],\n                        [ 0.0108,  0.0223, -0.0158],\n                        [ 0.0250,  0.0050, -0.0084]]],\n              \n              \n                      [[[-0.0064,  0.0188, -0.0179],\n                        [-0.0068, -0.0073, -0.0103],\n                        [-0.0268, -0.0258, -0.0232]],\n              \n                       [[ 0.0254, -0.0124, -0.0070],\n                        [ 0.0210,  0.0202, -0.0247],\n                        [-0.0037, -0.0132, -0.0118]],\n              \n                       [[-0.0168,  0.0068,  0.0111],\n                        [-0.0182, -0.0164, -0.0030],\n                        [ 0.0266,  0.0155,  0.0039]],\n              \n                       ...,\n              \n                       [[-0.0061,  0.0265,  0.0167],\n                        [ 0.0040,  0.0181, -0.0264],\n                        [ 0.0131, -0.0169,  0.0099]],\n              \n                       [[-0.0197, -0.0118,  0.0091],\n                        [-0.0257,  0.0043,  0.0205],\n                        [ 0.0247,  0.0228,  0.0285]],\n              \n                       [[ 0.0008, -0.0011, -0.0225],\n                        [ 0.0231,  0.0075,  0.0064],\n                        [ 0.0290,  0.0246, -0.0085]]],\n              \n              \n                      [[[-0.0206,  0.0159, -0.0251],\n                        [-0.0079, -0.0147,  0.0253],\n                        [ 0.0119,  0.0142, -0.0121]],\n              \n                       [[-0.0285,  0.0295, -0.0043],\n                        [-0.0119,  0.0251,  0.0144],\n                        [ 0.0284, -0.0163,  0.0252]],\n              \n                       [[ 0.0128, -0.0187,  0.0046],\n                        [-0.0085, -0.0014,  0.0275],\n                        [ 0.0267,  0.0204, -0.0172]],\n              \n                       ...,\n              \n                       [[ 0.0164, -0.0116, -0.0036],\n                        [-0.0202, -0.0224, -0.0075],\n                        [-0.0244, -0.0219,  0.0262]],\n              \n                       [[-0.0122,  0.0262,  0.0080],\n                        [ 0.0124, -0.0141,  0.0102],\n                        [ 0.0282,  0.0131,  0.0111]],\n              \n                       [[ 0.0194,  0.0074, -0.0225],\n                        [-0.0171,  0.0243,  0.0217],\n                        [-0.0144, -0.0235,  0.0058]]]], device='cuda:0')),\n             ('block3.3.bias',\n              tensor([-0.0230, -0.0064, -0.0261,  0.0249, -0.0240,  0.0217,  0.0067,  0.0265,\n                      -0.0242,  0.0236, -0.0153,  0.0230, -0.0274,  0.0047, -0.0046, -0.0066,\n                       0.0031,  0.0058,  0.0282,  0.0083,  0.0065, -0.0086,  0.0280,  0.0193,\n                      -0.0162, -0.0030, -0.0012, -0.0218,  0.0019,  0.0086,  0.0015, -0.0131,\n                      -0.0080,  0.0161, -0.0245, -0.0281, -0.0269, -0.0092, -0.0078,  0.0049,\n                       0.0213, -0.0128,  0.0035,  0.0195,  0.0270,  0.0090, -0.0222,  0.0233,\n                       0.0064,  0.0030, -0.0051,  0.0082,  0.0225, -0.0032,  0.0134, -0.0037,\n                       0.0277, -0.0284,  0.0212,  0.0296,  0.0262,  0.0244, -0.0258,  0.0178,\n                       0.0225, -0.0264, -0.0139,  0.0064, -0.0051, -0.0201,  0.0039,  0.0051,\n                       0.0187,  0.0236, -0.0241,  0.0210, -0.0190,  0.0246, -0.0103,  0.0174,\n                      -0.0233,  0.0037, -0.0126, -0.0016,  0.0292, -0.0230, -0.0214,  0.0218,\n                       0.0201, -0.0161,  0.0142,  0.0137, -0.0133,  0.0052,  0.0074,  0.0089,\n                       0.0235,  0.0279,  0.0082,  0.0228, -0.0093, -0.0266, -0.0198, -0.0289,\n                       0.0212,  0.0069, -0.0041,  0.0260, -0.0182,  0.0080,  0.0245,  0.0019,\n                      -0.0262,  0.0095, -0.0177,  0.0252,  0.0219, -0.0260,  0.0098, -0.0051,\n                       0.0143, -0.0042,  0.0098, -0.0080, -0.0020,  0.0242, -0.0089,  0.0205],\n                     device='cuda:0')),\n             ('block3.5.weight',\n              tensor([1.0001, 0.9998, 1.0007, 1.0002, 1.0000, 1.0000, 0.9997, 1.0001, 1.0005,\n                      1.0003, 1.0004, 0.9999, 0.9998, 1.0001, 0.9995, 1.0000, 1.0002, 1.0002,\n                      0.9998, 0.9999, 0.9999, 1.0006, 0.9999, 0.9995, 0.9997, 1.0006, 1.0003,\n                      0.9997, 0.9994, 0.9999, 0.9994, 0.9998, 0.9999, 1.0003, 1.0004, 1.0001,\n                      1.0005, 0.9999, 1.0004, 0.9997, 0.9999, 1.0001, 0.9995, 0.9990, 0.9996,\n                      1.0001, 1.0002, 1.0003, 0.9997, 1.0001, 0.9999, 0.9994, 0.9994, 0.9996,\n                      0.9994, 0.9995, 0.9995, 0.9996, 1.0003, 0.9999, 1.0005, 1.0002, 0.9999,\n                      0.9999, 1.0001, 0.9997, 1.0004, 1.0000, 1.0005, 1.0002, 1.0004, 0.9999,\n                      1.0002, 1.0001, 1.0000, 1.0000, 1.0004, 1.0002, 1.0004, 0.9999, 1.0003,\n                      1.0008, 0.9997, 0.9992, 1.0005, 0.9999, 1.0000, 1.0000, 1.0001, 0.9999,\n                      1.0005, 0.9999, 0.9995, 1.0002, 1.0000, 1.0003, 0.9996, 1.0001, 0.9998,\n                      1.0002, 1.0003, 0.9999, 1.0000, 1.0001, 0.9997, 1.0000, 0.9998, 0.9999,\n                      1.0002, 0.9997, 1.0001, 1.0001, 1.0000, 1.0001, 1.0000, 0.9996, 1.0001,\n                      1.0000, 1.0002, 0.9997, 1.0000, 1.0001, 0.9998, 1.0007, 1.0002, 1.0001,\n                      1.0003, 1.0000], device='cuda:0')),\n             ('block3.5.bias',\n              tensor([ 2.5281e-04,  3.8106e-04, -1.1800e-04,  2.8795e-05, -8.6583e-05,\n                       1.8927e-04, -2.0426e-05, -1.7990e-04,  1.1763e-04, -8.8630e-05,\n                       8.3322e-05,  4.3607e-05,  1.0462e-05,  1.3306e-05,  5.0612e-05,\n                       3.3162e-05, -2.2425e-04, -5.6039e-05, -6.2941e-05,  2.7585e-04,\n                      -1.4935e-04,  1.4103e-04,  1.0580e-04, -2.0489e-05, -1.2325e-04,\n                      -5.0716e-05, -3.9268e-05, -1.7269e-05, -7.0020e-05,  7.5773e-05,\n                       1.4927e-04,  4.9068e-05, -1.2795e-04,  4.4187e-04,  1.1105e-04,\n                       2.9658e-04, -1.0883e-04,  2.3351e-06, -3.2343e-04, -1.4623e-04,\n                      -1.9551e-04,  2.4371e-04, -2.4658e-04,  1.4074e-04, -8.5945e-05,\n                       1.8761e-04,  2.7588e-05, -1.6329e-04,  1.3635e-04,  4.2949e-05,\n                       1.3667e-04,  1.9223e-04, -3.7312e-05, -1.3835e-04,  1.4971e-04,\n                      -2.0311e-05,  2.6775e-04,  1.5144e-04,  2.0246e-04,  2.7851e-05,\n                       2.1128e-04, -1.1962e-05,  6.3863e-05, -1.8979e-05, -9.3669e-05,\n                       2.2351e-04, -1.2340e-04, -9.6440e-05,  9.3242e-05,  4.0755e-05,\n                       3.5186e-05,  8.5163e-05,  2.9306e-05,  2.6108e-05,  2.8869e-04,\n                       8.4753e-05,  1.8993e-04,  6.7867e-05,  3.0480e-05,  9.7659e-05,\n                      -9.5632e-05, -1.7838e-04,  3.0238e-04,  2.2271e-04, -4.0679e-04,\n                      -2.8220e-05, -8.9054e-05,  3.8082e-04, -2.6171e-05,  2.0853e-04,\n                       3.2592e-04,  2.5110e-04,  3.7887e-04, -1.3630e-04, -1.6951e-04,\n                       2.1064e-04,  4.7079e-04, -7.7295e-05, -2.0768e-04,  2.1625e-04,\n                       1.2483e-04,  6.3082e-05, -2.0696e-05,  1.4325e-04,  2.6412e-04,\n                       1.4446e-04,  2.6011e-04,  9.8278e-06, -2.5912e-04,  1.2770e-05,\n                      -2.3393e-05, -1.3186e-04, -8.1550e-05,  1.8092e-05, -2.2507e-05,\n                      -1.7275e-04,  3.6167e-04,  1.1718e-04,  2.1663e-05, -6.4029e-05,\n                      -2.1265e-04,  4.3003e-04,  4.9458e-05, -5.9160e-05,  1.0224e-04,\n                      -7.3288e-05,  3.2875e-04,  1.6612e-04], device='cuda:0')),\n             ('block3.5.running_mean',\n              tensor([0.1803, 0.1892, 0.2365, 0.1816, 0.2424, 0.1961, 0.2730, 0.3168, 0.2358,\n                      0.2734, 0.2403, 0.2152, 0.1916, 0.2195, 0.2379, 0.2438, 0.1731, 0.2207,\n                      0.2402, 0.1969, 0.2053, 0.2136, 0.3199, 0.2260, 0.2949, 0.2449, 0.2952,\n                      0.3066, 0.2472, 0.2628, 0.1521, 0.1987, 0.1852, 0.2840, 0.2529, 0.1880,\n                      0.2857, 0.1636, 0.2683, 0.1629, 0.2266, 0.1919, 0.2107, 0.2556, 0.3131,\n                      0.1966, 0.1800, 0.2878, 0.2216, 0.2248, 0.1851, 0.2412, 0.2105, 0.2071,\n                      0.3271, 0.2079, 0.1708, 0.2469, 0.2427, 0.2683, 0.2468, 0.3143, 0.1624,\n                      0.2487, 0.3108, 0.2382, 0.2609, 0.2605, 0.2937, 0.2148, 0.1765, 0.2161,\n                      0.2078, 0.1876, 0.2162, 0.2136, 0.1914, 0.2366, 0.2308, 0.2995, 0.2375,\n                      0.2592, 0.1722, 0.2909, 0.1856, 0.2988, 0.2121, 0.3074, 0.2018, 0.1860,\n                      0.2222, 0.2669, 0.2328, 0.2067, 0.2077, 0.2283, 0.2144, 0.3059, 0.2023,\n                      0.2857, 0.1919, 0.2581, 0.1950, 0.1953, 0.1523, 0.3262, 0.2999, 0.2384,\n                      0.1540, 0.2206, 0.2075, 0.1661, 0.2223, 0.2035, 0.3403, 0.2391, 0.1912,\n                      0.1707, 0.2298, 0.2647, 0.1903, 0.1810, 0.3371, 0.2015, 0.2074, 0.2815,\n                      0.1813, 0.2183], device='cuda:0')),\n             ('block3.5.running_var',\n              tensor([0.0737, 0.0678, 0.1602, 0.0673, 0.1008, 0.0694, 0.1525, 0.1911, 0.0946,\n                      0.1640, 0.0975, 0.0898, 0.0813, 0.0836, 0.1481, 0.1346, 0.0532, 0.1766,\n                      0.1648, 0.0589, 0.0865, 0.1154, 0.2808, 0.1209, 0.1712, 0.1325, 0.1742,\n                      0.1361, 0.1228, 0.1754, 0.0523, 0.0971, 0.1164, 0.1700, 0.1127, 0.1209,\n                      0.1029, 0.0712, 0.1498, 0.0837, 0.1369, 0.1629, 0.1423, 0.1818, 0.2191,\n                      0.1075, 0.0544, 0.1491, 0.1293, 0.1370, 0.0958, 0.1639, 0.1305, 0.1729,\n                      0.2019, 0.0710, 0.0679, 0.2287, 0.0851, 0.1352, 0.1249, 0.2628, 0.0528,\n                      0.0620, 0.1292, 0.1024, 0.2274, 0.1008, 0.1896, 0.1046, 0.0905, 0.1261,\n                      0.1058, 0.0636, 0.1935, 0.0838, 0.1564, 0.0932, 0.1255, 0.1652, 0.0964,\n                      0.1158, 0.0654, 0.2223, 0.0846, 0.1077, 0.1026, 0.1599, 0.1388, 0.0935,\n                      0.0986, 0.1773, 0.1752, 0.0828, 0.0993, 0.1133, 0.0612, 0.3203, 0.0811,\n                      0.1273, 0.0569, 0.1949, 0.1086, 0.1731, 0.0650, 0.2906, 0.1492, 0.0873,\n                      0.0549, 0.1623, 0.0719, 0.1383, 0.1487, 0.0495, 0.2352, 0.1008, 0.0833,\n                      0.0735, 0.0729, 0.1234, 0.0772, 0.0762, 0.1717, 0.0697, 0.0844, 0.1261,\n                      0.1582, 0.0645], device='cuda:0')),\n             ('block3.5.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block3.6.weight',\n              tensor([[[[ 3.0395e-03, -6.4718e-03, -2.4420e-02],\n                        [-1.1759e-04,  2.5331e-02, -1.2658e-02],\n                        [-2.0200e-02, -4.6506e-04, -1.2768e-02]],\n              \n                       [[-2.7487e-02, -1.6631e-02,  3.5260e-03],\n                        [-1.2955e-02, -5.3926e-03, -1.8902e-02],\n                        [-2.9367e-02, -1.6213e-02, -2.3992e-02]],\n              \n                       [[ 1.6143e-02, -3.8297e-03, -1.2825e-02],\n                        [ 1.8036e-02,  2.7052e-02, -1.5305e-02],\n                        [ 6.2825e-03, -1.1606e-02, -1.7258e-02]],\n              \n                       ...,\n              \n                       [[-1.0233e-02, -1.8191e-02,  1.3216e-02],\n                        [-1.5979e-02,  2.6784e-02, -2.3143e-03],\n                        [ 2.4260e-02,  2.4772e-02,  8.2261e-03]],\n              \n                       [[-2.2024e-02, -2.8230e-02, -3.1946e-04],\n                        [ 2.4770e-02, -1.2402e-02, -8.5481e-03],\n                        [-3.8577e-03,  1.4349e-02,  2.5752e-02]],\n              \n                       [[-2.9071e-02, -1.5887e-02, -5.5369e-03],\n                        [-1.7004e-02, -1.0580e-03, -2.8700e-02],\n                        [-1.4091e-02,  1.2542e-02, -9.5935e-03]]],\n              \n              \n                      [[[ 1.5376e-02,  1.1578e-02, -2.1524e-02],\n                        [ 2.9768e-03,  6.7530e-03, -2.4228e-02],\n                        [ 6.0675e-03,  1.3631e-02,  1.5910e-02]],\n              \n                       [[ 6.7456e-03, -1.2245e-02,  1.1214e-03],\n                        [ 2.7957e-02, -2.7353e-02, -1.7845e-02],\n                        [ 8.8917e-03,  7.2470e-03, -5.1590e-03]],\n              \n                       [[ 3.0150e-02,  2.4815e-02, -2.3856e-02],\n                        [-1.8536e-02,  6.1962e-03, -2.3548e-03],\n                        [-2.3197e-02, -9.5359e-03, -1.7702e-02]],\n              \n                       ...,\n              \n                       [[ 2.1950e-02, -1.5785e-03, -3.6089e-03],\n                        [-4.6494e-03,  2.7871e-02,  4.0271e-03],\n                        [ 9.3483e-03,  2.7322e-03,  1.2817e-02]],\n              \n                       [[ 5.1854e-03,  2.1616e-02, -4.8697e-03],\n                        [-2.7411e-02,  2.6956e-02,  1.3918e-02],\n                        [ 2.4717e-02, -2.2926e-02, -4.1375e-03]],\n              \n                       [[ 1.7998e-02,  2.3193e-02, -1.0678e-03],\n                        [-2.4794e-03, -1.8464e-03, -8.3403e-03],\n                        [-8.3858e-03, -2.5027e-02, -1.1766e-02]]],\n              \n              \n                      [[[ 1.7976e-02, -1.7502e-02, -2.5104e-02],\n                        [ 1.9382e-02, -1.2700e-02,  3.3154e-03],\n                        [ 2.2569e-02,  1.9107e-02,  2.8308e-02]],\n              \n                       [[-2.2305e-02,  1.8141e-02,  1.5408e-02],\n                        [-1.7659e-02,  1.9579e-02,  7.1694e-03],\n                        [ 1.1373e-02, -8.3901e-03, -1.2860e-02]],\n              \n                       [[ 7.6609e-03,  2.6772e-03,  1.2919e-02],\n                        [-1.2151e-02,  7.0483e-04, -2.7967e-02],\n                        [ 1.4537e-02,  2.8331e-02,  4.1375e-05]],\n              \n                       ...,\n              \n                       [[ 8.1021e-03,  2.8440e-02,  9.5340e-03],\n                        [ 2.3861e-02,  1.1800e-02,  2.5941e-02],\n                        [-2.1896e-03, -1.3405e-02,  4.6401e-03]],\n              \n                       [[-1.9833e-02,  2.7501e-02,  1.0012e-03],\n                        [-2.4156e-02, -2.3799e-02,  2.7681e-02],\n                        [-2.7901e-03,  1.2094e-02,  2.7145e-02]],\n              \n                       [[ 1.3068e-02, -2.7318e-02, -2.0397e-02],\n                        [-1.8129e-02,  1.8423e-03, -1.8409e-02],\n                        [ 2.2878e-02, -1.1293e-02,  8.6961e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-4.2473e-03, -2.2313e-02,  1.9380e-02],\n                        [-2.8013e-02, -9.4273e-03, -2.5206e-02],\n                        [ 1.9612e-02,  2.7507e-02, -2.8627e-02]],\n              \n                       [[-1.5770e-02, -1.1514e-02, -9.1752e-04],\n                        [-2.2298e-02,  1.6826e-02,  1.1905e-02],\n                        [ 7.3748e-03,  2.3406e-02,  1.3420e-02]],\n              \n                       [[ 5.5810e-03, -7.9063e-03,  2.3884e-02],\n                        [ 2.3559e-02, -1.4793e-02,  1.6494e-02],\n                        [-3.8433e-03,  5.0730e-03,  1.9044e-02]],\n              \n                       ...,\n              \n                       [[ 2.3937e-02, -7.5636e-03, -4.7094e-03],\n                        [-2.8645e-02, -1.1276e-02, -1.8386e-02],\n                        [-1.8670e-02,  1.2717e-02,  4.7787e-03]],\n              \n                       [[ 7.2196e-03, -2.6954e-02,  2.3065e-02],\n                        [ 1.2845e-02, -8.5303e-03, -4.9939e-03],\n                        [ 1.9247e-02,  2.3014e-02,  7.1280e-03]],\n              \n                       [[ 1.0675e-02, -2.5743e-02,  2.0070e-02],\n                        [ 6.7753e-03, -2.3157e-02, -1.4781e-02],\n                        [ 1.8176e-02,  4.1755e-03, -1.5938e-02]]],\n              \n              \n                      [[[-1.1650e-02, -2.8386e-02,  8.6060e-03],\n                        [ 1.9623e-02, -2.7934e-02, -1.5618e-02],\n                        [ 2.3711e-02,  1.4785e-02, -1.3762e-02]],\n              \n                       [[ 1.3229e-02,  2.5603e-02, -1.7245e-03],\n                        [-5.3874e-03, -7.0072e-03, -1.4292e-02],\n                        [-6.0569e-03,  7.5102e-03,  2.0865e-02]],\n              \n                       [[-2.3415e-02,  2.5732e-02,  2.3265e-02],\n                        [ 2.0047e-02,  2.6769e-02,  8.4772e-04],\n                        [ 3.7667e-04, -2.4352e-02, -1.5337e-02]],\n              \n                       ...,\n              \n                       [[ 2.8658e-02,  2.3304e-02,  2.7197e-02],\n                        [ 4.3692e-03, -9.0910e-03,  1.2353e-02],\n                        [-1.9689e-02, -1.6537e-02,  2.7784e-02]],\n              \n                       [[ 1.5509e-02, -2.1541e-02,  6.2287e-03],\n                        [-5.3374e-03,  1.6971e-02,  3.2749e-03],\n                        [-2.4575e-02,  1.5858e-02,  2.7287e-02]],\n              \n                       [[ 2.5517e-03,  2.4613e-03,  2.2302e-02],\n                        [ 2.4586e-02,  2.9124e-02, -2.8182e-02],\n                        [-2.6821e-02,  7.5733e-03,  1.1348e-02]]],\n              \n              \n                      [[[ 9.5329e-03, -2.2394e-02, -2.1807e-02],\n                        [ 1.4969e-02,  9.4467e-03,  2.1565e-02],\n                        [ 2.6046e-02, -1.8323e-02,  2.3871e-02]],\n              \n                       [[ 6.1520e-03, -4.2211e-03, -2.7660e-02],\n                        [ 5.3867e-03,  1.5877e-02, -5.1756e-03],\n                        [ 1.5546e-02, -1.0844e-03, -2.6652e-02]],\n              \n                       [[ 1.7628e-02, -8.8811e-03,  2.1661e-02],\n                        [-2.8029e-03, -2.8384e-02, -2.5555e-02],\n                        [-2.4676e-02, -1.2070e-02,  4.5281e-04]],\n              \n                       ...,\n              \n                       [[ 2.4284e-02,  1.9230e-02,  8.0378e-03],\n                        [-5.1650e-03,  1.7645e-03, -2.3051e-02],\n                        [-1.7707e-02, -4.5652e-03, -1.8815e-02]],\n              \n                       [[ 1.8313e-02, -2.0636e-02,  4.3223e-03],\n                        [-2.4855e-03,  1.7959e-02,  2.1793e-02],\n                        [-1.7088e-02,  9.2938e-03,  2.5595e-02]],\n              \n                       [[-9.6048e-03,  1.2688e-02,  1.4475e-02],\n                        [ 6.4693e-05,  1.7044e-02,  8.4609e-03],\n                        [ 2.1310e-02, -2.3263e-02,  2.3210e-02]]]], device='cuda:0')),\n             ('block3.6.bias',\n              tensor([-0.0267,  0.0140,  0.0029, -0.0196,  0.0052,  0.0216,  0.0225,  0.0262,\n                      -0.0045, -0.0056, -0.0231, -0.0264, -0.0155,  0.0226,  0.0232,  0.0250,\n                       0.0186,  0.0160, -0.0252,  0.0214, -0.0245, -0.0232, -0.0057, -0.0031,\n                       0.0203, -0.0162, -0.0194, -0.0043, -0.0265, -0.0186,  0.0107,  0.0090,\n                      -0.0175,  0.0157, -0.0182, -0.0190, -0.0063,  0.0208,  0.0292,  0.0146,\n                      -0.0039, -0.0208, -0.0152,  0.0121, -0.0150, -0.0281, -0.0220,  0.0236,\n                       0.0125,  0.0102,  0.0109, -0.0046,  0.0116, -0.0258, -0.0179, -0.0184,\n                       0.0129,  0.0215, -0.0031,  0.0068, -0.0080, -0.0094, -0.0224, -0.0248,\n                      -0.0113, -0.0091,  0.0081, -0.0189, -0.0140,  0.0285,  0.0210,  0.0198,\n                      -0.0243, -0.0107,  0.0056, -0.0087, -0.0070,  0.0122,  0.0068,  0.0277,\n                       0.0177,  0.0146, -0.0018, -0.0260,  0.0113,  0.0165, -0.0283, -0.0012,\n                       0.0125, -0.0046,  0.0271, -0.0025,  0.0050,  0.0053, -0.0175, -0.0186,\n                      -0.0044,  0.0025,  0.0195, -0.0208, -0.0154,  0.0028,  0.0120,  0.0230,\n                      -0.0233, -0.0116, -0.0147,  0.0256, -0.0103, -0.0295,  0.0020, -0.0143,\n                      -0.0065,  0.0036,  0.0066,  0.0138, -0.0069,  0.0030, -0.0092, -0.0199,\n                      -0.0013,  0.0099,  0.0262, -0.0148, -0.0146,  0.0142,  0.0108,  0.0040],\n                     device='cuda:0')),\n             ('block3.8.weight',\n              tensor([1.0000, 1.0001, 1.0001, 1.0001, 0.9999, 1.0005, 1.0000, 0.9996, 0.9994,\n                      0.9996, 0.9998, 1.0003, 0.9999, 1.0002, 0.9999, 1.0003, 1.0001, 1.0001,\n                      1.0006, 0.9999, 0.9995, 1.0001, 0.9998, 0.9999, 1.0005, 0.9999, 1.0006,\n                      0.9997, 0.9999, 0.9999, 1.0000, 0.9995, 0.9999, 1.0002, 0.9997, 1.0003,\n                      1.0001, 0.9998, 1.0000, 0.9999, 0.9998, 1.0000, 1.0001, 1.0002, 1.0004,\n                      1.0005, 1.0001, 0.9999, 1.0002, 0.9998, 1.0001, 1.0004, 0.9999, 1.0001,\n                      1.0003, 0.9999, 0.9998, 1.0002, 1.0000, 1.0004, 1.0003, 1.0002, 1.0001,\n                      1.0000, 1.0003, 0.9999, 0.9998, 0.9999, 1.0000, 1.0001, 1.0000, 0.9997,\n                      0.9998, 1.0003, 0.9998, 0.9997, 0.9998, 0.9998, 1.0001, 0.9997, 1.0000,\n                      1.0002, 0.9997, 0.9997, 0.9999, 1.0001, 1.0005, 1.0006, 0.9998, 1.0007,\n                      1.0004, 0.9998, 1.0000, 1.0000, 0.9998, 0.9997, 1.0002, 0.9999, 0.9997,\n                      0.9994, 0.9994, 1.0000, 1.0001, 1.0004, 0.9996, 1.0001, 1.0002, 0.9996,\n                      0.9997, 0.9998, 0.9998, 0.9998, 1.0002, 1.0002, 0.9998, 1.0002, 0.9999,\n                      0.9999, 1.0006, 1.0002, 1.0002, 0.9998, 0.9996, 0.9996, 0.9998, 0.9998,\n                      0.9999, 1.0000], device='cuda:0')),\n             ('block3.8.bias',\n              tensor([ 3.3816e-05, -1.3459e-04, -1.2620e-04, -2.7891e-05, -5.7861e-05,\n                      -6.3013e-05, -1.6000e-05,  1.3653e-04, -4.4993e-05, -5.5351e-05,\n                       7.6626e-05,  1.1371e-04,  1.3555e-05,  2.5104e-04, -8.3615e-05,\n                      -3.1451e-05,  1.2210e-04,  2.6246e-04, -2.1117e-05,  1.4136e-04,\n                       7.8529e-05,  1.8617e-04,  1.2861e-04,  7.6654e-05, -3.5289e-05,\n                      -1.3524e-05, -1.0949e-05,  4.6170e-05, -1.2714e-04,  3.9704e-05,\n                       7.3585e-05, -1.1822e-04,  1.3310e-04,  1.7577e-05, -6.4900e-05,\n                      -3.3658e-06,  6.5003e-05, -1.4707e-04,  1.5731e-05,  1.1190e-04,\n                       2.4146e-05, -9.9064e-05,  2.6923e-05, -3.0893e-05,  2.5804e-04,\n                       3.4950e-05, -1.5326e-04, -1.2375e-04, -9.4497e-06, -9.1342e-05,\n                       1.0201e-04,  1.4815e-04,  1.2541e-04, -4.9353e-05,  3.2298e-04,\n                       6.6669e-05,  1.6969e-04, -6.5158e-05,  7.7238e-05,  2.1973e-04,\n                       1.2536e-05, -1.3874e-04, -8.0193e-05,  2.0166e-04,  6.1276e-05,\n                       1.3466e-04,  1.0227e-04,  5.5455e-05,  1.1416e-04,  1.9167e-04,\n                       1.4546e-04, -6.6170e-05, -4.8378e-05,  5.0382e-05, -3.5045e-04,\n                      -1.3197e-04, -1.0763e-04, -6.7529e-05,  1.6513e-04,  9.2835e-05,\n                       5.4455e-05, -1.3924e-04,  2.1162e-04, -2.6765e-04, -3.2170e-04,\n                       1.9510e-04,  7.8550e-06, -4.0266e-05, -2.2562e-04,  6.2973e-05,\n                      -4.4395e-05, -2.2157e-04, -2.8540e-05, -1.8905e-04,  7.9893e-05,\n                       1.9766e-05,  2.7170e-04, -1.3962e-05, -1.1843e-05,  9.6112e-05,\n                       2.5297e-04,  1.9354e-04, -3.4434e-05, -2.2308e-05, -5.1200e-05,\n                       5.4637e-05, -4.3222e-05,  1.1052e-04,  1.6860e-04,  2.1869e-04,\n                      -5.5413e-05,  1.4184e-04, -1.4117e-04, -5.5402e-06,  5.9889e-05,\n                       9.6143e-05,  7.1572e-05,  6.9180e-05,  7.5022e-05, -8.1936e-06,\n                      -1.1300e-04,  2.0911e-04,  1.0105e-04,  3.8839e-05,  1.2316e-06,\n                       1.4874e-04, -7.2155e-05, -1.0927e-05], device='cuda:0')),\n             ('block3.8.running_mean',\n              tensor([0.1887, 0.2348, 0.2825, 0.2235, 0.3725, 0.2374, 0.3508, 0.2717, 0.1834,\n                      0.2308, 0.1970, 0.2209, 0.2201, 0.1963, 0.2155, 0.3050, 0.1942, 0.2022,\n                      0.2341, 0.2253, 0.2580, 0.2544, 0.3023, 0.2131, 0.2548, 0.2282, 0.2377,\n                      0.2602, 0.2987, 0.2001, 0.1995, 0.2789, 0.1936, 0.3132, 0.2227, 0.1609,\n                      0.2080, 0.2470, 0.2316, 0.2347, 0.2505, 0.2517, 0.1970, 0.2435, 0.1900,\n                      0.1779, 0.1576, 0.2615, 0.3035, 0.2651, 0.1885, 0.2067, 0.1968, 0.2003,\n                      0.1756, 0.2141, 0.2312, 0.2938, 0.2189, 0.2194, 0.1889, 0.2123, 0.1897,\n                      0.2178, 0.3321, 0.2457, 0.2823, 0.2179, 0.2044, 0.2935, 0.2560, 0.2959,\n                      0.2104, 0.2193, 0.2353, 0.1945, 0.2152, 0.2470, 0.3216, 0.2105, 0.2614,\n                      0.2188, 0.3390, 0.2262, 0.3039, 0.2142, 0.1558, 0.2400, 0.1860, 0.2810,\n                      0.2094, 0.2646, 0.2655, 0.3378, 0.3053, 0.2107, 0.2519, 0.1907, 0.2193,\n                      0.2369, 0.1654, 0.2529, 0.2443, 0.1898, 0.2140, 0.2411, 0.2569, 0.2175,\n                      0.2567, 0.2154, 0.2339, 0.1852, 0.2976, 0.2199, 0.2970, 0.2637, 0.1879,\n                      0.2708, 0.2511, 0.1986, 0.1878, 0.2684, 0.2535, 0.1962, 0.2296, 0.1624,\n                      0.1938, 0.2498], device='cuda:0')),\n             ('block3.8.running_var',\n              tensor([0.0791, 0.1801, 0.2388, 0.1068, 0.2834, 0.1005, 0.1846, 0.2947, 0.0683,\n                      0.1016, 0.1015, 0.1417, 0.1232, 0.0930, 0.1005, 0.1382, 0.0674, 0.0966,\n                      0.1335, 0.1116, 0.1689, 0.1336, 0.2532, 0.1063, 0.1266, 0.1828, 0.2128,\n                      0.1821, 0.1713, 0.0817, 0.0969, 0.1664, 0.0753, 0.1240, 0.1215, 0.1112,\n                      0.0823, 0.1034, 0.0877, 0.0883, 0.1895, 0.1151, 0.1655, 0.1240, 0.0640,\n                      0.1016, 0.0828, 0.1149, 0.1836, 0.1369, 0.1148, 0.0931, 0.0933, 0.0646,\n                      0.1067, 0.1462, 0.1282, 0.1371, 0.0908, 0.0894, 0.1178, 0.0940, 0.0685,\n                      0.1299, 0.1709, 0.1582, 0.1791, 0.1216, 0.0979, 0.1497, 0.1501, 0.1530,\n                      0.1554, 0.1038, 0.1305, 0.1030, 0.1012, 0.1630, 0.1735, 0.1279, 0.1264,\n                      0.0952, 0.2273, 0.0974, 0.1822, 0.1330, 0.0547, 0.1942, 0.0687, 0.1900,\n                      0.1134, 0.1165, 0.1383, 0.2873, 0.1918, 0.1195, 0.2291, 0.1044, 0.0755,\n                      0.1079, 0.0831, 0.1404, 0.0969, 0.0909, 0.0780, 0.1585, 0.1614, 0.1127,\n                      0.1487, 0.2507, 0.1447, 0.1098, 0.1142, 0.1748, 0.2734, 0.1071, 0.1158,\n                      0.1469, 0.1768, 0.0935, 0.1318, 0.1599, 0.1726, 0.0744, 0.1523, 0.0958,\n                      0.0808, 0.1024], device='cuda:0')),\n             ('block3.8.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block4.0.weight',\n              tensor([[[[ 1.7555e-02,  3.0986e-03,  1.8296e-02],\n                        [-2.1231e-02,  2.1163e-03,  2.4796e-02],\n                        [ 2.1595e-02, -2.3850e-02, -1.1856e-04]],\n              \n                       [[ 2.7817e-02, -2.8503e-02, -2.4579e-02],\n                        [ 1.5323e-02,  2.1819e-02, -1.6047e-02],\n                        [-1.7274e-03,  1.5376e-02, -2.9030e-02]],\n              \n                       [[ 1.5928e-02,  2.0908e-02, -2.5394e-02],\n                        [ 1.7963e-02, -1.1859e-02,  7.6659e-05],\n                        [ 2.0501e-02,  5.4703e-03, -5.5467e-03]],\n              \n                       ...,\n              \n                       [[-3.2885e-03,  1.3302e-02,  2.4624e-02],\n                        [-2.3673e-02,  1.6421e-02, -4.8623e-03],\n                        [-1.4471e-02,  6.8922e-03, -1.7857e-02]],\n              \n                       [[ 1.5484e-02,  4.2926e-03,  1.0290e-02],\n                        [-1.7910e-02, -1.6255e-03, -2.2099e-02],\n                        [ 1.6687e-02,  2.9004e-02, -1.0835e-02]],\n              \n                       [[-1.0721e-02, -2.8861e-02, -2.8264e-02],\n                        [ 1.9603e-02, -1.3902e-02,  1.7837e-02],\n                        [ 2.7697e-02, -8.0947e-03, -2.4847e-02]]],\n              \n              \n                      [[[ 2.5081e-02,  1.8537e-02,  9.2007e-03],\n                        [ 1.0881e-02, -1.2928e-02,  1.9756e-02],\n                        [-1.2320e-02,  9.6483e-03, -2.4423e-02]],\n              \n                       [[-1.5299e-02,  5.9425e-03,  1.1119e-02],\n                        [ 3.5073e-03, -1.7843e-02, -1.7282e-02],\n                        [ 6.6536e-03,  1.0889e-02, -5.4668e-03]],\n              \n                       [[-2.2726e-02, -2.3986e-02, -2.7609e-03],\n                        [-1.4909e-02, -7.6075e-03,  2.8100e-02],\n                        [ 4.7590e-03,  1.1653e-02, -2.9462e-02]],\n              \n                       ...,\n              \n                       [[ 1.0251e-02, -5.9645e-03, -2.9579e-02],\n                        [ 6.7636e-03, -1.2087e-02,  2.5480e-02],\n                        [ 1.1895e-03, -2.4902e-02, -1.5548e-02]],\n              \n                       [[ 7.3142e-03,  2.2569e-02, -2.5165e-02],\n                        [-2.2446e-02, -5.8161e-03,  7.5163e-03],\n                        [ 2.9940e-02,  2.6193e-02,  2.4605e-02]],\n              \n                       [[-2.3172e-02, -1.2548e-03, -1.6914e-02],\n                        [-5.0500e-03,  2.3426e-02,  1.0103e-03],\n                        [ 1.4453e-03, -1.5284e-03, -9.4107e-03]]],\n              \n              \n                      [[[ 2.8685e-02,  1.7315e-02, -2.4257e-02],\n                        [ 1.5397e-02, -1.5662e-02,  1.3862e-02],\n                        [-1.6801e-02,  2.5531e-02,  1.0260e-02]],\n              \n                       [[ 1.9705e-02, -4.8367e-03, -2.4754e-02],\n                        [ 1.0371e-02, -8.2925e-03,  2.8353e-02],\n                        [ 1.7788e-02, -2.7285e-02,  2.1359e-02]],\n              \n                       [[-8.8396e-03,  6.2135e-03,  4.9312e-03],\n                        [-1.3422e-03, -2.8232e-02, -7.3609e-03],\n                        [-5.9327e-03,  2.8021e-02, -2.7292e-02]],\n              \n                       ...,\n              \n                       [[ 1.4189e-02,  2.7334e-02, -2.3637e-02],\n                        [-2.8325e-02, -3.8526e-03, -9.1928e-03],\n                        [-2.3760e-02,  2.8761e-02, -2.4515e-02]],\n              \n                       [[-6.0195e-03, -1.5165e-02, -1.1609e-02],\n                        [-2.9958e-03,  2.0393e-02,  1.7846e-02],\n                        [ 2.5454e-02, -1.8080e-02, -1.1162e-02]],\n              \n                       [[ 2.4391e-02, -2.2273e-02, -2.9143e-02],\n                        [ 2.3163e-02,  5.4155e-03,  2.9381e-02],\n                        [-2.1225e-03, -1.0405e-02, -2.4725e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.0682e-02, -1.9407e-02, -2.4572e-02],\n                        [ 1.8413e-02, -4.0602e-04, -2.5150e-02],\n                        [ 3.3547e-03, -1.4554e-02,  9.2537e-03]],\n              \n                       [[ 9.6203e-03,  2.7337e-02,  1.3549e-02],\n                        [ 1.5935e-02,  2.4832e-02,  1.3804e-04],\n                        [-2.0117e-02,  1.5712e-02, -2.2401e-02]],\n              \n                       [[-2.3922e-02,  5.6421e-03,  2.2171e-02],\n                        [ 2.8578e-02,  2.1379e-02, -2.6866e-02],\n                        [ 2.6108e-02,  2.4042e-03, -2.8775e-02]],\n              \n                       ...,\n              \n                       [[-2.9923e-02, -2.5966e-02, -7.7969e-03],\n                        [-4.2308e-03, -2.8711e-03, -2.4501e-02],\n                        [ 2.3003e-02, -1.2052e-02, -2.6391e-02]],\n              \n                       [[-9.5175e-03, -1.0840e-02, -3.1088e-03],\n                        [ 1.8962e-02,  1.3105e-02, -5.0865e-04],\n                        [ 1.2207e-02,  1.9273e-02,  2.8460e-02]],\n              \n                       [[ 1.3860e-02, -8.9920e-03,  8.4282e-03],\n                        [-8.6701e-03,  2.9130e-02,  2.9351e-02],\n                        [-2.1961e-02,  2.2721e-02,  8.7546e-03]]],\n              \n              \n                      [[[ 2.8066e-02,  5.7111e-03,  2.4843e-02],\n                        [ 1.8123e-02, -3.6471e-03, -1.2157e-02],\n                        [ 2.5656e-02,  2.2664e-02,  1.1161e-02]],\n              \n                       [[ 4.1822e-03, -2.3620e-02, -1.7235e-02],\n                        [-2.9267e-02, -2.0010e-03,  7.2663e-03],\n                        [ 7.2208e-03,  6.5362e-03,  2.5946e-03]],\n              \n                       [[ 4.3402e-03, -1.7151e-02,  1.0679e-02],\n                        [ 4.4963e-03, -1.9610e-02,  1.5541e-02],\n                        [ 1.0536e-02,  2.8289e-02, -1.2101e-02]],\n              \n                       ...,\n              \n                       [[-1.4634e-02,  1.9791e-02,  2.2560e-02],\n                        [ 1.5695e-02,  1.2986e-02, -2.3222e-02],\n                        [ 2.7919e-02,  1.4468e-02, -2.8751e-02]],\n              \n                       [[-1.1556e-02,  1.2952e-02,  1.9821e-02],\n                        [ 1.7081e-02, -2.6615e-02, -1.0966e-03],\n                        [ 2.2839e-02, -2.6116e-02, -6.4748e-03]],\n              \n                       [[ 1.5337e-02,  2.3938e-02,  2.5537e-02],\n                        [ 1.2963e-02, -2.5198e-02,  2.7986e-02],\n                        [-2.4732e-02, -2.2616e-02, -1.8214e-03]]],\n              \n              \n                      [[[-3.2532e-03,  1.1004e-02, -2.6986e-02],\n                        [-4.8475e-03,  1.3157e-02,  1.1205e-02],\n                        [-1.8395e-02, -1.5645e-02, -2.4077e-02]],\n              \n                       [[-2.5241e-02,  1.7383e-02, -2.4916e-02],\n                        [ 4.2180e-03,  2.1057e-03, -1.5241e-02],\n                        [-1.6116e-02,  1.0563e-02,  7.3763e-03]],\n              \n                       [[ 1.7347e-02, -1.6042e-02, -2.1929e-02],\n                        [-2.4838e-02,  1.9870e-02,  1.3183e-02],\n                        [ 2.8657e-02, -2.4468e-02,  2.6176e-02]],\n              \n                       ...,\n              \n                       [[-1.6494e-02, -1.1467e-02, -1.8114e-02],\n                        [ 2.4670e-02,  2.7410e-02,  5.0440e-03],\n                        [-1.4602e-02, -6.6593e-03, -3.6376e-03]],\n              \n                       [[ 3.5244e-03,  7.1980e-03,  8.2077e-03],\n                        [ 2.2117e-02, -1.0639e-02, -2.7012e-02],\n                        [-2.2764e-02,  1.4946e-02, -2.2227e-03]],\n              \n                       [[-1.6786e-02,  4.0123e-03,  9.7952e-04],\n                        [ 2.9487e-02,  2.7943e-02, -2.2399e-02],\n                        [-2.0605e-03,  2.8853e-02,  1.9780e-02]]]], device='cuda:0')),\n             ('block4.0.bias',\n              tensor([-0.0047, -0.0205, -0.0138, -0.0061,  0.0183, -0.0117, -0.0258, -0.0220,\n                       0.0086, -0.0049,  0.0117, -0.0192, -0.0278,  0.0114,  0.0041,  0.0090,\n                      -0.0235, -0.0216,  0.0239,  0.0120, -0.0073,  0.0064, -0.0012,  0.0225,\n                      -0.0066, -0.0094, -0.0130, -0.0104, -0.0161,  0.0270, -0.0117,  0.0158,\n                       0.0051, -0.0126, -0.0170, -0.0080,  0.0244, -0.0249, -0.0194, -0.0274,\n                      -0.0118,  0.0122, -0.0241,  0.0247,  0.0243,  0.0243,  0.0277,  0.0079,\n                      -0.0147,  0.0277,  0.0029,  0.0042, -0.0017,  0.0250,  0.0006,  0.0259,\n                       0.0119,  0.0240,  0.0249, -0.0080,  0.0092,  0.0109,  0.0224, -0.0169,\n                       0.0103, -0.0260,  0.0255, -0.0276,  0.0202, -0.0086, -0.0065, -0.0292,\n                       0.0213,  0.0175,  0.0226,  0.0072,  0.0157, -0.0082, -0.0004,  0.0062,\n                      -0.0052,  0.0042,  0.0132,  0.0147, -0.0258,  0.0271,  0.0042, -0.0033,\n                       0.0171, -0.0050, -0.0100, -0.0011, -0.0162,  0.0275, -0.0277,  0.0268,\n                      -0.0004,  0.0088, -0.0207, -0.0044,  0.0270,  0.0118,  0.0224, -0.0138,\n                       0.0107, -0.0189, -0.0006,  0.0241,  0.0106,  0.0253,  0.0272,  0.0242,\n                      -0.0144,  0.0203, -0.0249,  0.0141,  0.0011,  0.0292,  0.0251,  0.0253,\n                       0.0137,  0.0248,  0.0119, -0.0221, -0.0096,  0.0148,  0.0043, -0.0106,\n                      -0.0163,  0.0073, -0.0209,  0.0180, -0.0233,  0.0006,  0.0088, -0.0207,\n                       0.0138, -0.0019,  0.0185,  0.0134, -0.0020, -0.0220, -0.0235, -0.0116,\n                       0.0030, -0.0220,  0.0133, -0.0199, -0.0131,  0.0006,  0.0185, -0.0141,\n                       0.0264,  0.0215,  0.0235, -0.0004, -0.0173, -0.0035,  0.0291, -0.0268,\n                      -0.0057, -0.0062,  0.0010,  0.0206, -0.0292,  0.0141,  0.0075, -0.0090,\n                       0.0151, -0.0054,  0.0210,  0.0277, -0.0171, -0.0139, -0.0019,  0.0259,\n                       0.0231,  0.0035,  0.0140,  0.0110, -0.0221, -0.0109,  0.0241,  0.0290,\n                       0.0265,  0.0023, -0.0098, -0.0035,  0.0061,  0.0160,  0.0007, -0.0258,\n                       0.0268, -0.0174, -0.0238,  0.0135, -0.0222,  0.0279,  0.0290,  0.0237,\n                       0.0246, -0.0036, -0.0130, -0.0093,  0.0294, -0.0034,  0.0050, -0.0168,\n                      -0.0152, -0.0242,  0.0156,  0.0136, -0.0112, -0.0119,  0.0188, -0.0134,\n                      -0.0249, -0.0147,  0.0059, -0.0096,  0.0086, -0.0231, -0.0231,  0.0088,\n                      -0.0189, -0.0259, -0.0065,  0.0113, -0.0094,  0.0023, -0.0168,  0.0259,\n                      -0.0105,  0.0068, -0.0220,  0.0244,  0.0191,  0.0291, -0.0011, -0.0185,\n                      -0.0143, -0.0135,  0.0106,  0.0180,  0.0091, -0.0129,  0.0265,  0.0056,\n                       0.0107,  0.0114,  0.0234, -0.0210,  0.0230, -0.0067,  0.0212, -0.0023],\n                     device='cuda:0')),\n             ('block4.2.weight',\n              tensor([0.9998, 1.0006, 0.9997, 0.9996, 1.0000, 1.0000, 1.0002, 1.0000, 1.0003,\n                      1.0000, 0.9999, 0.9999, 0.9996, 1.0000, 1.0001, 1.0001, 0.9998, 1.0003,\n                      0.9999, 1.0002, 0.9998, 0.9998, 1.0001, 1.0000, 0.9998, 1.0002, 0.9998,\n                      1.0002, 1.0001, 1.0001, 1.0000, 1.0000, 0.9999, 0.9998, 1.0001, 1.0000,\n                      1.0000, 1.0003, 0.9996, 0.9997, 0.9994, 1.0003, 0.9998, 1.0000, 0.9999,\n                      1.0001, 1.0004, 1.0000, 1.0004, 1.0001, 0.9998, 1.0000, 1.0002, 1.0001,\n                      0.9998, 0.9996, 0.9999, 0.9998, 1.0001, 1.0000, 1.0000, 1.0002, 1.0000,\n                      1.0000, 1.0000, 0.9998, 1.0003, 1.0005, 1.0002, 1.0000, 0.9999, 0.9999,\n                      1.0001, 1.0001, 0.9999, 0.9996, 0.9999, 1.0001, 0.9998, 1.0000, 0.9997,\n                      1.0000, 0.9996, 1.0003, 1.0001, 1.0001, 0.9999, 1.0001, 0.9999, 1.0000,\n                      1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 1.0004, 1.0000, 1.0001, 1.0000,\n                      0.9998, 1.0000, 0.9999, 1.0001, 0.9998, 0.9999, 1.0001, 1.0002, 0.9999,\n                      0.9998, 1.0000, 1.0000, 1.0001, 1.0000, 0.9999, 1.0000, 1.0002, 1.0002,\n                      0.9999, 0.9999, 0.9999, 1.0002, 1.0001, 1.0001, 1.0001, 0.9996, 1.0000,\n                      1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 1.0003, 1.0000, 1.0003, 1.0002,\n                      1.0002, 1.0001, 0.9999, 1.0002, 1.0002, 0.9998, 1.0000, 1.0003, 1.0001,\n                      1.0002, 1.0001, 1.0000, 0.9997, 1.0001, 0.9999, 1.0002, 1.0000, 0.9999,\n                      1.0001, 0.9999, 1.0001, 1.0001, 0.9999, 1.0000, 1.0002, 1.0000, 1.0000,\n                      0.9998, 1.0000, 1.0001, 1.0000, 1.0003, 1.0000, 0.9998, 0.9999, 0.9999,\n                      0.9999, 0.9997, 0.9997, 1.0002, 1.0003, 1.0000, 0.9998, 1.0000, 1.0002,\n                      1.0002, 1.0000, 1.0001, 1.0000, 1.0001, 1.0002, 1.0001, 0.9999, 1.0001,\n                      1.0001, 1.0000, 0.9996, 1.0000, 1.0001, 0.9998, 1.0000, 1.0000, 1.0001,\n                      1.0001, 1.0000, 0.9997, 1.0000, 1.0001, 1.0002, 1.0001, 0.9999, 1.0001,\n                      0.9998, 0.9999, 1.0000, 1.0001, 1.0001, 1.0000, 1.0000, 1.0002, 0.9997,\n                      0.9999, 1.0001, 1.0000, 1.0001, 0.9998, 1.0000, 1.0000, 1.0001, 1.0000,\n                      1.0003, 1.0001, 1.0002, 1.0002, 0.9996, 0.9998, 1.0001, 0.9999, 0.9999,\n                      1.0002, 1.0001, 1.0000, 1.0000, 1.0001, 1.0000, 0.9999, 1.0000, 1.0002,\n                      0.9996, 0.9995, 1.0001, 0.9999, 1.0001, 1.0003, 1.0000, 0.9998, 0.9997,\n                      1.0002, 1.0000, 1.0000, 1.0000], device='cuda:0')),\n             ('block4.2.bias',\n              tensor([ 5.1070e-05, -7.0222e-05, -1.2831e-04,  9.2585e-05, -2.2445e-05,\n                      -7.4394e-05, -1.7489e-05,  6.6976e-05,  4.1379e-05, -1.3081e-04,\n                       1.2871e-04,  1.3774e-04,  2.2848e-04, -9.6357e-05, -1.0909e-04,\n                       8.0347e-05,  4.8940e-05,  1.1603e-04, -6.7287e-05,  8.7034e-06,\n                       1.5590e-04,  2.4665e-06, -2.2759e-05,  1.1349e-04, -4.6020e-05,\n                      -1.2830e-04,  4.7902e-05,  1.5455e-04,  1.7027e-04,  6.3424e-05,\n                      -8.8986e-06, -1.3615e-05,  1.4342e-04, -5.9987e-05,  6.7786e-05,\n                      -5.8310e-06,  8.6560e-05, -5.1732e-05,  1.5069e-04,  1.2249e-04,\n                       2.2409e-04,  2.7711e-05,  5.6279e-07,  7.5485e-05,  3.1052e-04,\n                       3.2323e-05, -1.2423e-04,  1.5906e-04,  7.9746e-05,  2.7627e-04,\n                      -8.1813e-05, -7.6512e-05, -1.1211e-04, -6.3463e-05,  3.3697e-05,\n                       6.4540e-05,  1.3459e-04, -8.3237e-05,  2.5470e-05, -7.6332e-05,\n                       8.6079e-05, -3.2848e-05,  2.3486e-05, -8.2173e-05,  1.1694e-04,\n                      -1.3229e-04, -3.0675e-05, -1.6958e-04,  1.0650e-04, -4.2635e-05,\n                       7.2878e-05, -1.7871e-07, -3.7810e-05, -8.6926e-05, -1.1961e-04,\n                       1.3002e-04,  4.1114e-05, -8.3905e-05, -3.0145e-05,  6.6613e-05,\n                      -4.2941e-05, -8.0595e-05, -7.5426e-05, -1.8975e-04,  1.8938e-04,\n                       3.7033e-05,  3.7660e-05,  1.1094e-04,  9.3300e-07,  1.3085e-04,\n                       2.7335e-04,  7.1264e-05,  2.4483e-04, -5.7511e-05,  6.8434e-06,\n                      -2.0053e-04,  9.2628e-05,  1.1382e-04, -5.2382e-05,  7.5804e-05,\n                       8.1669e-05,  1.6644e-04,  1.6692e-04,  8.4711e-05,  5.4970e-05,\n                       5.1180e-05,  2.4937e-04, -5.3120e-05,  9.1821e-05,  5.3627e-05,\n                      -3.1878e-05,  2.1886e-06,  4.9125e-05, -5.9661e-05,  1.5295e-04,\n                       1.2450e-04, -6.7151e-05, -9.3206e-05,  1.6091e-04, -6.6633e-06,\n                       1.6776e-04,  1.2519e-04,  1.7610e-04, -1.2033e-04,  1.3032e-04,\n                      -1.5984e-04, -1.2136e-04, -2.5630e-05, -1.7282e-05, -1.2081e-04,\n                      -6.5938e-05,  5.3692e-06, -1.0245e-04, -1.5959e-05,  3.9874e-05,\n                      -2.8379e-06, -1.0299e-04, -7.2491e-05, -2.0369e-05, -1.8055e-05,\n                       1.5936e-04,  6.7299e-05,  4.5725e-05,  5.7753e-05, -4.2683e-05,\n                      -1.4453e-06,  3.2574e-05,  9.2902e-06,  1.5777e-04,  1.1770e-04,\n                      -2.2417e-04,  6.4328e-05,  6.5069e-05,  1.1793e-05,  3.0276e-05,\n                      -6.9773e-05,  9.2363e-05, -7.3189e-05,  1.2348e-04, -2.9322e-05,\n                       1.1041e-04,  3.9032e-04, -7.9937e-05, -4.1536e-05,  9.0248e-06,\n                      -1.7027e-05, -8.4241e-06,  1.0696e-06, -7.9591e-05,  4.7827e-05,\n                      -7.1782e-05, -2.0103e-04,  1.6961e-04,  5.5430e-05, -2.5855e-05,\n                       1.9552e-05,  3.5601e-05, -9.3792e-05,  3.8619e-05, -9.5331e-05,\n                       1.5737e-04, -3.6523e-05, -1.9018e-04, -3.1825e-05,  8.9572e-05,\n                      -3.6639e-05, -7.4505e-05,  2.7140e-05, -1.6345e-04, -5.3260e-05,\n                       1.2234e-04,  2.5281e-04, -1.5020e-05, -6.6689e-05,  7.5846e-05,\n                       6.8007e-05, -1.7661e-05, -1.8592e-04,  6.5587e-05, -1.2629e-05,\n                       7.8411e-05, -1.7588e-05,  1.9574e-05, -6.3877e-05,  6.9698e-05,\n                      -6.9179e-05, -9.5362e-05, -2.7689e-06,  1.4749e-05, -8.6031e-05,\n                      -1.8418e-05, -3.1681e-05,  1.3116e-04,  6.0055e-05, -1.1514e-04,\n                       3.3935e-05, -1.5092e-04,  4.2445e-05,  1.7395e-04,  1.0126e-04,\n                      -1.1626e-05, -1.8009e-05,  2.5098e-05,  1.8387e-04, -1.8554e-04,\n                       5.6578e-05,  8.0163e-05, -8.3754e-07,  1.8015e-04,  1.4636e-04,\n                      -3.9356e-05,  1.6167e-04,  1.6229e-04, -1.3754e-04,  5.6493e-05,\n                       5.7363e-05, -1.0380e-04, -1.3622e-04,  1.1404e-04, -8.2798e-05,\n                       2.8023e-04, -1.2357e-04, -1.8976e-04,  1.5631e-04,  3.4424e-04,\n                       2.0889e-05, -1.6409e-06, -6.8724e-05,  2.8216e-05,  5.5220e-05,\n                       1.7103e-04, -1.2450e-05,  4.1235e-05,  2.0317e-04,  1.2040e-04,\n                       2.8686e-05], device='cuda:0')),\n             ('block4.2.running_mean',\n              tensor([0.2574, 0.1533, 0.1312, 0.3846, 0.3298, 0.3279, 0.1724, 0.2073, 0.3616,\n                      0.1530, 0.2660, 0.4889, 0.2462, 0.1286, 0.2405, 0.3131, 0.3946, 0.3257,\n                      0.4597, 0.2868, 0.3118, 0.2984, 0.2006, 0.1832, 0.3425, 0.3378, 0.2578,\n                      0.1122, 0.6034, 0.1777, 0.1485, 0.2253, 0.1763, 0.4076, 0.4470, 0.3128,\n                      0.1056, 0.3212, 0.3104, 0.3494, 0.3677, 0.3123, 0.2676, 0.2234, 0.2992,\n                      0.1804, 0.1770, 0.3079, 0.3831, 0.2314, 0.2312, 0.3290, 0.1442, 0.1463,\n                      0.3115, 0.4975, 0.3213, 0.2758, 0.1610, 0.1596, 0.1826, 0.1917, 0.3766,\n                      0.1180, 0.4206, 0.3147, 0.1934, 0.2010, 0.3344, 0.1876, 0.2513, 0.2300,\n                      0.1232, 0.3139, 0.2996, 0.3337, 0.2171, 0.3117, 0.2911, 0.3306, 0.3135,\n                      0.1525, 0.3632, 0.1088, 0.2164, 0.1460, 0.1608, 0.1728, 0.3544, 0.3518,\n                      0.2880, 0.1949, 0.3697, 0.3818, 0.0863, 0.2483, 0.2465, 0.1153, 0.4941,\n                      0.2768, 0.3358, 0.4946, 0.1050, 0.2444, 0.4523, 0.3248, 0.3606, 0.2230,\n                      0.3166, 0.3667, 0.3493, 0.2948, 0.1753, 0.2915, 0.1893, 0.3131, 0.1755,\n                      0.2383, 0.4875, 0.1874, 0.1221, 0.1457, 0.4027, 0.1610, 0.2059, 0.3566,\n                      0.3688, 0.1857, 0.1716, 0.2876, 0.1844, 0.2430, 0.1758, 0.3104, 0.3232,\n                      0.1999, 0.2180, 0.2190, 0.1650, 0.2546, 0.2427, 0.1831, 0.3597, 0.3411,\n                      0.2399, 0.3110, 0.4777, 0.2434, 0.2037, 0.2577, 0.1570, 0.1613, 0.2825,\n                      0.2233, 0.1849, 0.1902, 0.1811, 0.2102, 0.3691, 0.1709, 0.4762, 0.4229,\n                      0.3045, 0.2513, 0.2281, 0.1752, 0.2243, 0.2480, 0.1929, 0.4781, 0.2089,\n                      0.1905, 0.2855, 0.2921, 0.2339, 0.4284, 0.3837, 0.2745, 0.4931, 0.2622,\n                      0.2040, 0.1374, 0.2202, 0.2017, 0.2919, 0.3020, 0.1521, 0.2697, 0.3187,\n                      0.3204, 0.3370, 0.3539, 0.2924, 0.2239, 0.5209, 0.2727, 0.1564, 0.2529,\n                      0.5299, 0.4096, 0.3521, 0.2723, 0.1855, 0.1667, 0.2512, 0.3515, 0.4220,\n                      0.3194, 0.1957, 0.2292, 0.1536, 0.2438, 0.1087, 0.1288, 0.2614, 0.1419,\n                      0.3709, 0.2265, 0.1792, 0.2081, 0.3733, 0.1344, 0.1991, 0.6380, 0.2093,\n                      0.1695, 0.1776, 0.2720, 0.3259, 0.3525, 0.3773, 0.3623, 0.4168, 0.2679,\n                      0.1906, 0.3402, 0.2098, 0.2288, 0.2527, 0.2067, 0.2647, 0.1613, 0.1619,\n                      0.3575, 0.3021, 0.3327, 0.4195, 0.2714, 0.4879, 0.3592, 0.3276, 0.5202,\n                      0.2660, 0.2461, 0.4052, 0.2367], device='cuda:0')),\n             ('block4.2.running_var',\n              tensor([0.1843, 0.0619, 0.0713, 0.2234, 0.1625, 0.1876, 0.1063, 0.1126, 0.2241,\n                      0.0651, 0.1609, 0.4801, 0.2274, 0.0551, 0.1767, 0.1978, 0.2499, 0.3036,\n                      0.2976, 0.1725, 0.2069, 0.2389, 0.1257, 0.0908, 0.2224, 0.2366, 0.1721,\n                      0.0464, 0.7950, 0.1016, 0.0735, 0.0890, 0.1112, 0.3265, 0.3794, 0.1747,\n                      0.0491, 0.1616, 0.3069, 0.1879, 0.3449, 0.2029, 0.1669, 0.0908, 0.3280,\n                      0.0828, 0.0802, 0.2139, 0.4078, 0.1023, 0.1610, 0.1740, 0.0576, 0.0883,\n                      0.2108, 0.3347, 0.1742, 0.1111, 0.0627, 0.0980, 0.1159, 0.0741, 0.2367,\n                      0.0500, 0.3052, 0.2223, 0.0873, 0.0992, 0.1643, 0.0959, 0.1497, 0.1550,\n                      0.0593, 0.1755, 0.1655, 0.2609, 0.1072, 0.2680, 0.2749, 0.2843, 0.2824,\n                      0.0768, 0.1826, 0.0523, 0.1013, 0.0883, 0.0753, 0.0876, 0.2607, 0.1891,\n                      0.1359, 0.0999, 0.3038, 0.4204, 0.0475, 0.1165, 0.1272, 0.0534, 0.2745,\n                      0.1425, 0.1964, 0.5674, 0.0423, 0.1485, 0.5068, 0.1843, 0.2953, 0.1046,\n                      0.2572, 0.2183, 0.2639, 0.2490, 0.0920, 0.1745, 0.1068, 0.2410, 0.0859,\n                      0.1177, 0.5581, 0.0740, 0.0582, 0.0654, 0.4882, 0.0582, 0.1500, 0.2730,\n                      0.2618, 0.0893, 0.0664, 0.3482, 0.0924, 0.1070, 0.0853, 0.1579, 0.2097,\n                      0.0971, 0.0980, 0.1319, 0.0639, 0.1402, 0.1452, 0.1258, 0.2450, 0.2596,\n                      0.1686, 0.1806, 0.2797, 0.1298, 0.1200, 0.1166, 0.0496, 0.0729, 0.1738,\n                      0.1469, 0.1091, 0.1448, 0.0842, 0.1795, 0.1609, 0.0752, 0.5089, 0.2167,\n                      0.1609, 0.1336, 0.1493, 0.1047, 0.1299, 0.1621, 0.0786, 0.2609, 0.1015,\n                      0.0873, 0.1784, 0.2058, 0.1315, 0.3346, 0.2213, 0.1397, 0.4615, 0.1491,\n                      0.0934, 0.0739, 0.1047, 0.0918, 0.2595, 0.1537, 0.0673, 0.1673, 0.1652,\n                      0.2845, 0.2840, 0.3209, 0.1665, 0.1024, 0.4304, 0.1936, 0.0807, 0.1133,\n                      0.3649, 0.3224, 0.2216, 0.1891, 0.0970, 0.0672, 0.1050, 0.2206, 0.2638,\n                      0.2138, 0.0997, 0.1296, 0.0825, 0.1633, 0.0409, 0.0534, 0.2313, 0.1060,\n                      0.2582, 0.1280, 0.0949, 0.1426, 0.2063, 0.0679, 0.1112, 0.5035, 0.1476,\n                      0.0785, 0.0788, 0.1602, 0.2427, 0.2684, 0.3086, 0.3160, 0.3890, 0.1298,\n                      0.1110, 0.3319, 0.0925, 0.1000, 0.1059, 0.0892, 0.1422, 0.1038, 0.0763,\n                      0.2146, 0.3025, 0.2226, 0.2739, 0.1500, 0.4100, 0.1865, 0.1802, 0.3803,\n                      0.1332, 0.1271, 0.2700, 0.0972], device='cuda:0')),\n             ('block4.2.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block4.3.weight',\n              tensor([[[[-1.0930e-02,  9.1372e-03, -8.1457e-03],\n                        [-9.1215e-03,  1.0452e-02,  1.6400e-02],\n                        [-5.5537e-04,  7.2592e-04, -1.3947e-02]],\n              \n                       [[-1.6317e-02, -2.5509e-03, -1.6983e-02],\n                        [ 9.7899e-04, -6.0168e-03, -1.0598e-02],\n                        [-1.8595e-02, -1.9046e-02, -1.8949e-02]],\n              \n                       [[ 5.9515e-03, -2.0012e-02, -1.5949e-02],\n                        [-6.5714e-03,  1.8554e-02,  9.9059e-03],\n                        [ 5.5552e-04, -1.0561e-02,  3.4801e-03]],\n              \n                       ...,\n              \n                       [[ 9.0180e-03, -2.6621e-03, -1.4502e-02],\n                        [ 6.2458e-04,  3.0001e-03, -3.6331e-03],\n                        [ 3.4846e-03,  1.9338e-02,  1.8730e-02]],\n              \n                       [[ 2.0121e-02, -1.5928e-02, -3.0301e-03],\n                        [-1.3213e-02,  1.9349e-02, -1.4503e-02],\n                        [-2.1268e-04,  7.9852e-03, -8.0028e-03]],\n              \n                       [[-1.6651e-02, -8.2251e-03, -1.0961e-02],\n                        [-9.6935e-03,  1.8235e-02, -5.4489e-04],\n                        [ 9.7441e-03,  1.9844e-02, -9.0348e-03]]],\n              \n              \n                      [[[-7.4043e-03, -1.3147e-02, -1.1761e-02],\n                        [ 7.3175e-03, -4.7104e-03,  1.5871e-02],\n                        [-5.7504e-03,  1.2932e-02,  7.2754e-03]],\n              \n                       [[ 2.1086e-02,  9.7454e-03, -1.4772e-02],\n                        [-1.7736e-02, -1.2423e-02,  1.9264e-03],\n                        [-1.5327e-02, -1.8237e-03, -1.8732e-02]],\n              \n                       [[-1.3070e-03, -2.9414e-03,  1.2726e-02],\n                        [-1.8274e-02, -8.8381e-03,  1.2995e-02],\n                        [ 1.2279e-02,  9.1853e-04,  1.1084e-02]],\n              \n                       ...,\n              \n                       [[-1.4328e-02,  1.1700e-02,  1.0393e-02],\n                        [-1.2090e-03,  2.0363e-02, -9.3401e-03],\n                        [ 8.1192e-03,  1.3306e-02,  5.3848e-03]],\n              \n                       [[-2.1554e-03, -3.8856e-03,  9.8041e-03],\n                        [ 2.3558e-03, -2.3188e-03,  1.5915e-02],\n                        [-5.5796e-03, -1.0127e-02, -8.9452e-03]],\n              \n                       [[-1.3513e-02, -4.8675e-04, -2.0556e-02],\n                        [-3.7231e-03,  8.4458e-03, -1.1528e-02],\n                        [ 1.9589e-02,  1.0305e-03, -1.6298e-02]]],\n              \n              \n                      [[[ 3.6783e-03, -7.1684e-05,  1.7650e-02],\n                        [ 1.3416e-02, -8.4094e-03,  1.3074e-02],\n                        [-1.6694e-02,  1.3345e-02,  1.3199e-02]],\n              \n                       [[ 9.8870e-04, -1.3660e-02,  8.5887e-03],\n                        [-1.8423e-02,  1.5767e-02, -9.1695e-03],\n                        [-1.8945e-02,  7.5467e-03, -1.3941e-02]],\n              \n                       [[ 1.5859e-02,  5.3707e-03, -7.6413e-03],\n                        [-1.1045e-02, -1.5585e-02,  5.4739e-04],\n                        [-1.2518e-02,  9.2990e-03,  1.3443e-02]],\n              \n                       ...,\n              \n                       [[-1.8270e-02, -1.0512e-02,  1.9136e-02],\n                        [ 8.3766e-03,  9.7836e-04,  7.2758e-03],\n                        [ 6.1226e-03, -1.8314e-02,  2.1085e-03]],\n              \n                       [[-6.3064e-03,  2.8654e-03, -1.7087e-02],\n                        [-1.2209e-02, -3.2470e-03,  2.0259e-02],\n                        [ 1.9046e-02,  4.0585e-03, -6.8443e-04]],\n              \n                       [[-1.6082e-03, -8.4079e-04,  1.0992e-02],\n                        [ 1.0856e-02, -2.0346e-02, -2.0170e-02],\n                        [ 1.6713e-02, -2.0647e-02, -8.5868e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-4.9295e-03, -5.8629e-03, -8.3947e-03],\n                        [ 1.2124e-02,  1.4300e-05, -1.5813e-02],\n                        [-4.3571e-03,  6.3489e-04,  1.3322e-02]],\n              \n                       [[ 4.5927e-03,  7.8614e-03, -1.5559e-02],\n                        [ 9.8109e-03,  7.6347e-03,  1.5947e-02],\n                        [ 1.5077e-02, -4.6620e-04,  1.6203e-02]],\n              \n                       [[-6.2802e-03, -7.9384e-03, -1.8677e-02],\n                        [-6.4954e-03, -6.6817e-03,  8.7245e-03],\n                        [-1.2375e-02, -1.9084e-02, -7.2195e-03]],\n              \n                       ...,\n              \n                       [[ 1.9333e-03,  1.3840e-02, -4.0144e-03],\n                        [ 1.5296e-02,  1.2373e-02, -1.0700e-02],\n                        [ 1.3475e-02, -1.2476e-02, -1.5203e-02]],\n              \n                       [[-3.7536e-03,  9.7055e-03, -5.7227e-04],\n                        [-5.1082e-03,  1.9382e-02, -1.9745e-02],\n                        [-1.0897e-02, -4.6454e-03, -1.7492e-02]],\n              \n                       [[ 1.8622e-04,  1.0382e-03, -1.6992e-02],\n                        [ 1.3812e-03,  4.9527e-03,  1.8732e-02],\n                        [-5.5494e-03,  1.8100e-02,  1.5475e-02]]],\n              \n              \n                      [[[-1.3378e-02,  5.4635e-03,  1.7360e-02],\n                        [ 3.8922e-03, -1.2865e-02,  1.3268e-02],\n                        [-1.5165e-03, -3.3401e-03, -1.8315e-02]],\n              \n                       [[-1.1972e-02, -1.4719e-02,  1.7009e-03],\n                        [ 1.2113e-02,  4.5574e-03,  3.2470e-03],\n                        [ 9.6984e-03, -1.4781e-02,  2.3630e-03]],\n              \n                       [[-4.5939e-03,  6.9832e-03, -7.3341e-03],\n                        [-1.1346e-02, -1.8990e-02, -1.1891e-02],\n                        [ 2.0539e-02, -1.9983e-02, -8.7652e-04]],\n              \n                       ...,\n              \n                       [[-2.0252e-02, -6.6610e-03, -1.4838e-02],\n                        [-1.7360e-02,  4.3179e-03,  1.6963e-02],\n                        [-1.2453e-02, -1.2323e-02,  4.8987e-03]],\n              \n                       [[-1.5907e-02,  5.4007e-03, -1.9224e-02],\n                        [ 4.6324e-03, -4.3759e-03, -1.1489e-02],\n                        [ 4.8304e-03,  1.1892e-02, -1.9536e-02]],\n              \n                       [[-1.3977e-02, -7.9741e-03,  1.9653e-02],\n                        [-4.3986e-03,  1.4024e-02, -7.3496e-03],\n                        [-3.3811e-03, -2.0521e-02, -7.4028e-03]]],\n              \n              \n                      [[[-8.0693e-03,  1.8149e-02,  9.4700e-03],\n                        [ 1.7494e-02,  1.9985e-02,  1.8308e-02],\n                        [ 1.3040e-02, -1.5321e-02, -1.6495e-03]],\n              \n                       [[-2.5810e-03,  1.1250e-02, -1.4464e-02],\n                        [ 3.8039e-03, -6.7812e-03, -6.9011e-03],\n                        [-1.8868e-02, -1.5224e-02, -1.7075e-02]],\n              \n                       [[-1.2941e-03,  1.8081e-02, -5.1303e-03],\n                        [ 1.3429e-02,  1.0722e-02,  1.3647e-02],\n                        [ 1.0485e-02, -1.3802e-03,  5.1812e-04]],\n              \n                       ...,\n              \n                       [[-1.5628e-02,  1.5002e-02,  1.2503e-02],\n                        [-9.9631e-03,  8.5892e-03,  2.0026e-02],\n                        [ 7.1324e-03,  8.1100e-03, -1.0741e-02]],\n              \n                       [[-4.3422e-03, -5.0225e-03,  1.4652e-02],\n                        [ 9.3636e-03, -1.7230e-02,  1.5785e-02],\n                        [ 2.0072e-03,  1.0985e-02, -1.0113e-02]],\n              \n                       [[-1.8817e-02,  1.2502e-02, -1.6929e-02],\n                        [-4.4691e-03, -1.8649e-02, -5.9401e-03],\n                        [-2.0032e-02,  4.8374e-04, -5.6414e-03]]]], device='cuda:0')),\n             ('block4.3.bias',\n              tensor([ 1.8316e-02,  7.6635e-03, -3.4284e-03,  1.7025e-02, -5.4686e-03,\n                       1.3187e-02, -1.5790e-02,  1.3554e-02,  4.6817e-03,  3.7569e-03,\n                       9.1613e-03,  2.4296e-03, -9.5643e-03, -6.8831e-03, -1.0820e-02,\n                      -5.6279e-03,  6.7768e-05, -1.1780e-02,  2.5381e-03, -1.0450e-02,\n                      -6.0712e-03, -6.8534e-03, -7.7610e-03,  8.1846e-04, -1.5245e-02,\n                       1.4044e-02, -1.4824e-02, -1.7930e-02,  1.0435e-02, -3.6247e-03,\n                       3.2208e-03,  1.8009e-02,  4.5660e-03, -1.0592e-03, -1.8521e-02,\n                       9.7960e-03, -1.9161e-02, -1.2381e-02,  8.5457e-03,  2.0415e-02,\n                       4.1304e-03,  8.4833e-03, -1.4318e-02, -9.6494e-03,  7.8483e-03,\n                      -1.7932e-02, -6.8152e-03,  1.4883e-02, -1.0434e-02, -5.9725e-03,\n                       1.0352e-02,  2.4570e-03, -4.2806e-04, -5.3377e-03, -1.3823e-02,\n                       1.2052e-02,  8.0978e-03, -2.2879e-03, -4.5911e-05,  1.3596e-02,\n                       1.9440e-02, -8.1402e-03, -1.5502e-02, -1.0723e-03, -1.8137e-02,\n                      -1.0717e-02, -9.2304e-04,  1.7809e-02, -1.9602e-02, -1.2646e-02,\n                      -1.2227e-02,  6.5481e-03,  1.9405e-02,  1.3902e-02, -4.1292e-03,\n                      -4.6216e-03, -9.1348e-03, -2.3286e-03, -1.6798e-02,  3.4654e-03,\n                       1.9940e-02, -6.9856e-03,  8.3047e-03, -1.2846e-02, -8.4890e-04,\n                      -1.6897e-02, -7.0580e-03,  1.8110e-02, -1.2223e-02,  1.2101e-02,\n                       3.1178e-03,  6.1481e-03,  5.9543e-03,  1.0452e-02,  1.0450e-02,\n                      -2.0318e-02,  4.2065e-03,  1.8218e-02,  1.6398e-02, -1.8421e-02,\n                      -1.5214e-02, -4.4482e-03,  1.5858e-02, -2.0230e-02, -2.4701e-03,\n                      -7.7294e-03, -1.9833e-02, -9.1692e-03, -1.0456e-02, -1.9827e-03,\n                      -1.7498e-02, -5.7673e-03, -1.8049e-02, -1.7281e-03, -5.9188e-03,\n                       1.8665e-02,  1.2977e-02,  1.4997e-02, -1.5312e-02, -1.9245e-02,\n                       3.5626e-03,  1.0289e-02, -1.5145e-02, -1.5039e-02,  1.9424e-03,\n                       1.1508e-02, -1.2888e-02, -2.6716e-03, -7.0336e-03, -2.4187e-03,\n                      -1.7870e-02,  2.0162e-02,  6.6554e-03,  4.7385e-03,  1.6640e-02,\n                       1.3435e-02,  1.5064e-02, -2.4611e-03,  3.7354e-03,  1.5256e-02,\n                      -3.8676e-03,  8.0504e-03,  1.9289e-03,  1.1858e-02,  2.3021e-03,\n                       1.5033e-02,  1.2563e-02,  7.4197e-03,  1.9708e-02, -9.1433e-03,\n                       1.0779e-02,  7.2135e-03,  1.3676e-02, -7.3898e-03, -5.8518e-03,\n                       7.5844e-03, -1.0698e-02, -1.4826e-02,  8.5755e-03,  1.7297e-02,\n                      -1.1857e-02,  2.3839e-05,  6.7819e-03,  1.1141e-02, -1.4206e-03,\n                      -9.1844e-03,  6.8005e-03,  1.7985e-02,  8.0966e-03, -7.8104e-03,\n                       1.1490e-02, -2.4066e-03, -9.9870e-03, -1.2439e-02,  6.1363e-04,\n                      -4.8052e-03,  1.4116e-02,  1.4568e-02, -1.2326e-02, -1.1066e-02,\n                       3.4831e-03,  1.4484e-02, -1.5629e-02, -1.1127e-02,  1.9242e-02,\n                      -8.5849e-04,  1.3320e-02, -1.1503e-02,  1.1304e-02, -1.9567e-03,\n                       9.2300e-03,  1.5414e-02, -6.0418e-03,  2.0008e-02, -8.6009e-03,\n                      -1.0580e-02, -1.5036e-02,  1.9080e-02, -9.7127e-03,  1.5609e-03,\n                       1.1684e-02,  1.4093e-02,  1.1912e-02, -1.8053e-02, -1.2427e-02,\n                      -1.5842e-03, -3.9855e-03, -2.0262e-02,  6.1107e-03, -1.6955e-02,\n                       1.4625e-02, -5.5469e-03, -7.1976e-03,  1.5312e-02, -1.8331e-02,\n                       8.0090e-03, -1.1761e-02,  2.9007e-03, -2.0560e-02, -1.3060e-02,\n                       5.8407e-03, -1.1194e-02, -2.0413e-02, -9.6750e-03,  1.2681e-02,\n                       1.2954e-02,  1.9752e-02,  8.4286e-03, -1.8160e-02, -7.3935e-03,\n                       7.3678e-03,  1.0628e-02, -5.5919e-03,  1.5959e-02,  9.4059e-03,\n                       1.6295e-03,  1.6181e-02, -1.4003e-02, -7.5977e-03, -1.0892e-02,\n                       8.7590e-03, -2.0573e-02, -1.0777e-02,  9.6540e-03,  1.8688e-02,\n                      -9.1158e-04,  5.4145e-03, -2.0515e-02, -1.9830e-02,  2.1036e-02,\n                       2.9369e-03,  6.1589e-03,  1.0616e-02, -9.6597e-04, -1.5465e-02,\n                       1.4806e-02], device='cuda:0')),\n             ('block4.5.weight',\n              tensor([1.0002, 0.9998, 1.0001, 0.9999, 1.0001, 1.0003, 1.0000, 0.9999, 1.0000,\n                      0.9996, 0.9998, 1.0001, 1.0000, 1.0002, 1.0000, 1.0001, 0.9999, 0.9996,\n                      1.0003, 0.9999, 1.0001, 0.9997, 1.0002, 1.0000, 1.0001, 1.0002, 1.0000,\n                      0.9999, 0.9999, 1.0000, 1.0001, 1.0001, 0.9999, 1.0002, 0.9999, 1.0000,\n                      0.9998, 1.0002, 0.9999, 1.0000, 1.0000, 1.0001, 1.0003, 1.0002, 0.9996,\n                      0.9999, 1.0001, 1.0002, 1.0000, 1.0000, 1.0000, 1.0001, 0.9997, 0.9999,\n                      1.0002, 0.9998, 1.0001, 1.0000, 1.0000, 0.9999, 1.0001, 1.0001, 0.9999,\n                      1.0000, 1.0001, 1.0002, 1.0001, 1.0001, 1.0001, 0.9999, 1.0000, 1.0001,\n                      1.0000, 1.0000, 1.0001, 1.0000, 1.0002, 0.9996, 1.0000, 1.0000, 0.9999,\n                      1.0004, 1.0002, 1.0000, 1.0002, 1.0002, 0.9999, 0.9999, 0.9998, 1.0001,\n                      1.0003, 1.0003, 1.0001, 1.0000, 1.0000, 1.0002, 0.9999, 1.0001, 1.0000,\n                      1.0001, 1.0000, 1.0002, 0.9999, 0.9998, 1.0001, 0.9997, 0.9999, 1.0002,\n                      0.9999, 1.0002, 0.9999, 0.9998, 1.0002, 0.9998, 1.0001, 1.0000, 0.9999,\n                      0.9999, 0.9997, 1.0000, 1.0000, 0.9995, 1.0002, 1.0001, 0.9998, 0.9996,\n                      0.9999, 0.9998, 1.0001, 0.9999, 1.0003, 1.0001, 1.0002, 0.9998, 0.9996,\n                      0.9997, 0.9994, 1.0001, 0.9999, 1.0003, 1.0000, 1.0000, 1.0001, 1.0001,\n                      1.0001, 1.0000, 0.9998, 1.0001, 1.0001, 0.9999, 0.9998, 1.0002, 0.9998,\n                      1.0000, 0.9998, 1.0001, 0.9999, 0.9998, 1.0003, 1.0000, 1.0001, 1.0001,\n                      1.0000, 0.9999, 1.0000, 0.9999, 1.0002, 0.9998, 0.9998, 0.9998, 1.0001,\n                      0.9997, 1.0000, 1.0000, 1.0003, 1.0002, 1.0001, 1.0001, 1.0000, 1.0001,\n                      0.9999, 0.9999, 1.0001, 0.9999, 1.0001, 1.0001, 1.0001, 1.0001, 0.9997,\n                      1.0000, 1.0002, 1.0000, 0.9999, 1.0001, 0.9999, 1.0003, 1.0000, 1.0001,\n                      0.9999, 0.9997, 1.0000, 1.0001, 0.9996, 0.9998, 1.0000, 0.9998, 1.0002,\n                      1.0000, 1.0002, 1.0003, 1.0001, 0.9997, 1.0001, 1.0001, 0.9996, 1.0002,\n                      1.0001, 1.0001, 0.9999, 1.0001, 1.0002, 0.9997, 1.0001, 1.0001, 1.0001,\n                      1.0002, 0.9998, 1.0002, 0.9999, 1.0000, 0.9999, 1.0002, 1.0000, 1.0001,\n                      1.0002, 1.0001, 0.9997, 1.0001, 1.0001, 0.9999, 1.0003, 1.0000, 1.0000,\n                      0.9997, 0.9995, 1.0002, 0.9999, 1.0000, 1.0001, 0.9998, 0.9999, 1.0001,\n                      1.0000, 1.0000, 0.9999, 1.0000], device='cuda:0')),\n             ('block4.5.bias',\n              tensor([-6.8979e-05,  3.8351e-04, -5.9963e-05, -1.6154e-04, -9.7055e-05,\n                      -1.4113e-04,  2.4623e-05,  2.1814e-05, -2.0044e-04, -1.2464e-04,\n                      -1.9290e-04,  1.6698e-04, -1.8313e-05,  3.4398e-06, -4.4692e-05,\n                      -1.5434e-04,  4.0313e-07,  2.1852e-04,  2.5265e-04,  1.5086e-06,\n                      -1.8536e-04,  9.3665e-05, -8.4814e-06, -1.8651e-04, -1.0278e-04,\n                       2.9772e-04, -2.9021e-04,  2.7776e-04,  3.9577e-05,  5.5066e-05,\n                       2.1967e-04, -5.1485e-05,  1.5922e-04, -9.7226e-05,  1.5345e-04,\n                       8.6555e-06, -3.4608e-06, -1.1161e-04, -2.4703e-05,  7.6643e-05,\n                      -1.9557e-04, -5.2184e-06,  2.3706e-04,  2.6656e-04,  3.0437e-05,\n                       1.8318e-04,  2.0285e-04,  2.0897e-05, -2.4148e-04,  1.9668e-04,\n                       2.7814e-04,  6.5920e-05,  8.5135e-05, -2.4825e-04,  1.6370e-05,\n                      -6.1302e-05,  1.8733e-04, -1.3596e-04, -5.3862e-05, -1.7950e-04,\n                       2.0447e-04, -1.3937e-05, -1.0020e-04,  2.9591e-04,  9.3392e-05,\n                      -8.0146e-05,  7.3133e-06,  1.7539e-06,  1.1766e-04,  2.1656e-04,\n                      -1.4693e-04, -4.9905e-05, -1.6968e-04, -5.2848e-05,  1.0227e-05,\n                      -7.1034e-05,  3.2692e-04,  1.3309e-04, -3.6705e-04,  3.6074e-04,\n                      -1.1455e-04,  4.0205e-04,  7.8279e-05, -1.1198e-04, -8.0715e-05,\n                       1.3264e-05,  3.2084e-04,  2.7814e-04, -1.3421e-04, -1.1083e-04,\n                      -1.4102e-04,  4.5295e-05,  5.0448e-05,  2.3787e-04,  2.9461e-05,\n                       1.7833e-04, -5.0551e-05, -1.0223e-04,  1.9213e-05, -3.5442e-05,\n                      -8.3084e-06,  8.0864e-05,  3.1471e-04,  1.8342e-05, -7.0740e-05,\n                       1.0066e-04,  1.2177e-04,  4.5602e-05,  6.9467e-05, -5.8966e-05,\n                      -1.1121e-04,  1.4213e-05, -1.8369e-04,  1.2397e-04, -2.3157e-04,\n                      -1.4606e-04,  2.0735e-04, -1.1456e-04, -1.4076e-04, -3.9570e-05,\n                      -2.9898e-05, -4.6668e-05, -3.5037e-04,  2.6200e-04,  8.3800e-05,\n                       1.4169e-04,  1.3225e-04,  1.3347e-06, -1.5176e-04, -1.0979e-04,\n                      -1.7124e-04,  1.1847e-04, -9.3845e-05, -1.9846e-07,  2.0416e-04,\n                       4.8352e-04, -4.0194e-05,  3.7486e-05,  1.8820e-04, -2.4076e-04,\n                       2.7234e-04, -3.8374e-05, -9.5461e-05, -9.2285e-05, -9.7406e-05,\n                      -1.7444e-04, -4.2109e-06, -2.6531e-05,  6.5364e-06,  5.1753e-05,\n                       2.0762e-04, -3.3145e-04,  1.4238e-04,  7.9183e-05,  1.8152e-04,\n                      -3.1738e-06, -3.6453e-05, -6.2346e-05, -9.5650e-05, -1.6469e-05,\n                      -7.0254e-05,  3.4873e-04, -6.1875e-05,  1.6289e-04,  1.0807e-05,\n                      -9.8956e-05,  9.1216e-05, -1.0155e-04,  1.5713e-04,  3.1215e-05,\n                      -1.5461e-04,  3.0784e-04,  1.7839e-04, -2.0231e-05, -2.2885e-05,\n                      -4.5020e-05, -2.4030e-05, -2.0245e-04,  6.9038e-05, -9.4543e-05,\n                       1.2291e-04,  2.1749e-04,  2.0188e-04,  3.5574e-05, -1.6482e-04,\n                      -6.8231e-06,  3.0039e-05, -1.0169e-04,  2.0402e-04, -1.8724e-04,\n                      -2.4818e-05, -1.6796e-04,  2.0765e-05,  3.6971e-04,  1.4406e-04,\n                      -1.4333e-04, -7.7527e-05, -3.1520e-04, -2.5075e-05,  1.5349e-04,\n                       8.7647e-05,  1.5904e-04,  2.6816e-04, -2.3855e-05, -1.0797e-04,\n                      -1.2841e-04,  2.5517e-04,  1.2551e-04,  2.7893e-04,  1.4344e-05,\n                      -7.2386e-05,  5.3051e-04,  5.0644e-05,  1.5771e-04, -2.1302e-05,\n                      -3.9975e-05, -4.4893e-05, -1.5505e-04,  4.9388e-06, -4.0476e-05,\n                      -3.6651e-04, -3.1112e-04,  1.6222e-04,  1.6341e-04, -1.7009e-04,\n                       2.0002e-05, -8.1142e-07, -1.3231e-04, -3.5607e-05,  1.9480e-04,\n                      -5.2213e-05,  8.7902e-05, -2.0386e-05, -3.6293e-04,  2.9747e-04,\n                       1.9950e-05,  2.7604e-04,  1.1398e-04, -2.8917e-04,  2.4734e-04,\n                       2.2116e-04,  2.5842e-04, -5.9665e-05, -1.4934e-04,  3.1040e-04,\n                      -3.3214e-04, -1.7601e-05,  2.0302e-05,  1.7545e-04,  7.2717e-05,\n                       5.6779e-05, -6.0818e-05,  2.8710e-04, -8.1800e-05,  8.1263e-05,\n                       1.7750e-05], device='cuda:0')),\n             ('block4.5.running_mean',\n              tensor([0.2216, 0.2152, 0.2472, 0.2775, 0.2455, 0.2408, 0.2693, 0.2472, 0.2098,\n                      0.3107, 0.2399, 0.2509, 0.2071, 0.2382, 0.2349, 0.2420, 0.2261, 0.2081,\n                      0.1905, 0.2184, 0.2138, 0.2311, 0.2486, 0.2064, 0.2192, 0.2214, 0.1848,\n                      0.2312, 0.1880, 0.2387, 0.1951, 0.2736, 0.2140, 0.2899, 0.2841, 0.2472,\n                      0.2463, 0.2092, 0.2540, 0.2287, 0.2519, 0.2809, 0.2339, 0.2037, 0.2408,\n                      0.2416, 0.2933, 0.2470, 0.2118, 0.2261, 0.2184, 0.2847, 0.1970, 0.1867,\n                      0.2103, 0.2347, 0.2231, 0.2086, 0.2822, 0.2291, 0.2261, 0.2223, 0.2239,\n                      0.2299, 0.2471, 0.2427, 0.2175, 0.3036, 0.2490, 0.2413, 0.2317, 0.2500,\n                      0.2391, 0.2621, 0.2407, 0.2829, 0.2309, 0.2912, 0.2468, 0.2356, 0.2258,\n                      0.2223, 0.2406, 0.2299, 0.2822, 0.2107, 0.3001, 0.2015, 0.2339, 0.2673,\n                      0.2144, 0.2342, 0.2472, 0.1832, 0.1967, 0.2241, 0.2345, 0.2638, 0.2399,\n                      0.2558, 0.2035, 0.2248, 0.2057, 0.2036, 0.2013, 0.2328, 0.2377, 0.2314,\n                      0.2421, 0.2400, 0.2292, 0.2153, 0.2225, 0.2000, 0.2663, 0.2321, 0.2202,\n                      0.2869, 0.1829, 0.1966, 0.1988, 0.2259, 0.2312, 0.2039, 0.2621, 0.2068,\n                      0.2024, 0.2188, 0.2158, 0.2348, 0.2386, 0.2545, 0.2435, 0.2695, 0.2199,\n                      0.2604, 0.2185, 0.2975, 0.2289, 0.2537, 0.2247, 0.2508, 0.2676, 0.2465,\n                      0.2383, 0.2056, 0.2208, 0.2610, 0.2814, 0.2297, 0.2961, 0.2579, 0.2237,\n                      0.2636, 0.2492, 0.2327, 0.2723, 0.2623, 0.2218, 0.2389, 0.2189, 0.1854,\n                      0.2524, 0.3092, 0.2119, 0.2139, 0.2216, 0.2353, 0.2372, 0.2085, 0.2467,\n                      0.3364, 0.1986, 0.1911, 0.2031, 0.2402, 0.2077, 0.2107, 0.2610, 0.2147,\n                      0.2485, 0.1997, 0.2874, 0.2081, 0.2399, 0.1891, 0.2679, 0.3021, 0.2372,\n                      0.2197, 0.2637, 0.2636, 0.2299, 0.2128, 0.2093, 0.1983, 0.2327, 0.2427,\n                      0.1958, 0.2336, 0.2184, 0.2715, 0.2263, 0.2325, 0.2442, 0.2851, 0.2302,\n                      0.2778, 0.2701, 0.2223, 0.2289, 0.2351, 0.2488, 0.2481, 0.2137, 0.2385,\n                      0.1724, 0.2610, 0.2258, 0.2231, 0.2724, 0.2953, 0.2235, 0.2273, 0.2222,\n                      0.2273, 0.2303, 0.2267, 0.2076, 0.2132, 0.2744, 0.1849, 0.1941, 0.2467,\n                      0.2256, 0.2105, 0.2972, 0.2121, 0.2848, 0.2058, 0.2507, 0.2089, 0.2097,\n                      0.2260, 0.2112, 0.2326, 0.2157, 0.1905, 0.2767, 0.2266, 0.2527, 0.2494,\n                      0.2489, 0.2393, 0.2722, 0.2509], device='cuda:0')),\n             ('block4.5.running_var',\n              tensor([0.0974, 0.1149, 0.1137, 0.1538, 0.1094, 0.0962, 0.1562, 0.1119, 0.0924,\n                      0.3479, 0.1268, 0.1183, 0.0929, 0.1187, 0.0885, 0.1071, 0.1244, 0.1099,\n                      0.0829, 0.1066, 0.0922, 0.2060, 0.1068, 0.0890, 0.1233, 0.1057, 0.0902,\n                      0.1296, 0.0897, 0.1132, 0.0736, 0.2234, 0.1535, 0.1607, 0.2211, 0.1173,\n                      0.1960, 0.0831, 0.1166, 0.0981, 0.1073, 0.1108, 0.0923, 0.0994, 0.2655,\n                      0.1411, 0.1700, 0.1370, 0.1147, 0.0964, 0.0800, 0.2347, 0.1482, 0.0679,\n                      0.0999, 0.1117, 0.1061, 0.0892, 0.1403, 0.0982, 0.1103, 0.1013, 0.0982,\n                      0.0968, 0.1643, 0.0892, 0.1312, 0.1704, 0.1382, 0.1298, 0.1326, 0.1169,\n                      0.1172, 0.1327, 0.1041, 0.1506, 0.1052, 0.2594, 0.1070, 0.1562, 0.0826,\n                      0.0911, 0.1092, 0.0810, 0.1331, 0.0893, 0.1374, 0.0863, 0.1139, 0.1371,\n                      0.1133, 0.1223, 0.1113, 0.0678, 0.0857, 0.1259, 0.1208, 0.1289, 0.1104,\n                      0.1298, 0.0897, 0.0891, 0.0724, 0.0884, 0.0686, 0.1114, 0.1347, 0.1251,\n                      0.1648, 0.0982, 0.1373, 0.1264, 0.1397, 0.0976, 0.1440, 0.1084, 0.0974,\n                      0.1528, 0.0838, 0.0846, 0.0723, 0.1832, 0.1285, 0.0871, 0.1500, 0.1074,\n                      0.1108, 0.1423, 0.1108, 0.0949, 0.1414, 0.1086, 0.0953, 0.1606, 0.1073,\n                      0.1639, 0.1592, 0.2163, 0.0977, 0.1300, 0.1134, 0.1449, 0.1225, 0.1030,\n                      0.1241, 0.0846, 0.1064, 0.1234, 0.1374, 0.1166, 0.2308, 0.1269, 0.1107,\n                      0.1219, 0.1139, 0.0979, 0.2130, 0.1648, 0.1044, 0.0969, 0.1243, 0.0787,\n                      0.1394, 0.2005, 0.1678, 0.0958, 0.1321, 0.1077, 0.1610, 0.1267, 0.0958,\n                      0.2159, 0.0856, 0.1002, 0.0929, 0.1110, 0.0958, 0.0858, 0.1561, 0.0747,\n                      0.1601, 0.0783, 0.2882, 0.1023, 0.1146, 0.0677, 0.2194, 0.1631, 0.2010,\n                      0.0931, 0.0989, 0.1434, 0.1409, 0.1074, 0.0948, 0.1089, 0.1041, 0.1139,\n                      0.0725, 0.1831, 0.1048, 0.1651, 0.1225, 0.1477, 0.0888, 0.1945, 0.1220,\n                      0.1446, 0.1188, 0.1105, 0.1000, 0.1711, 0.0980, 0.1460, 0.1262, 0.1417,\n                      0.0592, 0.1378, 0.0953, 0.1087, 0.1473, 0.2467, 0.1113, 0.1263, 0.0796,\n                      0.1018, 0.1164, 0.0947, 0.0927, 0.0812, 0.1431, 0.0608, 0.0780, 0.1222,\n                      0.1084, 0.0921, 0.2566, 0.0886, 0.1503, 0.1296, 0.1106, 0.1019, 0.0881,\n                      0.1352, 0.1153, 0.1077, 0.0781, 0.0619, 0.1977, 0.0896, 0.1600, 0.1293,\n                      0.1602, 0.0987, 0.1352, 0.0989], device='cuda:0')),\n             ('block4.5.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('block4.6.weight',\n              tensor([[[[-0.0186,  0.0077,  0.0175],\n                        [ 0.0109,  0.0103, -0.0066],\n                        [-0.0143,  0.0042, -0.0019]],\n              \n                       [[ 0.0075,  0.0077, -0.0033],\n                        [-0.0128,  0.0027,  0.0145],\n                        [-0.0037,  0.0118, -0.0108]],\n              \n                       [[-0.0168,  0.0174,  0.0181],\n                        [ 0.0114, -0.0184, -0.0203],\n                        [-0.0025,  0.0203, -0.0139]],\n              \n                       ...,\n              \n                       [[ 0.0073, -0.0163, -0.0133],\n                        [-0.0026,  0.0161,  0.0103],\n                        [-0.0003, -0.0186, -0.0120]],\n              \n                       [[-0.0191, -0.0082,  0.0058],\n                        [-0.0172,  0.0063,  0.0048],\n                        [-0.0101,  0.0039,  0.0146]],\n              \n                       [[-0.0061, -0.0101,  0.0194],\n                        [-0.0130, -0.0151, -0.0155],\n                        [-0.0003, -0.0121,  0.0165]]],\n              \n              \n                      [[[-0.0152, -0.0014, -0.0060],\n                        [ 0.0139, -0.0116,  0.0100],\n                        [ 0.0131,  0.0181, -0.0153]],\n              \n                       [[-0.0120, -0.0077,  0.0169],\n                        [-0.0198,  0.0019,  0.0100],\n                        [ 0.0093, -0.0058, -0.0186]],\n              \n                       [[-0.0050, -0.0165,  0.0076],\n                        [-0.0019, -0.0089,  0.0111],\n                        [-0.0100, -0.0172,  0.0017]],\n              \n                       ...,\n              \n                       [[-0.0138,  0.0023,  0.0018],\n                        [ 0.0191,  0.0015,  0.0133],\n                        [ 0.0100,  0.0055,  0.0134]],\n              \n                       [[ 0.0073, -0.0160,  0.0110],\n                        [ 0.0146, -0.0147,  0.0168],\n                        [-0.0202, -0.0074,  0.0050]],\n              \n                       [[ 0.0062,  0.0003, -0.0157],\n                        [-0.0201,  0.0122, -0.0062],\n                        [-0.0085, -0.0195, -0.0207]]],\n              \n              \n                      [[[-0.0066, -0.0204, -0.0196],\n                        [-0.0173,  0.0208, -0.0137],\n                        [-0.0189, -0.0038, -0.0150]],\n              \n                       [[-0.0208, -0.0168,  0.0008],\n                        [ 0.0190, -0.0104, -0.0012],\n                        [ 0.0020, -0.0205,  0.0157]],\n              \n                       [[ 0.0076,  0.0102, -0.0119],\n                        [-0.0141,  0.0158, -0.0205],\n                        [-0.0150,  0.0175, -0.0090]],\n              \n                       ...,\n              \n                       [[-0.0053, -0.0113, -0.0068],\n                        [ 0.0117,  0.0072,  0.0046],\n                        [-0.0152,  0.0121,  0.0071]],\n              \n                       [[ 0.0131,  0.0142,  0.0029],\n                        [-0.0146, -0.0012,  0.0023],\n                        [ 0.0088,  0.0102, -0.0111]],\n              \n                       [[ 0.0030, -0.0058,  0.0052],\n                        [ 0.0041,  0.0022, -0.0144],\n                        [-0.0040, -0.0059,  0.0038]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0037,  0.0208,  0.0168],\n                        [ 0.0191,  0.0102, -0.0196],\n                        [-0.0099,  0.0131, -0.0003]],\n              \n                       [[-0.0019, -0.0016,  0.0048],\n                        [-0.0113, -0.0072, -0.0161],\n                        [-0.0010,  0.0132,  0.0126]],\n              \n                       [[-0.0066, -0.0081, -0.0038],\n                        [-0.0157, -0.0070, -0.0063],\n                        [-0.0175,  0.0173,  0.0168]],\n              \n                       ...,\n              \n                       [[ 0.0010,  0.0091, -0.0193],\n                        [-0.0044,  0.0074, -0.0075],\n                        [-0.0160, -0.0085,  0.0204]],\n              \n                       [[ 0.0169, -0.0122, -0.0049],\n                        [-0.0150, -0.0002,  0.0038],\n                        [ 0.0123,  0.0054,  0.0028]],\n              \n                       [[-0.0106, -0.0103, -0.0041],\n                        [-0.0032,  0.0009, -0.0040],\n                        [ 0.0054, -0.0175, -0.0047]]],\n              \n              \n                      [[[ 0.0078,  0.0167,  0.0074],\n                        [-0.0157, -0.0011,  0.0144],\n                        [ 0.0146, -0.0052,  0.0119]],\n              \n                       [[ 0.0152,  0.0199,  0.0054],\n                        [ 0.0088,  0.0129,  0.0097],\n                        [-0.0185,  0.0174,  0.0171]],\n              \n                       [[-0.0032, -0.0008, -0.0036],\n                        [-0.0003, -0.0126,  0.0125],\n                        [ 0.0188, -0.0131,  0.0018]],\n              \n                       ...,\n              \n                       [[ 0.0184, -0.0173,  0.0085],\n                        [ 0.0094, -0.0206, -0.0189],\n                        [-0.0090,  0.0122,  0.0187]],\n              \n                       [[-0.0005, -0.0166, -0.0076],\n                        [-0.0013,  0.0165, -0.0017],\n                        [-0.0207, -0.0149, -0.0107]],\n              \n                       [[-0.0135,  0.0160, -0.0149],\n                        [ 0.0131,  0.0180,  0.0012],\n                        [-0.0165,  0.0120,  0.0025]]],\n              \n              \n                      [[[-0.0151,  0.0133, -0.0118],\n                        [ 0.0184,  0.0028,  0.0033],\n                        [-0.0162,  0.0151,  0.0056]],\n              \n                       [[ 0.0189, -0.0009,  0.0207],\n                        [-0.0188, -0.0112, -0.0011],\n                        [-0.0013,  0.0099, -0.0206]],\n              \n                       [[-0.0046,  0.0040,  0.0148],\n                        [-0.0036,  0.0043,  0.0087],\n                        [-0.0054,  0.0203, -0.0139]],\n              \n                       ...,\n              \n                       [[-0.0203, -0.0122,  0.0066],\n                        [ 0.0002,  0.0180,  0.0192],\n                        [-0.0056, -0.0006, -0.0035]],\n              \n                       [[ 0.0084, -0.0087, -0.0033],\n                        [ 0.0087,  0.0122, -0.0120],\n                        [-0.0079,  0.0108,  0.0093]],\n              \n                       [[ 0.0125,  0.0093,  0.0143],\n                        [ 0.0015,  0.0043,  0.0153],\n                        [ 0.0051, -0.0173,  0.0018]]]], device='cuda:0')),\n             ('block4.6.bias',\n              tensor([ 4.9491e-04,  2.0924e-02,  1.1626e-02, -6.6692e-03,  1.7155e-02,\n                       1.5386e-02,  1.3194e-02,  1.8788e-02,  1.6679e-04,  5.0770e-03,\n                       1.7827e-02, -1.0040e-02,  1.1029e-02,  9.9592e-03,  1.6791e-02,\n                      -1.3289e-02,  1.4263e-03,  2.0418e-02,  1.1615e-02, -1.0390e-02,\n                       4.9115e-03, -1.3562e-02,  1.6213e-02, -3.3636e-03, -1.3867e-02,\n                       9.5696e-03, -1.1112e-02, -3.5082e-03, -4.4376e-03,  1.2276e-02,\n                      -1.1168e-02, -4.7799e-03,  8.4675e-03, -1.1633e-02,  1.7635e-02,\n                       6.4750e-03,  8.3839e-03,  1.9762e-02, -1.9322e-02,  4.5415e-03,\n                      -1.8730e-02, -1.3885e-02,  1.7752e-02, -6.2627e-03, -1.5861e-02,\n                      -1.8613e-02,  1.1685e-02,  2.1517e-03, -1.2909e-02, -1.8329e-02,\n                       1.5404e-02,  1.8580e-02,  1.6255e-02,  1.9855e-02,  1.0534e-02,\n                      -1.7329e-02, -1.3738e-02, -3.7356e-03, -1.0588e-02, -3.5846e-03,\n                      -6.1118e-03, -2.0132e-02, -1.8764e-03,  1.6322e-02,  1.1717e-02,\n                       1.7960e-03, -1.4451e-02,  1.4075e-02,  7.6069e-03,  9.0294e-03,\n                      -8.3745e-03, -5.5235e-03, -1.6145e-02,  1.3278e-02, -2.6862e-03,\n                      -1.2866e-02, -9.9199e-03, -2.0549e-03,  1.8696e-02,  1.2715e-02,\n                       7.5950e-03,  1.7956e-03,  9.8362e-03, -3.1460e-03,  8.7952e-03,\n                       7.2014e-03,  2.0767e-02, -4.4974e-06,  1.3798e-02, -8.0672e-03,\n                      -5.0624e-04, -1.2199e-02,  6.6680e-03, -2.6045e-03,  1.9744e-03,\n                      -1.3349e-02, -1.1364e-02, -6.0945e-03, -6.2336e-04,  1.5358e-02,\n                       1.1611e-02,  3.0405e-03, -1.2403e-02, -7.9828e-03, -1.7341e-03,\n                      -1.9224e-03,  1.5444e-02, -1.7746e-02,  9.8599e-03, -1.7245e-03,\n                      -1.6165e-02,  1.8063e-02,  1.9367e-02, -2.4940e-03, -1.2676e-02,\n                      -2.4149e-03, -2.2350e-03,  9.4411e-03, -9.7262e-03,  7.4475e-03,\n                       1.5092e-02, -1.3666e-02, -1.0239e-02,  1.5967e-02, -3.0135e-04,\n                      -2.0008e-03,  1.1446e-02, -2.0404e-03, -1.5198e-02,  1.0182e-02,\n                       3.0179e-03, -1.6435e-02, -1.8520e-02,  1.5244e-02,  2.0759e-02,\n                       9.1279e-03, -6.2069e-03, -9.8454e-04,  1.0310e-02,  1.0389e-02,\n                       7.3672e-03,  1.9344e-02, -1.8378e-02,  1.0974e-02,  4.9697e-03,\n                      -9.7705e-03,  2.1484e-03, -3.3143e-03,  1.4247e-02,  9.7844e-03,\n                       1.6519e-02, -8.9269e-03,  1.0027e-02, -8.3046e-03,  9.4463e-03,\n                      -2.1580e-03, -1.5077e-02, -3.2933e-03, -7.7874e-03,  1.8083e-02,\n                      -1.0337e-02,  6.6022e-03, -1.7802e-02,  1.9613e-02, -1.8291e-03,\n                       1.3141e-03, -1.1553e-02, -2.0060e-02,  1.2126e-02, -1.0307e-02,\n                       3.5954e-03, -6.7427e-03, -2.4632e-03, -5.1642e-03,  1.5183e-02,\n                       1.2602e-02,  2.1077e-02, -1.9710e-02, -1.2687e-02,  1.7246e-02,\n                       1.2058e-02, -7.2660e-03, -1.8045e-02,  1.2410e-02,  1.9256e-02,\n                      -1.2900e-02, -9.7074e-03,  4.2299e-03, -1.4812e-02, -7.4910e-03,\n                       1.4351e-02, -1.4770e-03, -1.5975e-02,  5.2226e-03, -1.2061e-02,\n                       1.6947e-02, -7.0604e-03, -1.4853e-02, -1.5786e-02, -6.9455e-03,\n                      -7.5875e-03, -8.1142e-04,  1.4541e-02,  1.6970e-02,  1.3893e-02,\n                      -1.4126e-03,  4.3238e-03, -1.4602e-02, -3.1391e-03, -4.7920e-03,\n                       9.8223e-03, -7.2941e-03, -8.0164e-03,  2.8998e-04, -2.0670e-02,\n                       1.5322e-02, -1.1956e-02,  7.0872e-03,  1.2284e-03,  1.8603e-02,\n                      -5.8734e-03, -7.4298e-03, -1.4922e-02,  2.8036e-03,  1.7893e-02,\n                       6.7808e-03, -8.9577e-03, -1.7276e-02, -6.4082e-03,  1.6082e-02,\n                      -1.5441e-02, -1.8661e-02,  1.1377e-02, -1.7843e-03, -1.8455e-02,\n                      -9.9470e-03, -3.7345e-03, -7.0062e-03, -1.2990e-02, -1.1891e-02,\n                       2.2837e-03,  2.3323e-03,  2.0548e-02, -1.2721e-02, -1.2285e-02,\n                      -2.0061e-02,  1.7134e-02, -3.0000e-03,  1.9635e-02,  1.9770e-02,\n                      -1.3253e-02,  8.9824e-03,  9.8584e-03,  1.9956e-02,  1.6073e-03,\n                       5.8857e-03], device='cuda:0')),\n             ('block4.8.weight',\n              tensor([0.9994, 0.9994, 0.9998, 1.0001, 0.9998, 0.9996, 1.0002, 0.9999, 0.9997,\n                      1.0000, 0.9995, 0.9996, 0.9997, 0.9994, 0.9998, 1.0003, 0.9998, 0.9998,\n                      0.9994, 0.9999, 0.9998, 0.9995, 0.9997, 0.9999, 0.9999, 0.9994, 1.0005,\n                      0.9994, 0.9996, 0.9998, 0.9996, 1.0001, 0.9997, 0.9999, 0.9997, 0.9994,\n                      0.9996, 0.9995, 0.9992, 1.0000, 0.9995, 0.9994, 0.9997, 1.0001, 1.0004,\n                      0.9996, 0.9994, 0.9997, 0.9996, 0.9999, 0.9995, 1.0001, 0.9997, 0.9998,\n                      0.9992, 0.9998, 0.9996, 0.9997, 0.9998, 0.9998, 1.0003, 1.0000, 0.9998,\n                      0.9995, 1.0001, 1.0003, 0.9997, 0.9997, 0.9996, 0.9999, 0.9996, 0.9997,\n                      0.9996, 0.9995, 0.9998, 1.0005, 0.9993, 0.9998, 0.9997, 0.9997, 0.9996,\n                      0.9998, 0.9994, 1.0000, 0.9997, 1.0002, 0.9996, 1.0001, 0.9998, 0.9998,\n                      1.0001, 0.9995, 0.9994, 0.9997, 0.9999, 1.0003, 1.0000, 1.0001, 1.0001,\n                      0.9996, 0.9995, 0.9995, 1.0000, 1.0002, 0.9994, 0.9997, 0.9993, 0.9995,\n                      1.0000, 1.0002, 0.9998, 0.9998, 0.9997, 0.9997, 1.0002, 0.9998, 0.9999,\n                      0.9997, 0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 1.0000, 0.9992, 1.0002,\n                      0.9999, 1.0003, 1.0002, 1.0001, 0.9999, 1.0001, 0.9994, 0.9997, 0.9998,\n                      0.9991, 0.9998, 1.0001, 0.9993, 1.0002, 1.0000, 0.9999, 0.9994, 1.0001,\n                      0.9999, 0.9999, 0.9998, 1.0004, 0.9999, 0.9998, 1.0002, 1.0000, 0.9999,\n                      0.9998, 1.0000, 0.9997, 1.0001, 0.9993, 1.0002, 0.9999, 0.9997, 0.9996,\n                      1.0002, 0.9994, 0.9994, 1.0002, 0.9995, 0.9994, 1.0003, 1.0000, 0.9998,\n                      0.9996, 1.0000, 1.0001, 0.9999, 1.0001, 0.9996, 1.0000, 0.9997, 0.9998,\n                      0.9997, 0.9999, 1.0001, 1.0001, 0.9997, 0.9999, 1.0003, 0.9998, 0.9994,\n                      0.9998, 1.0000, 0.9999, 0.9994, 1.0002, 0.9990, 0.9999, 0.9999, 0.9993,\n                      0.9991, 0.9994, 1.0002, 1.0003, 0.9998, 0.9999, 0.9998, 0.9999, 0.9995,\n                      1.0001, 1.0000, 1.0001, 0.9993, 1.0001, 1.0000, 0.9998, 1.0001, 0.9999,\n                      1.0003, 0.9995, 1.0001, 1.0001, 0.9999, 0.9997, 0.9995, 1.0002, 0.9996,\n                      0.9999, 0.9997, 0.9998, 0.9994, 1.0002, 1.0002, 0.9998, 0.9995, 0.9996,\n                      0.9994, 0.9996, 0.9993, 1.0001, 1.0000, 0.9995, 0.9996, 0.9998, 0.9996,\n                      0.9994, 1.0001, 0.9997, 0.9996, 0.9996, 1.0002, 0.9998, 1.0001, 0.9996,\n                      0.9994, 0.9995, 0.9996, 0.9993], device='cuda:0')),\n             ('block4.8.bias',\n              tensor([-2.9784e-05, -7.2069e-05,  1.2043e-05,  6.7488e-05, -1.2861e-05,\n                      -1.3217e-05,  4.7798e-05,  3.0610e-07, -4.4722e-05,  3.0490e-05,\n                      -2.9477e-05, -2.2820e-05, -4.0655e-05, -6.8371e-05,  1.5208e-05,\n                       7.3787e-05,  7.8143e-06, -1.0699e-05, -6.8801e-05,  1.6777e-05,\n                       3.0017e-05, -6.0540e-05, -5.2918e-05,  9.5889e-06,  2.6382e-05,\n                      -5.5316e-05,  7.0702e-05, -9.6646e-05, -2.8859e-05, -7.8827e-06,\n                      -3.9097e-05,  2.3910e-05, -4.6298e-06, -2.8811e-05, -1.5575e-05,\n                      -7.7357e-05, -2.9909e-05, -7.7817e-05, -3.3907e-05,  4.7165e-05,\n                       1.0154e-06, -2.1753e-05,  1.0157e-06,  2.3525e-05,  5.6538e-05,\n                      -2.8264e-05, -4.4504e-05, -3.2417e-05, -3.6888e-05,  4.4151e-05,\n                      -6.4892e-05,  1.0718e-04, -2.2268e-05, -7.5951e-06, -7.4060e-05,\n                       2.0795e-06,  1.7799e-05, -4.4862e-05, -1.4903e-05, -1.0394e-05,\n                       8.0165e-05,  2.3426e-05, -6.9051e-06, -4.1275e-05,  5.0447e-05,\n                       7.3258e-05, -1.3623e-05, -2.3236e-05, -2.1226e-05, -6.0021e-06,\n                      -3.2650e-05, -3.0471e-05,  1.7896e-05, -1.0241e-04, -6.2890e-06,\n                       5.7233e-05, -3.8651e-05,  5.6313e-06, -4.0731e-05, -2.8092e-05,\n                      -4.0476e-05,  2.3287e-05, -2.5634e-05,  4.8264e-05, -2.0079e-05,\n                       4.3664e-05, -1.9628e-05,  1.2751e-05, -1.7008e-05, -1.4546e-06,\n                       3.7889e-05, -8.0020e-05, -7.3330e-05,  3.4567e-05, -5.4663e-06,\n                       3.5361e-05,  3.0390e-05,  7.9767e-05,  3.6384e-05, -2.4124e-05,\n                      -3.6780e-05, -2.0239e-05,  2.9941e-06,  6.4410e-05, -5.4743e-05,\n                      -4.0673e-05, -8.1887e-05, -5.3115e-05,  3.0513e-05,  5.5869e-05,\n                       6.9301e-06, -1.8864e-05, -1.4454e-05, -4.1685e-05,  5.5330e-05,\n                      -2.8974e-05, -3.8983e-06, -8.9800e-06, -1.6893e-05, -6.7451e-05,\n                      -3.9036e-06,  3.1623e-05,  1.9486e-05,  2.0715e-05, -7.9428e-05,\n                       4.4706e-05,  6.5695e-06,  4.3434e-05,  3.7590e-05,  3.2556e-05,\n                       4.1505e-05,  5.0357e-05, -5.1185e-05, -4.7934e-06,  1.0221e-05,\n                      -6.5713e-05,  1.6676e-05,  2.4216e-05, -8.8199e-05,  3.0559e-05,\n                       3.4519e-06,  1.3995e-05, -6.6977e-05,  3.7065e-05,  2.5486e-05,\n                       1.8696e-06, -5.7256e-06,  8.1847e-05,  2.2531e-05,  2.5130e-05,\n                       3.3571e-05,  1.2968e-05, -1.1004e-06, -3.0335e-06, -7.1710e-06,\n                      -4.1151e-05,  2.6765e-05, -8.9411e-05,  5.2569e-05,  5.3895e-06,\n                      -2.1668e-05, -9.5600e-06,  4.7486e-05, -8.2305e-05, -4.3342e-05,\n                       3.6874e-06, -6.3161e-05, -4.7421e-05,  5.2608e-05,  2.5944e-05,\n                      -2.5355e-05,  2.2220e-05,  6.3089e-05,  2.8709e-05, -9.7398e-06,\n                       1.6078e-05, -6.3066e-05,  5.7325e-06, -3.8269e-05, -5.0678e-05,\n                      -2.4764e-05,  3.5433e-05,  2.1780e-05,  5.2124e-05, -1.3248e-05,\n                       3.5435e-06,  6.6890e-05,  2.5548e-05, -6.7028e-05,  3.4755e-05,\n                       1.1343e-05, -2.6413e-05, -4.6926e-05,  6.9943e-05, -9.0764e-05,\n                       2.4680e-05, -1.2069e-05, -6.0813e-05, -6.3468e-05, -5.0110e-05,\n                       5.1228e-05,  5.7051e-05, -3.0080e-05,  1.3416e-05, -8.1798e-06,\n                      -6.6912e-06, -3.5531e-05,  4.2978e-05,  2.0300e-05,  6.6669e-05,\n                      -5.5585e-05,  2.9807e-05,  3.6152e-05, -7.3741e-06,  1.7732e-05,\n                       1.3738e-05,  7.4124e-05, -1.8943e-05,  2.3065e-05,  2.5407e-05,\n                      -6.1443e-06,  1.2232e-05,  8.8042e-06,  7.0771e-05, -3.4084e-05,\n                       2.9341e-05, -1.2914e-05, -2.2284e-05, -5.7612e-05,  8.5314e-05,\n                       2.0233e-05, -3.1830e-05, -4.4354e-05, -3.9067e-05, -4.8838e-05,\n                      -3.7640e-05, -6.9499e-05,  3.5143e-05, -4.6137e-07,  1.5221e-06,\n                      -5.9423e-05,  3.7485e-05, -4.7760e-05, -6.1781e-05,  5.9614e-05,\n                      -1.5897e-05, -3.2803e-05, -6.1572e-06,  3.9397e-05,  7.7867e-06,\n                       6.1001e-05, -5.7970e-05, -6.8314e-05, -5.1618e-05, -3.9963e-05,\n                      -6.4858e-05], device='cuda:0')),\n             ('block4.8.running_mean',\n              tensor([0.2082, 0.2278, 0.3249, 0.2196, 0.2862, 0.2659, 0.2462, 0.2353, 0.2004,\n                      0.2194, 0.2143, 0.2171, 0.2485, 0.2007, 0.2187, 0.2439, 0.2970, 0.2358,\n                      0.2383, 0.2167, 0.2337, 0.2506, 0.2419, 0.2210, 0.2336, 0.2887, 0.2435,\n                      0.2112, 0.2527, 0.2207, 0.2276, 0.2017, 0.2690, 0.2076, 0.2108, 0.2522,\n                      0.3034, 0.3462, 0.2227, 0.2314, 0.2093, 0.2491, 0.2256, 0.1913, 0.2140,\n                      0.2525, 0.2585, 0.2581, 0.2438, 0.1883, 0.3197, 0.2662, 0.2122, 0.2091,\n                      0.2016, 0.2228, 0.2346, 0.2225, 0.2097, 0.2199, 0.2101, 0.2548, 0.2145,\n                      0.2404, 0.2468, 0.1906, 0.1985, 0.2158, 0.1957, 0.2779, 0.2230, 0.2418,\n                      0.2214, 0.2128, 0.2298, 0.1839, 0.2279, 0.2137, 0.2135, 0.2377, 0.2245,\n                      0.2261, 0.2261, 0.2245, 0.2252, 0.2700, 0.2227, 0.2672, 0.2459, 0.2513,\n                      0.2445, 0.2105, 0.2692, 0.2884, 0.2208, 0.2659, 0.2276, 0.2325, 0.2692,\n                      0.2611, 0.2462, 0.2424, 0.2224, 0.2297, 0.2132, 0.2581, 0.2949, 0.2264,\n                      0.2022, 0.1973, 0.2187, 0.2670, 0.2127, 0.2243, 0.2289, 0.2082, 0.1766,\n                      0.2275, 0.1966, 0.2579, 0.2098, 0.2575, 0.2143, 0.2512, 0.2391, 0.2131,\n                      0.2622, 0.1938, 0.2047, 0.2044, 0.2364, 0.2333, 0.2150, 0.2581, 0.2399,\n                      0.2823, 0.2881, 0.1940, 0.2354, 0.2221, 0.2145, 0.2371, 0.2400, 0.2094,\n                      0.2901, 0.2254, 0.2346, 0.2653, 0.2028, 0.2156, 0.2255, 0.2185, 0.2315,\n                      0.2221, 0.2114, 0.2292, 0.1855, 0.2300, 0.2529, 0.2474, 0.2063, 0.2213,\n                      0.2235, 0.2231, 0.2303, 0.2497, 0.2578, 0.2368, 0.2529, 0.1784, 0.2372,\n                      0.2775, 0.2334, 0.1879, 0.2148, 0.1970, 0.2049, 0.2717, 0.2485, 0.2371,\n                      0.2182, 0.2174, 0.1961, 0.2601, 0.2445, 0.2099, 0.2083, 0.2010, 0.2127,\n                      0.2185, 0.2875, 0.2241, 0.2207, 0.2330, 0.2357, 0.2179, 0.2252, 0.2512,\n                      0.2645, 0.2140, 0.2172, 0.2290, 0.2419, 0.2161, 0.2130, 0.2277, 0.2093,\n                      0.2086, 0.2025, 0.2526, 0.2266, 0.2447, 0.2374, 0.2102, 0.2338, 0.2538,\n                      0.2131, 0.2541, 0.1990, 0.2178, 0.2705, 0.3147, 0.1905, 0.2571, 0.2413,\n                      0.2227, 0.2267, 0.2396, 0.2761, 0.2343, 0.2287, 0.2000, 0.2302, 0.2827,\n                      0.2789, 0.2194, 0.2574, 0.1964, 0.2557, 0.2279, 0.2330, 0.2375, 0.2188,\n                      0.2261, 0.2101, 0.2079, 0.2308, 0.2062, 0.2132, 0.2535, 0.2811, 0.2096,\n                      0.2091, 0.2339, 0.2260, 0.2656], device='cuda:0')),\n             ('block4.8.running_var',\n              tensor([0.1051, 0.0955, 0.2105, 0.1508, 0.2058, 0.1270, 0.1208, 0.1075, 0.0945,\n                      0.0994, 0.0933, 0.1162, 0.1179, 0.0802, 0.1080, 0.1312, 0.2073, 0.0998,\n                      0.1314, 0.1181, 0.1191, 0.1459, 0.1081, 0.1059, 0.1408, 0.1998, 0.1446,\n                      0.1050, 0.1223, 0.1051, 0.1107, 0.0855, 0.1471, 0.0905, 0.0948, 0.1392,\n                      0.2093, 0.2423, 0.1312, 0.1065, 0.1009, 0.1478, 0.1298, 0.0850, 0.0971,\n                      0.1447, 0.1175, 0.1193, 0.1422, 0.0855, 0.2059, 0.1434, 0.0840, 0.0739,\n                      0.0876, 0.0959, 0.1317, 0.1251, 0.1064, 0.0985, 0.0969, 0.1415, 0.1101,\n                      0.1350, 0.1246, 0.0910, 0.0844, 0.1033, 0.0859, 0.1816, 0.1169, 0.1165,\n                      0.1097, 0.1182, 0.1078, 0.0762, 0.1091, 0.0981, 0.0937, 0.1008, 0.1178,\n                      0.1552, 0.1043, 0.1010, 0.1075, 0.1525, 0.0851, 0.1283, 0.1228, 0.1548,\n                      0.1361, 0.0785, 0.1289, 0.1906, 0.0959, 0.1870, 0.1153, 0.1438, 0.1528,\n                      0.1436, 0.1403, 0.1843, 0.1310, 0.1229, 0.1162, 0.1390, 0.1600, 0.1048,\n                      0.0818, 0.1108, 0.1244, 0.1397, 0.1057, 0.1311, 0.1094, 0.0778, 0.0766,\n                      0.1159, 0.0924, 0.1209, 0.0944, 0.1417, 0.0895, 0.1161, 0.1685, 0.0917,\n                      0.1327, 0.0974, 0.0864, 0.0921, 0.1311, 0.1296, 0.1279, 0.1386, 0.1260,\n                      0.2112, 0.1838, 0.0888, 0.1036, 0.0983, 0.0941, 0.1465, 0.1311, 0.0966,\n                      0.1884, 0.1155, 0.1388, 0.1503, 0.0862, 0.0902, 0.1000, 0.1025, 0.1143,\n                      0.1803, 0.0984, 0.1194, 0.0787, 0.1156, 0.1294, 0.1170, 0.0917, 0.1205,\n                      0.1058, 0.1116, 0.1391, 0.1155, 0.1220, 0.1263, 0.1228, 0.0758, 0.1270,\n                      0.1837, 0.1281, 0.0779, 0.0991, 0.0689, 0.0791, 0.1353, 0.1381, 0.1323,\n                      0.1272, 0.1222, 0.0950, 0.1280, 0.1234, 0.1014, 0.0951, 0.0833, 0.1262,\n                      0.1030, 0.2606, 0.1305, 0.1213, 0.1340, 0.1426, 0.0982, 0.1074, 0.1381,\n                      0.1947, 0.1106, 0.0998, 0.1216, 0.1515, 0.1139, 0.1128, 0.1152, 0.0958,\n                      0.0884, 0.0787, 0.1514, 0.1158, 0.1318, 0.1265, 0.0802, 0.1099, 0.1403,\n                      0.1140, 0.1583, 0.0834, 0.0874, 0.1495, 0.2380, 0.0720, 0.1286, 0.1151,\n                      0.1148, 0.1119, 0.1237, 0.1963, 0.1321, 0.1286, 0.1138, 0.1073, 0.2070,\n                      0.1701, 0.0848, 0.1399, 0.0889, 0.1581, 0.1274, 0.1135, 0.1259, 0.1087,\n                      0.1367, 0.0930, 0.0914, 0.1180, 0.0852, 0.1135, 0.1403, 0.1570, 0.0868,\n                      0.0923, 0.1239, 0.1104, 0.1236], device='cuda:0')),\n             ('block4.8.num_batches_tracked', tensor(2006, device='cuda:0')),\n             ('fc.weight',\n              tensor([[ 0.0107,  0.0492,  0.0132,  ...,  0.0533,  0.0298,  0.0555],\n                      [ 0.0544,  0.0240, -0.0471,  ...,  0.0421, -0.0292, -0.0118],\n                      [-0.0489,  0.0025,  0.0220,  ..., -0.0062, -0.0065,  0.0142],\n                      ...,\n                      [-0.0367, -0.0098,  0.0457,  ...,  0.0572, -0.0404,  0.0582],\n                      [ 0.0352,  0.0064, -0.0037,  ...,  0.0166, -0.0179,  0.0157],\n                      [ 0.0434,  0.0041, -0.0184,  ...,  0.0365, -0.0202,  0.0313]],\n                     device='cuda:0')),\n             ('fc.bias',\n              tensor([ 0.0440,  0.0240,  0.0021, -0.0194, -0.0242, -0.0325, -0.0260,  0.0208,\n                      -0.0103,  0.0104,  0.0413], device='cuda:0'))])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nimages = 0\n# mean = 0.\n# std = 0.\n# for batch, _ in train_loader:\n#     # Rearrange batch to be the shape of [B, C, W * H]\n#     batch = batch.view(batch.size(0), batch.size(1), -1)\n#     # Update total number of images\n#     nimages += batch.size(0)\n#     # Compute mean and std here\n#     mean += batch.mean(2).sum(0) \n#     std += batch.std(2).sum(0)\n\n# # Final step\n# mean /= nimages\n# std /= nimages\n\n# print(mean)\n# print(std)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}